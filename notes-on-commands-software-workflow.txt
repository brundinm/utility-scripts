Notes on commands, software, and workflow

MRBrundin
Fri 09-Jun-2017

Table of contents


Commands
Command Prompt, PowerShell, Unix, AWK, Perl, Python, PHP, and Ruby commands
Commands to archive, compress, and uncompress files
Commands to create and unpack JAR (Java Archive) files
Commands to delete a leftover Windows Update directory
Software
Databases
MySQL
CanWWR MySQL database
GeoNames MySQL database
drupal7 MySQL database
PostgreSQL
BaseX
Spreadsheets
Microsoft Excel
Text editors
Terminal text editors
Vim
nano
Emacs
Notepad++
TextWrangler
XML tools
Oxygen XML Editor
XMLStarlet
xmllint
xsltproc
Saxon-HE
XML Calabash
Xalan-Java
Xerces Java
Jing
Trang
Sun Multi Schema Validator (MSV)
Sun RELAX NG Converter (rngconv)
Tidy
xml_grep
xml_pp
Utilities
cURL
Wget
cloc
Zorba
Gmvault
Workflow
Activities
Notes on using the Fedora Resource Index Query Service and the Fedora RI REST API to query the Mulgara triplestore
Notes on using the Fedora Search Interface and the Fedora REST API to query the Fedora datastore
Notes on using the Islandora search and browse interface to query the Islandora repository
Notes on using the Solr Admin UI and the Solr REST API to query the Apache Solr index
Procedures to selectively re-index in Solr unindexed Fedora objects
Procedures to batch delete a set of Fedora objects from a list of PIDs
Procedures to obtain the number and file size of Fedora objects in a collection
Notes on using Drush (Drupal shell) scripts
Commands to install a Drupal module
GET methods for various LODs and APIs
Notes on the CWRC XML Validation Service
Notes on batch processing a directory of XML files
Notes on processing very large files (VLFs)
Steps to install the XMLStarlet utility in a user's home directory
Commands to install the Noid utility software
Software used to transform a historical map
Notes on the GeoNames database
Notes on Unix spell-checker and writing-aid programs
Server documentation
List of Unix servers
Web root directories on CWRC Web servers
Problem resolution
Oxygen XML Editor search and replace using both XPath expressions and regular expressions (Regeneration document <hi> element problem)
Resolution of Dell laptop display problem
Resolution of user profile problem
Remotely accessing the laptop in the office


Commands
Command Prompt, PowerShell, Unix, AWK, Perl, Python, PHP, and Ruby commands


* Command Prompt commands to get operating system information
   * ver
   * systeminfo | findstr /B /C:"OS Name" /C:"OS Version"
   * wmic os get buildnumber,caption,CSDVersion
   * winver
* Unix commands to get operating system information
   * uname -a
   * lsb_release -a
   * sw_vers (Mac OS X)
* Command Prompt command to securely log in to a remote host (PuTTY command)
   * plink brundin@gpu.srv.ualberta.ca
* Unix command to securely log in to a remote host
   * ssh brundin@gpu.srv.ualberta.ca
* Command Prompt commands to securely transfer files from a local host to a remote host (PuTTY commands)
   * pscp file1 file2 brundin@cwrc-apps-09.srv.ualberta.ca:~/tmp
   * psftp brundin@cwrc-apps-09.srv.ualberta.ca
* Unix commands to securely transfer files from a local host to a remote host
   * scp file1 file2 brundin@cwrc-apps-09.srv.ualberta.ca:~/tmp
   * sftp brundin@cwrc-apps-09.srv.ualberta.ca
* Command Prompt command to get the route (path) of data packets across an IP network
   * tracert
* Unix command to get the route (path) of data packets across an IP network
   * traceroute
* Command Prompt command to get private (local) IP address (network address)
   * ipconfig /all
* Unix command to get private (local) IP address (network address)
   * ifconfig
* Unix, Python, and PHP commands to get public (external) IP address
   * wget -q -O- icanhazip.com (Unix and Windows version)
   * curl ipecho.net/plain; echo (Unix version)
   * curl ipecho.net/plain -w "\n" (Unix and Windows version)
   * python -c "from urllib2 import urlopen; my_ip = urlopen('http://ip.42.pl/raw').read(); print my_ip" (Unix and Windows version)
   *  php -r '$u = "http://myexternalip.com/raw"; echo file_get_contents($u);' (Unix version)
   * php -r "$u = 'http://myexternalip.com/raw'; echo file_get_contents($u);" (Windows version)
* Perl command to resolve the IP address to the hostname
   * perl -MSocket -E 'say scalar gethostbyaddr(inet_aton("198.161.51.92"), AF_INET)'
* Python, PHP, and Ruby commands to run a simple Web server within a directory
   * python -m SimpleHTTPServer 8000
   * php -S localhost:8000 (need index.php or index.html file in the directory)
   * ruby -rwebrick -e"WEBrick::HTTPServer.new(:Port => 8000, :DocumentRoot => Dir.pwd).start"
* Command Prompt command to create a symbolic link
   * mklink /j BIBLIFO "C:\Users\brundin\Working\BIBLIFO"
* Unix command to create a symbolic link
   * ln -s /afs/ualberta.ca/home/c/w/cwrc cwrc
* Command Prompt and PowerShell commands to count number of lines in a file
   * find /c /v "" file.txt
   * findstr /r /n "^" file.txt | find /c ":"
   * powershell -command "(Get-Content file.txt | Measure-Object –Line).Lines"
* Unix, AWK, Perl, Python, PHP, and Ruby commands to count number of lines in a file
   * wc -l file.txt
   * wc -l < file.txt
   * sed -n "$=" file.txt
   * awk "END { print NR }" file.txt
   * perl -pe "}{$_=$." file.txt
   * perl -lne "} print $.; {" file.txt
   * perl -lne "END { print $. }" file.txt
   * python -c "num_lines = sum(1 for line in open('file.txt')); print num_lines"
   * php -r "$file = 'file.txt'; $lines = count(file($file)); echo $lines;"
   * ruby -ne 'END{printf "%8d %s\n", $., $FILENAME}' file.txt
* Command Prompt commands to count occurrences of a string in a file
   * find /c "string" file.txt
   * findstr /n /c:"string" file.txt | find /c ":"
   * findstr /c:"string" file.txt | find /c "string"
   * findstr /c:"string" file.txt | find /c /v ""
* Unix and AWK commands to count occurrences of a string in a file
   * grep -c "string" file.txt
   * grep "string" file.txt | wc -l
   * sed -n "/string/p" file.txt | sed -n "$="
   * awk "/string/" file.txt | awk "END { print NR }"
* Command Prompt and PowerShell commands to find occurrences of a string in a file
   * find "string" file.txt
   * findstr /c:"string" file.txt
   * powershell -command "get-content file.txt | where { $_ -match \"string\"}"
* Unix, AWK, Perl, Python, PHP, and Ruby commands to find occurrences of a string in a file
   * grep "string" file.txt
   * sed -n "/string/p" file.txt
   * awk "/string/" file.txt
   * perl -ne "/string/ && print" file.txt
   * cat file.txt | python -c "import re,sys;print('\n'.join(re.findall(r'^.*%s.*?$'%sys.argv[1],sys.stdin.read(),flags=re.M)))" "string"
   * php -r "$s='string';$ls=file('file.txt');foreach($ls as $l){if(strpos($l,$s)!==false)echo$l;}"
   * ruby -ne 'print if~/string/' file.txt
* Command Prompt command to find occurrences of a string in a directory or directories of files
   * findstr /s /n /i /c:"string" *
* Unix commands to find occurrences of a string in a directory or directories of files
   * find -name "*" -print | xargs grep "string"
   * grep -RnisI "string" *
* Command Prompt and PowerShell commands to print the first 10 lines of a file
   * for /l %l in (1,1,10) do @for /f "tokens=1,2* delims=:" %a in ('findstr /n /r "^" file.txt ^| findstr /r "^%l:"') do @echo %b
   * type file.txt | findstr /n ^^ | findstr "^[1-9]: ^10:"
   * powershell -command "& {Get-Content file.txt -TotalCount 10}"
* Unix, AWK, Perl, and Python commands to print the first 10 lines of a file
   * head -10 file.txt
   * sed 10q file.txt
   * awk "NR < 11" file.txt
   * awk "{print $0} NR==10{exit}" file.txt
   * perl -ne "print if $. <= 10" file.txt
   * perl -ne "$. <= 10 && print" file.txt
   * python -c "import sys; sys.stdout.write(''.join(sys.stdin.readlines()[:10]))" < files.txt
* Command Prompt and PowerShell commands to print the last 10 lines of a file
   * for /f %i in ('type file.txt ^| find /c /v ""') do set /a j=%i-10 && cmd /c more +%j% file.txt
   * powershell -command "& {Get-Content file.txt | Select-Object -last 10}"
* Unix, AWK, and Perl commands to print the last 10 lines of a file
   * tail -10 file.txt
   * sed -e :a -e "$q;N;11,$D;ba" file.txt
   * awk "{a[NR]=$0} END{for (i=NR-9;i<=NR;i++) print a[i]}" file.txt
   * awk "{ c=NR%n; a[c]=$0 } END{ for(i=1; i<=n; i++) print a[(c+i)%n] }" n=10 file.txt
   * perl -ne "push @a, $_; @a = @a[@a-10..$#a]; END { print @a }" file.txt
* Unix command to print from line 7 to line 14 of a file
   * sed -n '7,14p' file.txt
* Command Prompt commands to compare the contents of two ASCII text files
   * comp /a filename1 filename
   * fc /l filename1 filename2
* Unix commands to compare the contents of two ASCII text files
   * comm filename1 filename2
   * cmp -c filename1 filename2
   * diff filename1 filename2
* Command Prompt commands to find files in a directory or directories of files
   * dir *.txt /b /s
   * forfiles /s /m *.txt /c "cmd /c echo @path"
* Unix commands to find files in a directory or directories of files
   * find . -type f -name '*.txt'
   * find . -type f -regex '.+\.txt'
   * ls -R | grep txt$
* Command Prompt commands to find files modified in the last day
   * forfiles /p C:\Users\brundin\Desktop /s /m * /c "cmd /c echo @path @fdate" /d 0
   * forfiles /p C:\Users\brundin\Desktop /s /m * /c "cmd /c echo @path @fdate" /d 02/20/2014
   * xcopy C:\Users\brundin\Desktop\*.* /l /s /d:02-20-2014 .
* Unix commands to find files modified in the last day
   * find /data/opt/cwrc -ctime -1 -exec ls -l {} \;
   * find /data/opt/cwrc -mtime -1
   * find /data/opt/cwrc -newerct 'yesterday'
* Command Prompt command to loop through files in the current directory and run a command on the files
   * for %f in (*) do command %f
* Unix command to loop through files in the current directory and run a command on the files
   * for f in *; do command "$f"; done
* Unix command and shell script to use a for loop to loop through strings in a file, each string on one line of the file, and run a series of commands using the strings in a variable -- this command will retrieve the MODS datastreams from a list of PIDs for Fedora objects, and rename the files to get rid of the colon in the PID filename
   * for PID in `cat pids.txt`; do curl http://beta.cwrc.ca/islandora/object/$PID/datastream/MODS/download > $PID"_mods.xml"; mv $PID"_mods.xml" `echo $PID"_mods.xml" | sed s/:/_/`; done
   * for PID in `cat pids.txt`; do
    curl http://beta.cwrc.ca/islandora/object/$PID/datastream/MODS/download > $PID"_mods.xml"
    mv $PID"_mods.xml" `echo $PID"_mods.xml" | sed s/:/_/`
done
* Unix command and shell script to use a while loop to loop through strings in two files, each string on one line in each of the two files, and run a series of commands using the strings in two variables -- this command will change the Fedora label for a Fedora object, with the Fedora object PID string value coming from one file, and the new label string coming from the other file
   * paste pids.txt library-names.txt | while read PID LIBRARY_NAME; do curl -s -u fedoraAdmin:passwordHere -X PUT "http://beta.cwrc.ca:8080/fedora/objects/$PID?label=$LIBRARY_NAME"; done
   * paste pids.txt library-names.txt | while read PID LIBRARY_NAME; do
    while
        curl -s -u fedoraAdmin:passwordHere -X PUT "http://beta.cwrc.ca:8080/fedora/objects/$PID?label=$LIBRARY_NAME"
    done
* Unix commands to run a certain command 10 times
   * for ((n=0;n<10;n++)); do some_command; done
   * for i in {1..10}; do some_command; done
* Command Prompt command and batch file to generate a file consisting of 225 sequentially named with zero padding TIFF filenames, with each filename beginning on a new line in the file (only the batch file employs zero padding)
   * for /l %i in (1,1,225) do @echo flintfeathercomp00johnuoft-%i.tiff >> faf-tiff-names.txt
   * batch file:
@echo off
setlocal enableDelayedExpansion
for /l %%n in (1,1,225) do (
        set "n=00%%n"
        set "filename=flintfeathercomp00johnuoft-!n:~-3!.tiff
        echo !filename! >> faf-tiff-names.txt
)
* Unix commands to generate a file consisting of 225 sequentially named with zero padding TIFF filenames, with each filename beginning on a new line in the file
   * seq -w 1 225 | sed "s/^/flintfeathercomp00johnuoft-/" | sed "s/$/\.tiff/" > faf-tiff-names.txt
   * for i in $(seq -w 1 225); do echo "flintfeathercomp00johnuoft-$i.tiff"; done > faf-tiff-names.txt
   * for i in $(seq -f "%03g" 1 225); do echo "flintfeathercomp00johnuoft-$i.tiff"; done > faf-tiff-names.txt
   * for i in {1..225}; do echo flintfeathercomp00johnuoft-`printf %03d $i`.tiff; done > faf-tiff-names.txt
   * for i in {001..225}; do echo "flintfeathercomp00johnuoft-$i.tiff"; done > faf-tiff-names.txt
* Unix commands to copy a WORKFLOW.xml template file 100 times, and name each file sequentially with a zero-padded numeric suffix added to the base name (i.e., WORKFLOW-001.xml, WORKFLOW-002.xml, WORKFLOW-003.xml . . .)
   * for i in $(seq -w 1 100); do cp WORKFLOW.xml "WORKFLOW-$i.xml"; done
   * for i in $(seq -f "%03g" 1 100); do cp WORKFLOW.xml "WORKFLOW-$i.xml"; done
   * for i in {1..100}; do cp WORKFLOW.xml WORKFLOW-`printf %03d $i`.xml; done
   * for i in {001..100}; do cp WORKFLOW.xml "WORKFLOW-$i.xml"; done
* Command Prompt and PowerShell commands to rename XML files in the current directory by changing files with a space in the name to an underscore in the name
   * cmd /e:on /v:on /c "for %f in ("* *.xml") do (set "n=%~nxf" & set "n=!n: =_!" & ren "%~ff" "!n!" )"
   * forfiles /m *.xml /c "cmd /e:on /v:on /c set \"Phile=@file\" & if @ISDIR==FALSE ren @file !Phile: =_!"
   * powershell -command "Dir | Rename-Item –NewName { $_.name –replace \" \",\"_\" }"
* Unix commands to rename XML files in the current directory by changing files with a space in the name to an underscore in the name
   * rename 'y/ /_/' *
   * rename 's/ /_/g' *
   * ls -1 | while read file; do new_file=$(echo $file | sed s/\ /_/g); mv "$file" "$new_file"; done
   * for file in *.xml; do new=${file/ /_}; mv $file $new; done
* Unix commands to rename XML files in the current directory by changing each filename sequentially based on a list of new filenames contained in a file called "new-filenames.txt"
   * for file in *.xml; do read line;  mv -v "${file}" "${line}";  done < new-filenames.txt
   * ls *.xml | paste - new-filenames.txt | xargs -n2 mv
* Command Prompt and PowerShell commands to return the size of a top-level directory
   * dir . /s | find "File(s)" | sort /r | find /n /v "" | find "[1]"
   * powershell -command "(Get-ChildItem -Recurse '.' | Measure-Object -Property Length -Sum).Sum"
   * powershell -command "(ls -r | Measure-Object Length -Sum).Sum"
* Unix command to return the size of a top-level directory
   * du -ch | grep total
* PowerShell commands to return the size of a set of top-level directories
   * powershell -command "Get-ChildItem | Where-Object { $_.PSIsContainer } | ForEach-Object { $_.Name + \": \" + \"{0:N2}\" -f ((Get-ChildItem $_ -Recurse | Measure-Object Length -Sum -ErrorAction SilentlyContinue).Sum / 1MB) + \" MB\" }"
   * powershell -command "Get-ChildItem | Where-Object { $_.PSIsContainer } | ForEach-Object { \"{0:N2}\" -f ((Get-ChildItem $_ -Recurse | Measure-Object Length -Sum -ErrorAction SilentlyContinue).Sum / 1MB) + \" MB\" + \"`t\" + $_.Name }"
   * powershell -command "$colItems = Get-ChildItem \".\" | Where-Object {$_.PSIsContainer -eq $true} | Sort-Object; foreach ($i in $colItems) {$subFolderItems = Get-ChildItem $i.FullName -recurse -force | Where-Object {$_.PSIsContainer -eq $false} | Measure-Object -property Length -sum | Select-Object Sum; $i.FullName + \" -- \" + \"{0:N2}\" -f ($subFolderItems.sum / 1MB) + \" MB\"}"
* Unix commands to return the size of a set of top-level directories
   * du -d 1 -ch
   * du --max-depth 1 -ch (GnuWin32 version)
   * du -csh * (Unix and GnuWin32 version)
* Command Prompt and PowerShell commands to return the number of files in a top-level directory
   * dir . /b /a-d /s | find /c ":"
   * dir . /b /a-d /s | find /c /v ""
   * dir . /a-d /s | find "File(s)" | sort /r | find /n /v "" | find "[1]"
   * powershell -command "(Get-ChildItem -Recurse '.' | Measure-Object -Property Length).Count"
   * powershell -command "(ls -r | Measure-Object Length).Count"
* Unix command to return the number of files in a top-level directory
   * find . -type f | wc -l
* PowerShell commands and Windows batch file to return the number of files in a set of top-level directories
   * powershell -command "Get-ChildItem | Where-Object { $_.PSIsContainer } | ForEach-Object { $_.Name + \": \" + \"{0:N0}\" -f ((Get-ChildItem $_ -Recurse | Measure-Object Length -ErrorAction SilentlyContinue).Count)}"
   * powershell -command "Get-ChildItem | Where-Object { $_.PSIsContainer } | ForEach-Object { \"{0:N0}\" -f ((Get-ChildItem $_ -Recurse | Measure-Object Length -ErrorAction SilentlyContinue).Count) + \"`t\" + $_.Name }"
   * batch file: to run the batch file, in the parent directory of the set of top-level directories, type "batch_file.bat parent_directory", where "parent_directory" is the parent directory of the set of top-level directories (create a batch file from the 11 lines of code below):
@ECHO OFF
SET "rootpath=%~1"
FOR /D %%D IN ("%rootpath%\*") DO (
  SET cnt=0
  FOR /F %%K IN ('DIR /A-D /S "%%D" 2^>NUL ^| FIND "File(s)" ^|^| ECHO 0') DO (
    SET /A cnt=%%K
  )
  SETLOCAL EnableDelayedExpansion
  ECHO %%~nxD: !cnt!
  ENDLOCAL
)
* Unix commands to return the number of files in a set of top-level directories
   * find . -maxdepth 1 -type d | while read -r dir; do printf "%s:\t" "$dir"; find "$dir" -type f | wc -l; done
   * find . -maxdepth 1 -type d | sort | while read -r dir; do n=$(find "$dir" -type f | wc -l); printf "%4d : %s\n" $n "$dir"; done
* Unix command to find occurrences of a string in a very large ASCII text file
   * LC_ALL=C fgrep 'string' file.txt
* PowerShell and Command Prompt commands, and Windows batch file, to search and replace one pattern with another pattern in a file
   * powershell -command "get-content input_file | %{$_ -replace \"foo\",\"bar\"}" > output_file
   * cmd /q /v /c "for /f "tokens=* delims= " %a in (input_file) do (set str=%a& set str=!str:foo=bar!& echo !str!)" > output_file
   * batch file:
@echo off
setLocal EnableDelayedExpansionfor /f "tokens=* delims= " %%a in (input_file) do (


set str=%%a
set str=!str:foo=bar!
echo !str!>>output_file
)
endLocal
* Unix, AWK, Perl, Python, PHP, and Ruby commands to search and replace one pattern with another pattern in a file
   * sed "s/foo/bar/g" input_file > output_file
   * sed -i "s/foo/bar/g" input_file (in place edit)
   * awk '{gsub(/foo/,"bar"); print}' input_file > output_file
   * awk "{gsub(/foo/,\"bar\"); print}" input_file > output_file (GnuWin32 version)
   * awk '{gsub(/foo/,"bar")}; 1' input_file > output_file
   * awk "{gsub(/foo/,\"bar\")}; 1" input_file > output_file (GnuWin32 version)
   * awk '{gsub(/foo/,"bar")}; 1' input_file > tmp && mv tmp input_file (in place edit)
   * awk '{gsub(/foo/,"bar")}; 1' input_file | tee input_file (in place edit)
   * perl -pe "s/foo/bar/g" input_file > output_file
   * perl -pei "s/foo/bar/g" input_file (in place edit)
   * cat input_file | python -c "import sys,re;[sys.stdout.write(re.sub('foo', 'bar', line)) for line in sys.stdin]" > output_file
   * php -r "$file = 'input_file'; file_put_contents($file, str_replace('foo', 'bar', file_get_contents($file)));" > output_file
   * ruby -pe 'gsub(/foo/,"bar")' input_file > output_file
* Unix and Perl commands to search and replace one pattern with another pattern in a directory of files (these are recursive commands, so can operate on a directory tree as well); note: the sed version on a Mac (which is the BSD sed version) needs to have the single quotation marks after the -i option, i.e., -i ''
   * find ./mods -type f -print0 | xargs -0 sed -i 's/theCanadian/the Canadian/g'
   * find ./mods -type f -print0 | xargs -0 sed -i '' 's/theCanadian/the Canadian/g' (Mac version)
   * find ./mods -type f -exec sed -i 's/theCanadian/the Canadian/g' {} +
   * find ./mods -type f -exec sed -i '' 's/theCanadian/the Canadian/g' {} + (Mac version)
   * find ./mods -type f -name '*' -exec sed -i 's/theCanadian/the Canadian/g' {} +
   * find ./mods -type f -name '*' -exec sed -i '' 's/theCanadian/the Canadian/g' {} + (Mac version)
   * find ./mods -type f -print0 | xargs -0 perl -i -pe 's/theCanadian/the Canadian/g'
   * find ./mods -type f -exec perl -i -pe 's/theCanadian/the Canadian/g' {} +
* Unix, AWK, and Perl commands to search and replace one pattern with another pattern on a certain line number in a file (line number #4 in these examples)
   * sed '4s/foo/bar/' input_file > output_file
   * awk 'NR==4 {sub(/foo/,"bar")}1' input_file > output_file
   * perl -pe 's/foo/bar/ if $.==4' input_file > output_file
* Unix, AWK, and Perl commands to convert characters in a file from uppercase to lowercase (note: I have a Perl script called TitleCase.pl that will convert titles to "title case", i.e., "proper case" or "headline style" titles, with each word's initial letter capitalized, except for articles, prepositions, etc.; converting titles to "sentence case" or "sentence style" is far more difficult -- would need a large list of proper nouns in a file, and after comparing with the proper noun list file, the initial letter of each proper noun in the source file would then need to be capitalized [also, obviously, the first word of the title would be capitalized, and the first word after the colon if there is a subtitle would need to be capitalized])
   * dd if=input.txt of=output.txt conv=lcase
   * tr '[:upper:]' '[:lower:]' < input.txt > output.txt
   * sed 's/.*/\L&/' input.txt > output.txt
   * sed "s/.*/\L&/" input.txt > output.txt (GnuWin32 version)
   * awk '{print tolower($0)}' input.txt > output.txt
   * awk "{print tolower($0)}" input.txt > output.txt (GnuWin32 version)
   * perl -pe '$_= lc($_)' input.txt > output.txt
   * perl -pe "$_= lc($_)" input.txt > output.txt (Windows Strawberry Perl version)
* Unix command to convert file character encoding from Mac OS Roman (MacRoman, or Apple Standard Roman) to UTF-8
   * iconv -f MACINTOSH -t UTF-8 input_file.csv  > output_file.csv 
* Unix, AWK, and Perl commands to convert a Windows text file to a Unix text file (i.e., change line endings)
   * tr -d '\15\32' < winfile.txt > unixfile.txt
   * tr -d '\r' < winfile.txt > unixfile.txt
   * sed 's/\r$//' winfile.txt > unixfile.txt
   * awk '{ sub("\r$", ""); print }' winfile.txt > unixfile.txt
   * perl -p -e 's/\r$//' < winfile.txt > unixfile.txt
* Unix, AWK, and Perl commands to convert a Unix text file to a Windows text file (i.e., change line endings)
   * sed 's/$/\r/' unixfile.txt > winfile.txt
   * awk 'sub("$", "\r")' unixfile.txt > winfile.txt
   * perl -p -e 's/\n/\r\n/' < unixfile.txt > winfile.tx
* Unix commands to convert French letter characters with accents (diacritics) to their ASCII equivalents
   * cat input.txt | tr "àâäèéêëîïôùûüÿçÀÂÄÈÉÊËÎÏÔÙÛÜŸÇ" "aaaeeeeiiouuuycAAAEEEEIIOUUUYC" > output.txt
   * sed "s/œ/oe/g;s/Œ/OE/g;s/St /St\. /g;s/Ste /Ste\. /g;s/-/ /g;s/'s/s/g;s/’/'/g" input.txt > output.txt
* Unix command to print the lines with non-UTF-8 characters
   * grep -anxv '.*' filename.txt
   * grep -anxv ".*" filename.txt (GnuWin32 version)
* AWK command to print the lines with non-ASCII characters
   * awk '/[^\x00-\x7F]/{print NR":"$0}' filename.txt
   * awk "/[^\x00-\x7F]/{print NR":"$0}" filename.txt (GnuWin32 version)
* AWK command to print the names of files in a directory that have non-ASCII characters
   * awk '/[^\x00-\x7F]/{print FILENAME; nextfile}' * > ../files_list.txt
   * awk "/[^\x00-\x7F]/{print FILENAME; nextfile}" * > ..\files_list.txt (GnuWin32 version)
* Unix, AWK, and Perl commands to delete all blank lines in a file
   * grep '.' input.txt > output.txt
   * awk NF input.txt > output.txt
   * awk '/./' input.txt > output.txt
   * perl -ne 'print unless /^$/' input.txt > output.txt
   * perl -lne 'print if length' input.txt > output.txt
   * perl -ne 'print if /\S/' input.txt > output.txt
* AWK and Perl commands to strip leading whitespace (spaces and tabs) from beginning of each line
   * awk '{sub(/^[ \t]+/, "")};1' input.txt > output.txt
   * perl -ple 's/^[ \t]+//' input.txt > output.txt
   * perl -ple 's/^\s+//' input.txt > output.txt
* AWK and Perl commands to strip trailing whitespace (spaces and tabs) from end of each line
   * awk '{sub(/[ \t]+$/, "")};1' input.txt > output.txt
   * perl -ple 's/[ \t]+$//' input.txt > output.txt
* AWK and Perl commands to strip both leading and trailing whitespace (spaces and tabs) from each line
   * awk '{gsub(/^[ \t]+|[ \t]+$/,"")};1' input.txt > output.txt
   * perl -ple 's/^[ \t]+|[ \t]+$//g' input.txt > output.txt
* Unix, AWK, Perl, and Python commands to print out the longest line in a file
   * egrep -n "^.{$(wc -L < filename.txt)}$" filename.txt (includes line number preceding the line)
   * awk 'length > x { x = length; y = $0 } END { print y }' filename.txt
   * awk '{ print length(), NR, $0 | "sort -rn" }' filename.txt | head -n 1 (includes length and line number preceding the line)
   * perl -ne 'print ($l = $_) if (length > length($l));' filename.txt | tail -1
   * perl -ne '$l = $_ if length($_) > length($l); END { print $l }' filename.txt
   * perl -e 'while (<>) { if (length > length $max) { $max=$_}}; print $max' filename.txt
   * perl -ne 'print length()."  line $.  $_"' filename.txt | sort -nr | head -n 1 (includes length and line number preceding the line)
   * python -c "print max(open('filename.txt', 'r'), key=len)"
* Perl command to print out the shortest line in a file
   * perl -ne '$s = $_ if $. == 1; $s = $_ if length($_) < length($s); END { print $s }' filename.txt
* Unix, AWK, and PHP commands to globally strip out all opening and closing element tags from an XML file, i.e., just print the text node values or content
   * sed -e "s/<[^>]*>//g" input_file > output_file
   * awk '{gsub(/<[^>]*>/,"")};1' input_file > output_file
   * cat input_file | awk -F'>' '{print $2}' | awk -F'<' '{print $1}' > output_file
   * gawk -vRS='<|>' NR%2 input_file > output_file
   * grep -Po '<.*?>\K.*?(?=<.*?>)' input_file > output_file
   * cat input_file | php -R 'echo strip_tags($argn);' > output_file
   * lynx -dump -nolist input_file > output_file
* XML parser commands to globally strip out all opening and closing element tags from an XML file, i.e., just print the text node values or content (XML file needs to be well formed)
   * XMLStarlet: xml sel -T -t -c "//text()" input_file > output_file
   * xmllint: xmllint --xpath  "//text()" input_file > output_file
   * Saxon-HE: java -cp "/Users/brundin/Applications/Saxon-HE/saxon9he.jar" net.sf.saxon.Query -s:"input_file" -qs:"declare option saxon:output 'omit-xml-declaration=yes'; //text()" > output_file
   * BaseX: basex -i input_file -q "//text()" > output_file
   * Zorba: zorba --context-item input_file -r -q "//text()" > output_file
* Unix and XML parser commands to normalize and join (minify) a formatted and indented XML file into one line
   * tr '\n' ' ' < input_file | sed 's/>[ \t]*</></g' > output_file
   * xml fo --noindent input_file | tr -d "\n" > output_file
* Unix and AWK commands to retrieve and parse protocols plus the hostnames (resource names) from the URLs in the content or text of an HTML page to form an access control list or whitelist -- the HTML page is located at the following URL: http://library.macewan.ca/database-domain-list
   * wget -qO- http://library.macewan.ca/database-domain-list | sed -e "s/<[^>]*>//g" | sed 's/^[ \t]*//;s/[ \t]*$//' | grep "^http:" | awk -niord '{printf RT?$0chr("0x"substr(RT,2)):$0}' RS=%.. | sed "s/^http:\/\/ezproxy\.macewan\.ca\/login?qurl=//g" | sed "/^$/d" | awk -F/ '{print $1 "//" $3 "/"}' | sort | uniq > access-control-list.txt
   * curl -s http://library.macewan.ca/database-domain-list | sed -e "s/<[^>]*>//g" | sed 's/^[ \t]*//;s/[ \t]*$//' |  grep "^http:" | perl -pe 's/\+/ /g; s/%([0-9a-f]{2})/chr(hex($1))/eig' | sed "s/^http:\/\/ezproxy\.macewan\.ca\/login?qurl=//g" | awk -F/ '{print $1 "//" $3 "/"}' | sort | uniq > access-control-list.txt
   * curl -s http://library.macewan.ca/database-domain-list | xmllint --html --xpath "//div[@class='field-content']/a/text()" - 2>/dev/null | gsed "s/http/\nhttp/g" | sed 's/^[ \t]*//;s/[ \t]*$//' | perl -pe 's/\+/ /g; s/%([0-9a-f]{2})/chr(hex($1))/eig' | sed "s/^http:\/\/ezproxy\.macewan\.ca\/login?qurl=//g" | sed "/^$/d" | awk -F/ '{print $1 "//" $3 "/"}' | sort | uniq > access-control-list.txt
   * lynx -dump -listonly http://library.macewan.ca/database-domain-list | grep "http:\/\/ezproxy" | sed 's/^[ \t]*//;s/[ \t]*$//' |  perl -pe 's/\+/ /g; s/%([0-9a-f]{2})/chr(hex($1))/eig' | sed "s/^.*http:\/\/ezproxy\.macewan\.ca\/login?qurl=//g" | awk -F/ '{print $1 "//" $3 "/"}' | sort | uniq > access-control-list.txt
   * Notes:
      * Can manually add the following hostnames to the list:
         * http://macewan.library.ca/
         * http://www.macewan.ca/
         * http://libraryfiles.macwan.ca/ (add to the list for the third and fourth commands)
      * Command characteristics:
         * First command:
            * Uses Wget to retrieve file
            * Uses AWK to decode URLs
         * Second command:
            * Uses cURL to retrieve file
            * Uses Perl to decode URLs
         * Third command:
            * Uses xmllint to parse the text nodes
            * Uses gsed to add a new line character
         * Fourth command:
            * Uses Lynx to retrieve file
* Unix command to split a file into individual files based on a pattern (i.e., on context) -- used to process the Playwrights survey data file
   * csplit -f playwrights -b %02d.xml -s -z input.xml "/<?xml version/" {*}
* AWK commands to split a file into individual files based on a pattern -- used to process the Playwrights survey data file (escaped [i.e., \] double quotes so could run on Windows ports of the awk utility); first version runs using gawk and mawk utilities -- second version runs using gawk, mawk, and nawk utilities (i.e., nawk utility only runs using the second version)
   * awk "/<?xml version/{n++}{print >\"playwrights\" sprintf(\"%02d\",n) \".xml\" }" input.xml
   * awk "/<?xml version/{n++}{filename = \"playwrights\" sprintf(\"%02d\",n) \".xml\"; print >filename }" input.xml
* Unix command to copy a particular single filename that is repeated in a directory tree to a new directory, and rename the files sequentially (e.g., TEI0001.xml, etc.) -- used to copy the TEI (filename is "TEI.bin") and MODS (filename is "MODS.xml") datastreams from a downloaded Bag-It ZIP archive from the Spanish Civil War project (the collection is "Prisoner of War Letters"; PID is the following: islandora:3ed517db-86e0-4b84-b589-1b050e406dec) that has been unzipped into a directory
   * mkdir TEI_files; find ./Bag-* -name TEI.bin | awk 'BEGIN{ a=1 }{ printf "cp \"%s\" ./TEI_files/TEI%04d.xml\n", $0, a++ }' | bash
   * mkdir MODS_files; find ./Bag-* -name MODS.xml | awk 'BEGIN{ a=1 }{ printf "cp \"%s\" ./MODS_files/MODS%04d.xml\n", $0, a++ }' | bash
* Unix command to copy a particular single filename that is repeated in a directory tree to a new directory, and rename the files using the parent directory name of each file (this assumes each parent directory name is unique; in this case, the directory name is the PID of the object) -- used to copy the TEI (filename is "TEI.bin") and MODS (filename is "MODS.xml") datastreams from a downloaded Bag-It ZIP archive from the Spanish Civil War project (the collection is "Prisoner of War Letters"; PID is the following: islandora:3ed517db-86e0-4b84-b589-1b050e406dec) that has been unzipped into a directory
   *  mkdir TEI_files; for name in */*/*/*/TEI.bin; do cp "$name" "TEI_files/$(basename -- "$(dirname -- "$name")")_tei.xml"; done
   * mkdir MODS_files; for name in */*/*/*/MODS.xml; do cp "$name" "MODS_files/$(basename -- "$(dirname -- "$name")")_mods.xml"; done
* Perl command to strip the trailing slash and everything else that occurs after the numeric geonamesid integer record id number in the //PLACE/@REF attributes’ GeoNames URI values (i.e., http://www.geonames.org/number_here/placename_here.htm[l] ==> http://www.geonames.org/number_here); it does an in-place edit on a directory of files
   * perl -pin -e 's/(http:\/\/www\.geonames\.org\/\d+)\/[A-Za-z0-9_.-]*/$1/g' *
* Perl command to remove the //PLACE/@REF attributes with values that have incorrect "/maps" patterns in the GeoNames URI string references, i.e., the entire @REF attribute will be deleted for the @REF attributes that have the "/maps/" substring in the URI value string
   * perl -pin -e 's/ REF=\"http:\/\/www\.geonames\.org\/maps\/[A-Za-z0-9_.-]*\"//g' *
* Unix command to move files in sets of 9,900 files into separate directories (the directories have been created beforehand as sibling directories of the main directory that you are moving the files from, named 01, 02, 03, etc.) -- used to process the ECCJI MODS records, because the Islandora batch zip file import utility can only process a maximum of 9,984 files per set
   * for file in $(ls -p | grep -v | head -9900); do mv $file ../01; done
* Unix command to remove an installed program's files using the program's tar file
   * tar -tf file_name.tar | sort -r | (cd /; xargs -t -n 1 rm -d)
* Unix command to change the permissions of all directories in a directory tree to 755 (drwxr-xr-x)
   * find /home/brundin/working -type d -exec chmod 755 {} \;
* Unix command to change the permissions of all files in a directory tree to 644 (-rw-r--r--)
   * find /home/brundin/working -type f -exec chmod 644 {} \;
* Command Prompt commands to delete a directory containing hundreds of thousands of files
   * rmdir /s /q directory_name
   * Two pass approach:
      * del /f /s /q directory_name > nul
      * rmdir /s /q directory_name
* Unix and Perl commands to delete a directory containing hundreds of thousands of files
   * rm -rf directory_name
   * Using rsync:
      * mkdir empty_dir
      * rsync -a --delete empty_dir/    yourdirectory/
   * Using Perl:
      * cd yourdirectory
      * perl -e 'for(<*>){((stat)[9]<(unlink))}'
* Command Prompt commands to permanently delete a file or directory (sdelete is a utility that is part of the Windows Sysinternals Suite)
   * sdelete filename
   * sdelete -s directoryname
* Unix commands to permanently delete a file or directory
   * shred -u filename
   * find directoryname -type f -exec shred {} \;
   * srm filename
   * srm -r directoryname
* Command Prompt commands to permanently delete already deleted files on a hard drive by overwriting unallocated disk space with random bits or zeros, and then deleting these random bits
   * sdelete -z C:
   * cipher /w:C
* Unix commands to permanently delete already deleted files on a hard drive by overwriting unallocated disk space with random bits or zeros, and then deleting these random bits
   * sudo cat /dev/zero > zero.file; sync; rm zero.file
   * sudo cat /dev/urandom > urandom.file; sync; rm urandom.file (slower than using "zero" but slightly more secure)
   * sudo diskutil secureErase freespace 1 /dev/disk1 (run the "df" command first to determine the device name of the hard drive, e.g., hard drive device name in this example command is "/dev/disk1")
* Command Prompt command to add a new first line and a new last line to a file -- used to wrap a beginning <root> element and an ending </root> element around XML content to create a well-formed XML file
   * copy filename.xml temp.txt & echo.^<root^>>filename.xml & type temp.txt >>filename.xml & del temp.txt & echo.^<^/root^>>>filename.xml
* Unix, AWK, and Perl commands to add a new first line and a new last line to a file -- used to wrap a beginning <root> element and an ending </root> element around XML content to create a well-formed XML file
   * echo "New first line" > new_file; cat original_file >> new_file; echo "New last line" >> new_file
   * awk 'BEGIN {print "New first line"} {print} END {print "New last line"}' original_file > new_file
   * perl -ne 'BEGIN {print "New first line\n"} print; END {print "New last line\n"}' original_file > new_file
* Unix, AWK, and Perl commands to print every 5th line of a file
   * sed -n '0~5p' file.txt (uses GNU extension; not available in BSD Unix)
   * sed -n 'n;n;n;n;p;' file.txt
   * awk '!(NR%5)' file.txt
   * perl -lne '$.%5 or print;' file.txt
   * perl -ne 'print if $. % 5' file.txt
* Unix commands to print a random subset of 10 lines from a file
   * shuf -n10 (not available in BSD Unix)
   * jot -r "$(wc -l file.txt)" 1 | paste - file.txt | sort -n | cut -f 2- | head -n 10
* PowerShell command and Windows batch file to randomize the lines in a file
   * powershell -command "Get-Content "file.txt" | Sort-Object{Get-Random} | Out-file "output.txt"
   * batch file (to use: type file.txt | shuffle.bat)
@ECHO OFF
SETLOCAL ENABLEDELAYEDEXPANSION
SET TmpFile=tmp%RANDOM%%RANDOM%.tmp
TYPE NUL >%Tmpfile%
FOR /F "tokens=*" %%i IN ('MORE') DO SET Key=!RANDOM!!RANDOM!!RANDOM!000000000000& ECHO !Key:~0,15!%%i>> %TmpFile%
FOR /F "tokens=*" %%i IN ('TYPE %TmpFile% ^| SORT') DO SET Line=%%i&ECHO.!Line:~15!
DEL %TmpFile%
ENDLOCAL
* Unix, AWK, Perl, Python, PHP, and Ruby commands to randomize the lines in a file
   * shuf file.txt (not available in BSD Unix)
   * cat file.txt | awk 'BEGIN{srand();} {printf "%06d %s\n", rand()*1000000, $0;}' | sort -n | cut -c8-
   * cat file.txt | perl -MList::Util=shuffle -e 'print shuffle(<STDIN>);'
   * python -c "import random, sys; lines = open(sys.argv[1]).readlines(); random.SystemRandom().shuffle(lines); print ''.join(lines)," file.txt
   * python -c "import random; lines = open('file.txt').readlines(); random.shuffle(lines); open('file-out.txt', 'w').writelines(lines)"
   * php -r "$a=file('file.txt'); shuffle($a); foreach($a as $v) {echo $v;}"
   * cat file.txt | ruby -e 'puts STDIN.readlines.shuffle'
* Unix and AWK commands to print the lines in file1 that are not in file2
   * comm -23 <(sort file1) <(sort file2)
   * cat file1 file2 file2 | sort | uniq -u
   * grep -vFf file2 file1
   * fgrep -x -f file2 -v file1
   * diff file1 file2 --new-line-format="" --old-line-format="%L" --unchanged-line-format=""
   * diff --unchanged-group-format='' --old-group-format='%<' --new-group-format='' --changed-group-format='%<' file1 file2
   * diff file1 file2 | grep "<" | sed -e 's/< //'
   * awk 'FNR==NR{a[$0];next}!($0 in a)' file2 file1
   * awk 'FNR==NR{a[$0]++}FNR!=NR && !a[$0]{print}' file2 file1
* Unix, AWK, and Perl commands to print the lines in file1 that are also in file2
   * comm -12 <(sort file1) <(sort file2)
   * cat file1 file2 | sort | uniq -d
   * grep -Ff file2 file1
   * fgrep -x -f file2 file1
   * diff file1 file2 --new-line-format="" --old-line-format="" --unchanged-line-format="%L"
   * diff --unchanged-group-format='%<' --old-group-format='' --new-group-format='' --changed-group-format='' file1 file2
   * diff --side-by-side file1 file2 | grep -n -v "[|<>]" | awk '{print $2}'
   * diff --width=155 --left-column --side-by-side file1 file2 | grep -n -v -e '|' -e '<' -e '>' | sed 's/ *($//;s/^[0-9]*://g'
   * awk 'FNR==NR{a[$0];next}($0 in a)' file2 file1
   * awk 'FNR==NR{a[$0]++}FNR!=NR && a[$0]{print}' file2 file1
   * perl -ne 'print if ($seen{$_} .= @ARGV) =~ /10$/'  file1 file2
* PowerShell and Command Prompt commands, and Windows batch file, to print the unique lines in a file
   * powershell -command "& {Get-Content input_file | Sort-Object | Get-Unique}" > output_file
   * cmd /q /v /c "set "prev="& for /f "delims=" %f in ('sort input_file') do (set "curr=%f"& setlocal enabledelayedexpansion & if !prev! neq !curr! echo !curr!& endlocal & set "prev=%f")" > output_file
   * batch file:
@echo off
setlocal disabledelayedexpansion
set "prev="
for /f "delims=" %%F in ('sort input_file') do (
 set "curr=%%F"
 setlocal enabledelayedexpansion
 if !prev! neq !curr! echo !curr! >> output_file
 endlocal
 set "prev=%%F"
)
* Unix, AWK, Perl, Python, PHP, and Ruby commands to print the unique lines in a file
   * cat input_file | sort | uniq > output_file
   * cat input_file | gsort | uniq > output_file (GnuWin32 version)
   * sort < input_file | uniq > output_file
   * sed -n "G; s/\n/&&/; /^\([ -~]*\n\).*\n\1/d; s/\n//; h; P" input_file.txt > output_file.txt
   * awk "!a[$0]++" input_file > output_file
   * awk '!seen[$0]++' input_file > output_file
   * awk "!seen[$0]++" input_file > output_file (GnuWin32 version)
   * perl -ne "print unless $a{$_}++" input_file > output_file
   * python -c "import sys; lines = sys.stdin.readlines(); print ''.join(sorted(set(lines)))" < input_file > output_file
   * php -r "$lines = array_unique(file('input_file')); echo implode($lines);" > output_file
   * ruby -ne 'BEGIN{$h={}}; print unless $h[$_]; $h[$_] = $_' input_file > output_file
   * ruby -e 'print *readlines.uniq' input_file > output_file
* Unix commands (using XMLStarlet and xsltproc utilities) to print the unique XPaths (only element nodes [no attribute or text nodes], with no index value predicates) in an XML document; the first example (using the XMLStarlet utility) prints the entire XML element node tree structure, while the second example (using the xsltproc utility) only prints the terminal element leaf node branch structure (a leaf node has no children) -- the XPaths_tree.xsl XSLT file in the second example can be found in the "misc/scripts" directory in the CWRC GitHub set of repositories
   * xml el -u input.xml | sed -e "s/^/\//g" > XPaths.txt
   * xsltproc XPaths_tree.xsl input.xml | sed -e "s/\[[^][]*\]//g" | sort | uniq > XPaths.txt
* AWK command to print a blank line after every 5th line in a file
   * awk -v n=5 '1; NR % n == 0 {print ""}' input.txt
* Unix, AWK, and Perl commands to print the substring between two double quotation marks (") on each row of a file
   * cut -d'"' -f2 file.txt | sort > file-output.txt
   * cut -d\" -f2 file.txt | sort > file-output.txt
   * sed 's/^[^"]*"\([^"]*\)".*/\1/' file.txt | sort > file-output.txt
   * sed 's/^[^"]*"//; s/".*//' file.txt | sort > file-output.txt
   * awk -F '"' '{print $2}' file.txt | sort > file-output.txt
   * perl -F'"' -anle 'print $F[1]' file.txt | sort > file-output.txt
* Perl command to sort a file of hostnames (fully qualified domain names, or FQDNs) alphabetically
   * cat hostnames.txt | perl -ple'$_=join".",reverse split/\./' | sort | perl -ple'$_=join".",reverse split/\./'
* Unix command to sort a file alphabetically by the last word on each line
   * awk '{print $NF,$0}' filename.txt | sort | cut -f2- -d' '
   * awk "{print $NF,$0}" filename.txt | gsort | cut -f2- -d" " (GnuWin32 version)
* Unix command to sort first column (frequency count) in descending order and second column (title) in ascending order
   * sort -k1,1nr -k2,2 input.txt > output.txt
   * gsort -k1,1nr -k2,2 input.txt > output.txt (GnuWin32 version)
* AWK, Perl, Python, and Ruby commands to print the last column of every line in the file, with the columns separated by whitespace
   * awk '{print $NF}' filename.txt
   * awk "{print $NF}" filename.txt (GnuWin32 version)
   * perl -a -lane "print $F[-1]" filename.txt 
   * python -c "import sys;[sys.stdout.write(' '.join(line.split(' ')[-1:])) for line in sys.stdin]" < filename.txt
   * ruby -ane 'puts $F.last' filename.txt
* Unix, AWK, and Perl commands to print the first column of a tab-separated file
   * cut -f 1 filename.txt
   * awk -F"\t" '{print $1}' filename.txt
   * awk -F"\t" "{print $1}" filename.txt (GnuWin32 version)
   * perl -F"\t" -lane 'print "$F[0]"' filename.txt
   * perl -F"\t" -lane "print \"$F[0]\"" filename.txt (GnuWin32 version)
* AWK and Perl commands to print the first column and the last column of a tab-separated file
   * awk -F"\t" '{print $1,$NF}' filename.txt
   * awk -F"\t" "{print $1,$NF}" filename.txt (GnuWin32 version)
   * perl -F"\t" -lane 'print "@F[0,-1]"' filename.txt
   * perl -F"\t" -lane "print \"@F[0,-1]\"" filename.txt (GnuWin32 version)
* AWK command to print from the first column to the second last column (i.e., omit the last column) of every line in the file
   * awk '{$NF=""; print $0}' filename.txt
   * awk "{$NF=\"\"; print $0}" filename.txt (GnuWin32 version)
* AWK command to print the data in the file, and then at the bottom of the file print the total of column 2 values based on the column 1 value (column 1 contains "yes" and "no" values, and column 2 contains number values)
   * awk '{print}; $1 ~ /yes/ {sum_yes+=$2}; $1 ~ /no/ {sum_no+=$2} END {print "----------\n" "Yes: " sum_yes "\n" "No: " sum_no}' filename
   * awk "{print}; $1 ~ /yes/ {sum_yes+=$2}; $1 ~ /no/ {sum_no+=$2} END {print \"----------\n\" \"Yes: \" sum_yes \"\n\" \"No: \" sum_no}" filename (GnuWin32 version)
* AWK command to print out all the rows where column 2 matches a particular string
   * awk -F\t '$2 == "string" { print $0 }' input.txt > output.txt
   * awk -F\t "$2 == \"string\" { print $0 }" input.txt > output.txt (GnuWin32 version)
* AWK command to print out only the rows in file2 where column 6 in file2 matches column 2 in file1, and append the column 1 value in file1 as a new last column value in file2 (so rows with no match will get dropped in file2)
   * awk -F\t 'NR==FNR{a[$2]=$1;next}a[$6]{print $0"\t"a[$6]}' file1 file2 > file2-new
   * awk -F\t "NR==FNR{a[$2]=$1;next}a[$6]{print $0\"\t\"a[$6]}" file1 file2 > file2-new (GnuWin32 version)
* AWK command to print out the rows in file2, and where column 6 in file2 matches column 2 in file1, append the column 1 value in file1 as a new last column value in file2, and if there is no match, add the string "NA" as the value in the new appended last column value in file2
   * awk -F\t 'BEGIN{OFS="\t";}NR==FNR{a[$2]=$1;next}{print $0,a[$6]?a[$6]:"NA"}' file1 file2 > file2-new
   * awk -F\t "BEGIN{OFS=\"\t\";}NR==FNR{a[$2]=$1;next}{print $0,a[$6]?a[$6]:\"NA\"}" file1 file2 > file2-new (GnuWin32 version)
* AWK command to print out the rows in file2, and where column 6 in file2 is a case insensitive match with column 2 in file1, append the column 1 value in file1 as a new last column value in file2, and if there is no case insensitive match, add the string "NA" as the value in the new appended last column value in file2
   * awk -F\t 'BEGIN{OFS="\t";}NR==FNR{a[tolower($2)]=$1;next}{print $0,a[tolower($6)]?a[tolower($6)]:"NA"}' file1 file2 > file2-new
   * awk -F\t "BEGIN{OFS=\"\t\";}NR==FNR{a[tolower($2)]=$1;next}{print $0,a[tolower($6)]?a[tolower($6)]:\"NA\"}" file1 file2 > file2-new (GnuWin32 version)
* AWK commands to print the 1st, 13th, last, and 11th columns (i.e., fields) in file1 (file1 is called "catalog.txt") as tab-delimited output, then print only the rows in file1 where the 1st column value is also contained (i.e., is a string pattern or row entry) in file2 (file2 is called "documents.txt"), and then sort the resulting output from file1 by column 2 in integer descending order -- the fields in the original file1 (i.e., "catalog.txt") are delimited by 3 colons like so ":::", and the final resulting output file is called "jca-final.csv" (note: could also use "/dev/stdin" instead of the hyphen "-" to represent standard input in the commands below -- so you can replace the hyphen "-" in commands with the appropriate "/dev/stdin" [standard input], "/dev/stdout" [standard output], or "/dev/stderr" [standard error])
   * awk 'BEGIN {FS=":::"; OFS="\t"} {print $1,$13,$NF,$11}' catalog.txt | grep -Fwf documents.txt - | sort -k2,2nr > jca-final.csv
   * awk "BEGIN {FS=\":::\"; OFS=\"\t\"} {print $1,$13,$NF,$11}" catalog.txt | grep -Fwf documents.txt - | gsort -k2,2nr > jca-final.csv (GnuWin32 version)
   * awk 'BEGIN {FS=":::"; OFS="\t"} {print $1,$13,$NF,$11}' catalog.txt | awk 'NR==FNR {a[$0]; next} $1 in a' documents.txt - | sort -k2,2nr > jca-final.csv
   * awk "BEGIN {FS=\":::\"; OFS=\"\t\"} {print $1,$13,$NF,$11}" catalog.txt | awk "NR==FNR {a[$0]; next} $1 in a" documents.txt - | gsort -k2,2nr > jca-final.csv (GnuWin32 version)
   * awk 'BEGIN {FS=":::"; OFS="\t"} {print $1,$13,$NF,$11}' catalog.txt | awk 'BEGIN{i=0}FNR==NR{a[i++]=$1;next}{for(j=0;j<i;j++)if(index($0,a[j]))print $0}' documents.txt - | sort -k2,2nr > jca-final.csv
   * awk "BEGIN {FS=\":::\"; OFS=\"\t\"} {print $1,$13,$NF,$11}" catalog.txt | awk "BEGIN{i=0}FNR==NR{a[i++]=$1;next}{for(j=0;j<i;j++)if(index($0,a[j]))print $0}" documents.txt - | gsort -k2,2nr > jca-final.csv (GnuWin32 version)
* AWK command to output the values for a certain column in a CSV file -- in this example, the unique Alberta community names from the RSC Alberta libraries CSV file, which are the values in column 11 in a tab-separated file
   * awk 'BEGIN {FS="\t"} {print $11}' alberta.csv | sort | uniq
   * awk "BEGIN {FS=\"\t\"} {print $11}" alberta.csv | sort | uniq (GnuWin32 version)
* AWK command to output the text nodes for a certain element in an XML file -- in this example, the unique Alberta community names from the RSC Alberta libraries XML file, which are the text node values in the element <d:community>
   * grep 'd:community' alberta.xml | awk -F">" '{print $2}' | awk -F"<" '{print $1}' | sort | uniq
* Unix and AWK commands to output the values for a certain key in an array in a JSON file -- in this example, the unique Alberta community names from the RSC Alberta libraries JSON file (array is "items", key is "community", key values are the community place names)
   * grep -Po '"'"community"'"\s*:\s*"\K([^"]*)' alberta.json | sort | uniq
   * grep -Po '(?<="community":")(.*?)(?=",)' alberta.json | sort | uniq
   * awk -F\":\" -v RS="\"," '/community/ {print $2}' alberta.json | sort | uniq (gawk and mawk version)
   * awk 'BEGIN {FS="\":\""; RS="\","} /community/ {print $2}' alberta.json | sort | uniq (gawk and mawk version)
   * awk -F\":\" '/community/ {print $2}' alberta.json | sed 's/\",$//g' | sort | uniq (gawk, mawk, and nawk version)
   * awk 'BEGIN {FS="\":\""} /community/ {print $2}' alberta.json | sed 's/\",$//g' | sort | uniq (gawk, mawk, and nawk version)
* jq (JSON parser utility written in portable C that can be used in Unix chained pipe commands) command to output the values for a certain key in an array in a JSON file -- in this example, the unique Alberta community names from the RSC Alberta libraries JSON file (array is "items", key is "community", key values are the community place names)
   * jq -r ".items[].community" alberta.json | sort | uniq
* jq command to normalize and join (minify) a formatted and indented JSON file into one line
   * jq -c . alberta.json
* Unix command to generate a line frequency count of a document
   * sort input.txt | uniq -c | sort -nr > output.txt
   * gsort input.txt | uniq -c | gsort -nr > output.txt (GnuWin32 version)
* Unix command to generate a frequency distribution for a file consisting of rows of numbers, one number per row
   * sort input.txt | uniq -c | sort -k1,1nr -k2,2n | awk ' { t = $1; $1 = $2; $2 = t; print; } ' > output.txt
   * gsort input.txt | uniq -c | gsort -k1,1nr -k2,2n | awk " { t = $1; $1 = $2; $2 = t; print; } " > output.txt (GnuWin32 version)
* Unix commands to generate a word frequency count of a document
   * cat input.txt | tr -d "[:punct:]" | tr " " "\n" | tr "A-Z" "a-z" | sort | uniq -c | sort -k1,1nr -k2,2 > output.txt
   * cat input.txt | tr -d '[:punct:]' | tr '[A-Z]' '[a-z]' | awk '{for (i=1;i<=NF;i++) print $i;}' | sort | uniq -c |sort -rn -k1 > output.txt
* Unix command to normalize (trim trailing and leading spaces), and join (linearize the XML document into one line, with one space between the joined lines)
   * sed 's/^[ \t]*//;s/[ \t]*$//' input.xml | tr '\n' ' ' > output.xml
* Unix commands to normalize (trim trailing and leading spaces), join (linearize the XML document into one line, with one space between the joined lines), and pretty print (format and indent four spaces)
   * using a temporary variable:
      * TMP_VARIABLE=`sed 's/^[ \t]*//;s/[ \t]*$//' input.xml | tr '\n' ' '`; echo "$TMP_VARIABLE" | XMLLINT_INDENT='    ' xmllint --format --encode utf-8 - > output.xml; unset TMP_VARIABLE
   * using a temporary file:
      * sed 's/^[ \t]*//;s/[ \t]*$//' input.xml | tr '\n' ' ' > tmp_file; XMLLINT_INDENT='    ' xmllint --format --encode utf-8 tmp_file > output.xml; rm tmp_file
* Command Prompt and PowerShell commands to perform arithmetic and other mathematical operations
   * set /a (9900-7700)*100 (can only process and return 32-bit integers)
   * powershell -command "(9900-7700)*100/9900"
   * powershell -command "[System.Math]::Sqrt(64)"
* Unix and AWK commands to perform arithmetic and other mathematical operations
   * bc <<< "scale=1;9900-7700;last*100/9900"
   * echo "9900-7700;.*100/9900" | bc -l
   * echo "(9900-7700)*100/9900" | bc -l
   * gecho "(9900-7700)*100/9900" | bc -l (GnuWin32 version)
   * echo "sqrt(64)" | bc -l
   * gecho "sqrt(64)" | bc -l (GnuWin32 version)
   * awk 'BEGIN {print (9900-7700)*100/9900}'
   * awk 'BEGIN {print sqrt (25)}'
* Unix and Perl commands to generate a comma-separated list of prime numbers from 2 to 100 (a prime number is a whole number greater than 1 whose only factors are 1 and itself)
   * factor {2..100} | awk 'NF==2{print $2}' | tr '\n' ','
   * perl -e'$,=",";print sub { grep { $a=$_; !grep { !($a % $_) } (2..$_-1)} (2..$_[0]) }->(shift)' 100
* AWK command to generate the first 10 numbers of the Fibonacci sequence (the Fibonacci sequence is a discrete formula for exponential growth, and each element in the sequence is constructed by adding the two previous elements together, with the first two elements both defined as "1")
   * awk 'BEGIN {a=1;b=1; while(++x<=10){print a; t=a;a=a+b;b=t}; exit}'
* Command Prompt and PowerShell commands to generate a sequence of numbers from 1 to 1,000,000, one number per row in the file (listed in decreasing order of speed of execution of command, i.e., from fastest to slowest [using Dell machine])
   * cmd /q /c "(for /l %i in (1,1,1000000) do echo %i)" > numbers.txt (14.35)
   * powershell -command "1..1000000" > numbers.txt (200.10)
   * powershell -command "for($i=1; $i -le 1000000; $i=$i+1){$i}" > numbers.txt (212.03)
* Unix and Perl commands to generate a sequence of numbers from 1 to 1,000,000, one number per row in the file (listed in decreasing order of speed of execution of command, i.e., from fastest to slowest [using Mac machine])
   * seq 1 1000000 > numbers.txt (0.361)
   * perl -le 'print for 1..1000000' > numbers.txt (0.383)
   * echo {1..1000000} | tr ' ' '\n' > numbers.txt (2.805)
   * echo 'for (i = 1 ; i <= 1000000 ; ++i) i' | bc > numbers.txt (3.472)
   * printf '%s\n' {1..1000000} > numbers.txt (4.882)
   * for ((i=1; i<=1000000; ++i)); do echo $i; done > numbers.txt (16.763)
   * i=1; while [ $i -le 1000000 ]; do echo $i; i=$((i+1)); done > numbers.txt (25.117)
* Commands for various programs to add up numbers in a file (numbers.txt) consisting of rows of numbers, one number per row
   * awk '{s+=$1} END {print s}' numbers.txt
   * awk "{s+=$1} END {print s}" numbers.txt (Windows version)
   * paste -sd+ numbers.txt | bc (basic calculator, bench calculator)
   * paste -sd+ numbers.txt | sed 's/[eE]+*/*10^/g' | bc (if numbers use scientific notation)
   * (echo 0; sed 's/$/ +/' numbers.txt; echo p) | dc (desk calculator)
   * echo $(( $(cat numbers.txt | tr "\n" "+" ) 0 )) (Bash arithmetic expansion)
   * sum=0;for i in $(cat numbers.txt);do sum=$((sum+$i));done;echo $sum (Bash for loop)
   * powershell -command "(get-content numbers.txt | measure-object -sum).sum"
   * perl -lne '$x += $_; END { print $x; }' numbers.txt
   * perl -lne "$x += $_; END { print $x; }" numbers.txt (Windows version)
   * python -c "import sys; print sum((float(l) for l in sys.stdin))" < numbers.txt
   * php -r '$lines = file("numbers.txt"); print array_sum($lines);'
   * php -r "$lines = file('numbers.txt'); print array_sum($lines);" (Windows version)
   * php -r 'ini_set("memory_limit","-1"); $lines= file("numbers.txt"); print array_sum($lines);' (if need more memory)
   * cat numbers.txt | php -R '@$c+=$argn;' -E 'print($c);' (needs little memory, but is quite slow)
   * ruby -e "puts File.read('numbers.txt').split.inject(0){|mem, obj| mem += obj.to_f}"
   * R -q -e "sum(scan('numbers.txt'))"
   * octave-cli --silent --eval 'x = load("numbers.txt"); format long; sum(x)'
   * octave-cli --silent --eval "x = load('numbers.txt'); format long; sum(x)" (Windows version)
   * C program (uses GNU C getline() extension which is in the GNU C Library, so Windows gcc compilers can’t compile the file as they use only the C standard library [the ISO C library])
      * source file "sum.c"
#include <stdio.h>
#include <stdlib.h>


int main(int argc, char **argv) {
    ssize_t read;
    char *line = NULL;
    size_t len = 0;
    double sum = 0.0;


    while ((read = getline(&line, &len, stdin)) != -1) {
        sum += atof(line);
    }


    printf("%f", sum);
    return 0;
}
      * command to compile the source file "sum.c" into the object file "sum"
         * gcc sum.c -o sum
      * command to run the C program file "sum"
         * ./sum < numbers.txt
   * C program version 2 (uses C standard library, so can compile on Windows gcc compilers)
      * source file "sum2.c"
#include <stdio.h>
#include <stdlib.h>


int main(int argc, char **argv) {
    int a;
    double sum = 0.0;


    while(fscanf(stdin, "%d", &a) == 1) {
        sum += a;
    }


    printf("%f", sum);
    return 0;
}
      * command to compile the source file "sum2.c" into the object file "sum2"
         * gcc sum2.c -o sum2
      * command to run the C program file "sum2"
         * sum2 < numbers.txt
   * Java program
      * source file "SumIntegersFromFile.java"
import java.io.File;
import java.io.IOException;
import java.io.BufferedReader;
import java.io.FileReader;


public class SumIntegersFromFile {


    public static void main(String args[]) throws IOException {


        File file = new File(args[0]);
        BufferedReader br = new BufferedReader( new FileReader(file));
        String line;
        long i=0;
        while((line=br.readLine())!=null) {
            int k = Integer.parseInt(line);
            i+=k;
        }
        br.close();
        System.out.println(i);
    }
}
      * command to compile the source file "SumIntegersFromFile.java" into the class file "SumIntegersFromFile.class"
         * javac SumIntegersFromFile.java
      * command to run the Java program file "SumIntegersFromFile.class"
         * java SumIntegersFromFile numbers.txt
   * speed of above programs on various OSs
      * notes:
         * programs on each operating system machine listed from fastest to slowest
         * only programs that were installed or that could run on a particular machine are listed
         * used time utility on Unix machines, and timecmd.bat, ptime.exe, and TimeMem.exe utilities on Windows machines
            * Note: the Unix time utility writes to standard error, while the Windows timecmd.bat, ptime.exe, and TimeMem.exe utilities write to standard output; therefore if Windows command is redirecting standard output to a file, the command execution time will be written as the last line in the file
            * Note: can run complex summation commands on Unix with the time utility by enclosing the summation command in braces with a space at either end, with a final semicolon at the end of the summation command, e.g.,
               * time { summation command string here; }
            * Note: can run complex summation commands on Windows with the timing utilities by enclosing the summation command in double quotation marks, and preceding any inner double quotation marks by the Windows caret escape character ^, e.g.,
               * timecmd "summation ^"command^" string here"
            * Note: can also run complex summation commands on Windows by including cmd /c after the timing utility command and before the summation command that has double quotation marks around the summation command and carets preceding any inner quotation marks within the summation command, e.g.,
               * timecmd cmd /c "summation ^"command^" string here"
         * data set consisted of numbers from 1 to 1,000,000 (seq 1 1000000 > numbers.txt), one number per row in the file
      * Windows XP (TDO’s netbook): Java, C, Perl, mawk, gawk, Python, PHP, bc, R, Ruby, nawk, dc, Octave, Bash for loop, PowerShell
      * Windows 7 (Dell Precision laptop CWRCLAB04): C, Java, Perl, Python, PHP, Ruby, mawk, gawk, bc, R, nawk, dc, Octave, Bash for loop, PowerShell
      * Windows 7 (Dell Latitude laptop gm-0210072): C, PHP, mawk, Java, Perl, Python, gawk, Ruby, Bash arithmetic expansion, bc, R, Octave, dc, Bash for loop, PowerShell
      * Mac OS 10.9.5 (MacBook Pro laptop cwrcmac02): C, mawk, Java, Perl, gawk, Python, Ruby, PHP, nawk, R, Bash arithmetic expansion, bc, dc, Octave, Bash for loop
      * CentOS 6.7 (cwrc-apps-09.srv.ualberta.ca): C, mawk, Java, gawk, Perl, PHP, Python, bc, Bash arithmetic expansion, dc, Bash for loop
      * Ubuntu 14.04.2 LTS trusty (gpu.srv.ualberta.ca): C, mawk, PHP, Perl, Python, Java, gawk, bc, Bash arithmetic expansion, Ruby, Bash for loop
      * Ubuntu 12.04.5 precise (arrl-web001.artsrn.ualberta.ca): C, mawk, gawk, Perl, Python, Bash arithmetic expansion, PHP, bc, Ruby, Bash for loop
* Commands for various programs to output summary descriptive statistics for a distribution of numbers in a file (numbers.txt) consisting of rows of numbers, one number per row
   * R commands to output the minimum, first quartile, median, mean, third quartile, and maximum -- last command also outputs sample standard deviation
      * R -q -e 'summary(scan("numbers.txt"))'
      * cat numbers.txt | Rscript -e 'print(summary(scan("stdin")));'
      * R -q -e "summary(scan('numbers.txt'))" (Windows version)
      * cat numbers.txt | Rscript -e "print(summary(scan('stdin')));" (Windows version)
      * R -q -e "x <- read.csv('numbers.txt', header = F); summary(x); sd(x[ , 1])"
   * Octave command to return the minimum, first quartile, median, third quartile, maximum, mean, sample standard deviation, skewness, and kurtosis
      * octave-cli --silent --eval 'x = load("numbers.txt"); format long g; statistics(x)'
      * octave-cli --silent --eval "x = load('numbers.txt'); format long g; statistics(x)" (Windows version)
   * PowerShell command to print out the count (N), average (mean), sum, maximum, and minimum
      * powershell -command "Get-Content numbers.txt | Measure-Object -Ave -Sum -Max -Min"
   * Perl command to print out the count (N), sum, average (mean), population standard deviation, median, maximum, and minimum
      * perl -e 'use List::Util qw(max min sum); @a=();while(<>){$sqsum+=$_*$_; push(@a,$_)}; $n=@a;$s=sum(@a);$a=$s/@a;$m=max(@a);$mm=min(@a);$std=sqrt($sqsum/$n-($s/$n)*($s/$n));$mid=int @a/2;@srtd=sort @a;if(@a%2){$med=$srtd[$mid];}else{$med=($srtd[$mid+1]+$srtd[$mid])/2;};print "cnt: $n\nsum: $s\navg: $a\nstd: $std\nmed: $med\nmax: $m\min: $mm";' numbers.txt
   * AWK command to output the count or N
      * awk 'END {print "N = " NR}' numbers.txt
   * AWK command to output the minimum
      * sort -n numbers.txt | head -n1
      * sort -n numbers.txt | awk 'NR==1 {print "minimum = "$0}'
   * AWK command to output the maximum
      * sort -n numbers.txt | tail -n1
      * sort -n numbers.txt | awk 'END {print "maximum = " $0}'
   * AWK command to output the median
      * sort -n numbers.txt | awk ' { a[i++]=$1; } END { x=int((i+1)/2); if (x < (i+1)/2) print (a[x-1]+a[x])/2; else print a[x-1]; }'
      * sort -n numbers.txt | awk ' { a[i++]=$1; } END { x=int((i+1)/2); if (x < (i+1)/2) print "median = " (a[x-1]+a[x])/2; else print "median = " a[x-1]; }'
   * AWK command to output the mean
      * awk '{a+=$1} END{print a/NR}' numbers.txt
      * awk '{mean += $1} END {print "mean = " mean/NR;}' numbers.txt
   * AWK command to output the sample standard deviation
      * cat numbers.txt | awk '{delta = $1 - avg; avg += delta / NR; mean2 += delta * ($1 - avg); } END { print "sample stdev = " sqrt(mean2 / (NR-1)); }'
      * cat numbers.txt | awk "{delta = $1 - avg; avg += delta / NR; mean2 += delta * ($1 - avg); } END { print \"sample stdev = \" sqrt(mean2 / (NR-1)); }" (Windows version)
   * AWK commands to output the population standard deviation
      * awk '{sum+=$1; sumsq+=$1*$1;} END {print "pop stdev = " sqrt(sumsq/NR - (sum/NR)**2);}' numbers.txt
      * awk '{sum+=$1; array[NR]=$1} END {for(x=1;x<=NR;x++){sumsq+=((array[x]-(sum/NR))**2);}print "pop stdev = " sqrt(sumsq/NR)}' numbers.txt
      * cat numbers.txt | awk '{delta = $1 - avg; avg += delta / NR; mean2 += delta * ($1 - avg); } END { print "pop stdev = " sqrt(mean2 / NR); }'
   * AWK command to print out the count (N), minimum, maximum, median, and mean
      * sort -n numbers.txt | awk '{a[i++]=$0;s+=$0}END{print "count: " NR"\n" "minimum: " a[0] "\n" "maximum: " a[i-1] "\n" "median: " (a[int(i/2)]+a[int((i-1)/2)])/2 "\n" "mean: " s/i}'
      * gsort -n numbers.txt | awk "{a[i++]=$0;s+=$0}END{print \"count: \" NR\"\n\" \"minimum: \" a[0] \"\n\" \"maximum: \" a[i-1] \"\n\" \"median: \" (a[int(i/2)]+a[int((i-1)/2)])/2 \"\n\" \"mean: \" s/i}" (GnuWin32 version)
* Redirection of stdin (fd 0), stdout (fd 1), and stderr (fd 2)
   * redirect stderr to /dev/null (Unix) or NUL (Windows)
      * ./script 2>/dev/null
      * .\script 2>NUL
   * redirect both stdout and stderr to /dev/null (Unix) or NUL (Windows)
      * ./script >/dev/null 2>&1
      * .\script >NUL 2>&1
   * redirect stdout to /dev/null (Unix) or NUL (Windows) and stderr to stdout
      * ./script 2>&1 > /dev/null
      * .\script 2>&1 > NUL
* Common commands between Unix and Windows
   * the Unix and Windows find and sort commands have different arguments (flags or options), therefore need to give the full path to the Unix version of these commands when running Unix commands under Windows (if the Windows versions come first in the Windows PATH environment variable)
   * other common commands (in addition to find and sort) include date, echo, mkdir, and rmdir
   * can use findunix.exe (copy of the UnxUtils find.exe utility) and gsort.exe (GnuWin32 clone of the sort.exe utility) to specify the Unix versions of these commands (also have GnuWin32 Unix commands of gdate.exe, gecho.exe, gmkdir.exe, and grmdir.exe)
      * made copy of UnxUtils find.exe called findunix.exe, because the GnuWin32 and win-bash versions of find.exe don't work under Windows if using wildcards in the expression
   * if running a Unix command for loop in Windows, make sure to run the command from the Bash command prompt using the Octave bash utility (this is a much more recent and more fully featured version of Bash than the win-bash version); also make sure to use gecho rather than echo if echo is used in the loop
   * finally, if the Unix command is enclosed in single quotation marks, then these single quotation marks need to be changed to double quotation marks in the Windows version of the command, and any internal double quotation marks in the command need to be escaped with the backslash character, e.g.,
      * awk '{$NF=""; print $0}' filename.txt (Unix version)
      * awk "{$NF=\"\"; print $0}" filename.txt (Windows version)
* Sources for Unix utilities and scripting languages for Windows machines
   * Unix utilities, including awk (gawk/nawk/mawk)
      * GnuWin32
      * UnxUtils
      * Win-bash (includes bash)
      * GNU Octave (build tools include newer versions of Unix utilities [bash, awk, grep, sed, etc.], plus various compilers such as gcc, g++, gfortran, and the build tool GNU Make)
      * Subsystem for Unix-based Applications (SUA), with Utilities and SDK for Subsystem for Unix-based Applications (no longer supported -- superseded by WSL)
      * Cygwin (provides a Unix-like environment, with Unix utilities and programming languages of your choosing, such as AWK, Perl, Python, PHP, and Ruby)
      * MinGW (Minimalist GNU for Windows) (contains MSYS [minimal system] which provides a lightweight Unix shell environment)
      * Git for Windows (includes bash emulation and a variety of Unix utilities)
      * Windows Subsystem for Linux (WSL) (need 64-bit edition of Windows 10; runs a Linux Ubuntu image)
      * BusyBox (busybox-w32 is a port of BusyBox to Windows that contains many Unix utilities in a single binary)
      * MobaXterm (terminal application for Windows that includes various Unix utilities [uses BusyBox])
      * Oracle VM VirtualBox Manager (hypervisor or virtualization product that allows one to run a guest OS such as Linux, BSD, Solaris, etc. -- would then have access to Unix utilities and various programming languages)
   * Scripting languages
      * GnuWin32, UnxUtils, win-bash, and GNU Octave (all contain AWK)
      * XAMPP (contains Perl and PHP)
      * Strawberry Perl (contains Perl; also contains gcc compiler plus other tools)
      * Cygwin (contains AWK, Perl, Python, PHP, and Ruby)


Commands to archive, compress, and uncompress files


* Commands to tar and untar files
   * to combine multiple files into a single archive tar file (tarball)
      * tar -cvf filename.tar *.xml
   * to separate an archive file into individual files
      * tar -xvf filename.tar
* Commands to compress and uncompress tar files (archive then compress to GZIP format or Z format); note: the extensions .tgz and .tar.gz are equivalent (they both signify a tar file zipped with gzip)
   * Using GNU tar (which incorporates gzip)
      * compress (zip) the tar archive file
         * tar -cvzf filename.tar.gz *.xml
      * uncompress (unzip) the tar archive file
         * tar -xvzf filename.tar.gz
      * uncompress (unzip) the .bz2 tar archive file
         * tar -xvjf filename.tar.bz2
   * Using gzip command
      * compress (zip) the tar archive file
         * tar -cvf - *.xml | gzip > filename.tar.gz
      * uncompress (unzip) the tar archive file
         * gunzip -c filename.tar.gz | tar -xvf -
   * Using compress command
      * compress (zip) the tar archive file
         * tar -cvf - *.xml | compress > filename.tar.Z
      * uncompress (unzip) the tar archive file
         * uncompress -c filename.tar.Z | tar -xvf -
   * To view the contents of a .gz file
      * zcat filename.tar.gz
* Commands to zip and unzip files (compress then archive to ZIP format)
   * Using the 7-Zip command (can use either the "7z" command or the "7za" command)
      * zip files
         * 7z a -tzip filename.zip *.xml
      * unzip files
         * 7z e filename.zip -odata
   * Using the zip command
      * zip files
         * zip filename.zip *.xml
         * zip -r filename.zip /data
      * unzip files
         * unzip filename.zip
   * To list the contents of a ZIP archive
      * unzip -l filename.zip
   * To write the contents of a named file to standard out (the screen) without having to uncompress the entire archive
      * unzip -c filename.zip file1.xml > file1.xml
* Command to convert tar files to zip files
   * for f in *.tgz; do rm -rf ${f%.tgz}; mkdir ${f%.tgz}; tar -C ${f%.tgz} -zxvf $f; 7z a -tzip $f.zip ${f%.tgz}; rm -rf ${f%.tgz}; done
* Command to uncompress a bzip2 file that has not been tarred
   * Convert the original archive file into an uncompressed file
      * bzip2 -d filename.bz2
   * Preserve the original archive file by creating a new uncompressed file
      * bzip2 -dk filename.bz2
   * Convert a directory of bzip2 files into uncompressed files
      * bzip2 -d *.bz2


Commands to create and unpack JAR (Java Archive) files


* Command to create a JAR file
   * jar -cf filename.jar *.class
* Command to create a JAR file with packages (e.g., Java classes are in the package mycode.games.CrosswordSolver)
   * jar -cf filename.jar ./mycode/games/CrosswordSolver/*.class
* Command to extract or unpack a JAR file
   * jar -xf filename.jar


Commands to delete a leftover Windows Update directory


* Go to the Command Prompt as Administrator, and run the following commands sequentially to remove the directory that can’t be deleted (called "directory_name" in this example)
   * md empty
   * robocopy /e /purge /b empty directory_name
   * takeown /F directory_name
   * icacls directory_name /grant administrators:F
   * rd directory_name
   * rd empty


Software
Databases
                MySQL
CanWWR MySQL database


* MySQL is written in C and C++
* URL to the Web site        
   * http://canwwrfrom1950.org/
* URL to the database admin interface
   * http://db.canwwrfrom1950.org/
* New database user accounts added
   * berban (lowercase godhead squared)
   * mbrundin (lowercase godhead null)
* SSH in to the Web server arrl-web001.artsrn.ualberta.ca in order to access the database server arrl-db001.artsrn.ualberta.ca
   * use PuTTY to connect to arrl-web001.artsrn.ualberta.ca
* Command to connect to the database server arrl-db001.artsrn.ualberta.ca and the CanWWR MySQL database canwwrfrom1950_production as the user dhrsuser from the Web server arrl-web001.artsrn.ualberta.ca
   * mysql -u dhrsuser -p -h arrl-db001.artsrn.ualberta.ca   canwwrfrom1950_production
   * password hint: digital collaboratory lab (the capital of money will lower you)
* Some useful MySQL commands to administer the canwwrfrom1950_production database
   * MySQL command to generate an SHA hash for a password string (example of "frodo" -- SHA hash produces a 40-character alphanumeric hash string)
      * SELECT sha ('frodo');
   * MySQL command to insert a new user name and password into the users table (example of username "mbrundin" and hashed password string for "frodo")
      * INSERT INTO users (username,hashed_password) VALUES ('mbrundin','d157e537044e1ff674045d4929f089aa71f99c77');
   * MySQL command to delete a user's user name from the users table (example of "mbrundin")
      * DELETE FROM users WHERE username='mbrundin';
   * MySQL command to change a user's user name in the users table (example from "mbrundin2" to "mbrundin")
      * UPDATE users SET username='mbrundin' WHERE username='mbrundin2';
   * MySQL command to see the tables in the canwwrfrom1950_production database
      * SHOW TABLES;
   * MySQL command to see the columns (fields) formats in the users table
      * DESCRIBE users;
   * MySQL command to see the columns (fields) in the users table
      * SHOW COLUMNS FROM users;
   * MySQL command to see all the rows (records) in the users table
      * SELECT * FROM users;
   * MySQL command to see all the rows (records) in the users table sorted in ascending order by username
      * SELECT * FROM users ORDER BY username ASC;
   * MySQL command to see the row (record) in the users table where the username is "mbrundin"
      * SELECT * FROM users WHERE username='mbrundin';
   * MySQL command to see the rows (records) in the users table where username starts with "b"
      * SELECT * FROM users WHERE USERNAME LIKE "b%";
   * MySQL command to count the number of rows (records) in the users table
      * SELECT COUNT(*) FROM users;


GeoNames MySQL database


* location of the GeoNames MySQL server
   * cwrc-apps-07.srv.ualberta.ca (apps.testing.cwrc.ca)
* location of the the Plot-It (mapping and timeline application) directory on the Web server
   * /data/opt/cwrc/CWRC-MTP
* Command to connect to the MySQL database server on cwrc-apps-07.srv.ualberta.ca and the GeoNames MySQL database cwrc_time_mapping_db as the user cwrc_time
   * mysql -u cwrc_time -p cwrc_time_mapping_db
   * password hint: config file
* MySQL command to delete the places table
   * DROP TABLE places;
* MySQL statement to create the places table
   * CREATE TABLE places (
        geonameid int(11) DEFAULT NULL,`name` varchar(200) DEFAULT NULL,
        asciiname varchar(200) DEFAULT NULL,
        alternatenames varchar(5000) DEFAULT NULL,
        latitude decimal(10,7) DEFAULT NULL,
        longitude decimal(10,7) DEFAULT NULL,
        feature_class char(1) DEFAULT NULL,
        feature_code varchar(10) DEFAULT NULL,
        country_code char(2) DEFAULT NULL,
        cc2 char(60) DEFAULT NULL,
        admin1_code varchar(20),
        admin2_code varchar(80),
        admin3_code varchar(20),
        admin4_code varchar(20),
        population bigint(20),
        elevation int(11),
        dem int(11),
        timezone varchar(100) DEFAULT NULL,
        modification_date date DEFAULT NULL,
        UNIQUE KEY geonameid (geonameid),
        KEY `name` (`name`),
        KEY asciiname (asciiname),
        FULLTEXT KEY name_2 (`name`),
        FULLTEXT KEY asciiname_2 (asciiname)
);
* MySQL statement to import the full downloaded GeoNames database places.txt from my home directory into the MySQL cwrc_time_mapping_db database's places table
   * LOAD DATA LOCAL INFILE '/data/home/brundin/places.txt' INTO TABLE places;


drupal7 MySQL database


* Some useful commands to administer the MySQL database drupal7 on cwrc-dev-01.srv.ualberta.ca
   * Command to connect to the database drupal7 as the user drupal -- see /var/www/html/drupal7/sites/default/settings.php for MySQL connection information
      * mysql -u drupal -p -h localhost drupal7
   * Command to perform a database dump backup of the drupal7 database
      * mysql -u root -p --databases drupal7 > /home/brundin/drupal7-dump.sql
   * Command to restore the drupal7 database from a MySQL database dump SQL file
      * mysql -u root -p drupal7 < /var/lib/mysql.backup/dmp/tmp/drupal7.sql
   * MySQL command to output to a file the MySQL table "xml_forms"
      * SELECT * FROM xml_forms INTO OUTFILE '/home/brundin/xml_forms.csv' FIELDS ENCLOSED BY '"' TERMINATED BY ';' ESCAPED BY '"' LINES TERMINATED BY '\n';
   * Using the MySQL command above to create an SQL file, command to output to a file the MySQL table "xml_forms"
      * mysql -u drupal -p -h localhost drupal7 < my_requests.sql > xml_forms.csv
   * If for the two commands above are getting permission problems in trying to write out to a file the table data, then use this command to output to a file the MySQL table "xml_forms"
      * echo "SELECT * FROM xml_forms" | mysql -u drupal -p -h localhost drupal7 > xml_forms.txt
   * Command to pipe the output through the "less" command line tool, so when you run the SELECT command on the MySQL table "xml_forms" the output can be paged through
      * pager less -SFX
      * SELECT * FROM xml_forms;
   * Command to reset the pager to stdout
      * nopager
   * Command to output the MySQL table "xml_forms" in vertical mode, so that each column value is printed on a new line
      * SELECT * FROM xml_forms\G
* Procedure to find and re-ingest objects that were not ingested in a batch ingest
   * Log into the drupal7 database
   * To locate the object or objects where the item state is an "Error" (i.e., "-1"), as opposed to "Ingested" (i.e., "3"), run this MySQL command to select all objects with an item state of "-1" in the table islandora_batch_state
      * SELECT * FROM islandora_batch_state WHERE state='-1';
   * Record the PID of the object or objects with a state of "-1"
   * Then run the Report "Islandora Batch Sets", and then select "View items in set" for the set that has some objects that were not successfully ingested; click on the "ID" label at the top to sort all of the objects by their PIDs
   * Insert a page number in the appropriate place in the GET method URL string in the location bar to navigate to the likely region/area where the object PID of interest would be; modify the page number until the page with the object is located; this object will have an "ITEM STATE" value of "Error", as opposed to "Ingested"
   * Click on the "Set item state" for this object, and change the value from "Error" to "Ready to ingest"; click on the "Confirm" button
   * Now click on the "Process Set" link at the top of the page to reprocess the set; click on the "Start batch processing" button to initiate the reprocessing of the batch set
   * The object or objects that had an "Error" status should now be successfully ingested


PostgreSQL


* PostgreSQL (Postgres) -- object-relational database management system written in C
* PostgreSQL tutorial: http://www.postgresql.org/docs/8.0/static/tutorial.html
* Notes on how to query the Document Archive to retrieve freestanding events based on their EID (event identification) numbers
   * need to preprocess the list of freestanding events that Isobel compiled
      * events looked like this
         * 6808 (place is London, England)
39600 (place is Ames, Iowa)
26946 (place probably tho not certainly Harvard, i.e. Cambridge, MA)
      * ran this AWK command to just get the first column or field
         * awk "{print $1}" events_original.txt > events_eids.txt
      * resulting in a file that looked like this
         * 6808
39600
26946
      * then used Notepad++ to do regular expression search and replace to add " OR eid = ", i.e., search for "^" and replace with " OR eid = "; resulted in this
         *  OR eid = 6808
         *  OR eid = 39600
         *  OR eid = 26946
      * then added this statement before the first line "SELECT xml FROM events WHERE", and changed the old first line from " OR eid = 6808" to "eid = 6808", and  finally I added a semicolon after the last line -- a fragment of the resulting formulated SELECT query appears later below
   * log in to the cwrc-apps-08.srv.ualberta.ca server (note: I don't have access to this server -- need to get Jeff to log in)
   * note: \? is to get help, and \h is help for SQL commands, e.g., DROP, SELECT, INSERT, etc.
   * run the following command at the Unix command prompt to log in to the PostGreSQL database
      * psql -U orlando -d Delivery_Internal
   * run command to direct where to write the output from the SELECT command
      * \o /tmp/freestanding_events-2015-03-02.xml
   * run a SELECT query on the database to extract the freestanding events based on their EID numbers
      * SELECT xml FROM events WHERE
eid = 6808
 OR eid = 39600
 OR eid = 26946
        ;
   * type the following command to quit the PostGreSQL database
      * \q
   * copy the database output file from the tmp directory to a directory on the cwrc GPU volume -- run the sftp command from the /tmp directory on cwrc-apps-08.srv.ualberta.ca
      * sftp brundin@gpu.srv.ualberta.ca
      * put freestanding_events-2015-03-02.xml "/u/b/r/brundin/cwrc/SHARING/project data/new projects"
* Various SQL queries
   * SQL query to retrieve the filename and attribute value for the attribute @PROPOSEDALTERNATIVENAME in the element <TGENRE>
      * Delivery_Internal=> select old_filename, value from xhtml_tag_type xtt, xhtml_writings_tag xwt, xhtml_writings_attribute xwa where xtt.type_id = xwt.type_id and xwa.t_id = xwt.t_id and tag = 'TGENRE' and xwa.attribute='PROPOSEDALTERNATIVENAME' ;


BaseX


* BaseX -- XML database engine and XQuery/XPath processor written in Java (implements XPath 3.0, XSLT 2.0, and XQuery 3.0)
* BaseX available on Dell Precision laptop CWRCLAB04, MacBook Pro laptop cwrcmac02, and cwrc-apps-07.srv.ualberta.ca
   * BaseX Standalone Mode can be used to run XQuery expressions or run database commands on the command line -- there are Unix, Windows, and Java JAR file versions
      * The Standalone version console mode can be launched or started as follows:
         * basex
         * basex.bat
         * java -cp BaseX.jar org.basex.BaseX
      * The available Standalone command-line flags can be listed with "-h":
         * basex -h
         * basex.bat -h
         * java -cp BaseX.jar org.basex.BaseX -h
   * BaseX Standalone command to do an XPath query and return the names of all elements in an XML file
      * basex -i filename.xml -q "//*/name()" | sort | uniq > elements.txt
   * BaseX Standalone command to do an XPath query and return the names of all attributes in an XML file
      * basex -i filename.xml -q "//@*/name()" | sort | uniq > attributes.txt
   * BaseX Standalone command to run an XQuery
      * basex -q "1+1"
      * basex -i input_file.xml -o output_file.xml xquery_file.xquery
   * BaseX Standalone commands to run an XQuery using the BaseX JAR file (so all the commands above can be run using the BaseX.jar JAR file by substituting "java -cp BaseX.jar org.basex.BaseX" for "basex" in the commands above)
      * java -cp BaseX.jar org.basex.BaseX -q "1+1"
      * java -cp BaseX.jar org.basex.BaseX -i input_file.xml -o output_file.xml xquery_file.xquery
   * BaseX Standalone command to run an XSLT file using the BaseX JAR file (note: the Xalan processor doesn’t seem to be working properly in the BaseX.jar JAR file, so have both the Xalan (xalan.jar and serializer.jar) as well as Saxon-HE (saxon9he.jar) processors in the "lib" directory referenced in the class path; the two XSLT functions are "xslt:transform" [returns nodes] and "xslt:transform-text" [returns text] -- "xslt:transform-text" returns XML declaration statement)
      * java -cp BaseX.jar:./lib/* org.basex.BaseX "xslt:transform-text('mods-test.xml', 'cleanup_mods.xsl')"
      * To see the XSLT processor:
         * java -cp BaseX.jar:./lib/* org.basex.BaseX "xslt:processor()"
      * To see the XSLT version:
         * java -cp BaseX.jar:./lib/* org.basex.BaseX "xslt:version()"
   * GUI version command to define the namespace for an XQuery
      * declare namespace _="http://www.loc.gov/mods/v3";
                      //_:title[text()='December Days']
      * declare namespace mods="http://www.loc.gov/mods/v3";
           //mods:title[text()='December Days']


        Spreadsheets
                Microsoft Excel


* Microsoft Excel -- spreadsheet application for Microsoft Windows and Mac OS X written in C++; features calculation, graphing tools, pivot tables, and the VBA macro programming language
* Steps to find multicolumn duplicates
   * Using formulas
      * Copy the file you are interested in checking for duplicate entries, and use this copy for your analysis
      * Concatenate the columns you are checking; assuming there is a header row, and you are using the first two columns for the comparison, create a new column, label it "Names", and then enter the formula


=A2&" "&B2


in cell C2 (assuming the new column you created is the third column "C"); copy this formula all the way down column C (the "Names" column)
      * Next, create another column next to the column you just created (e.g., column "D"), label it "Duplicate_check", and in cell D2 enter the formula
        
        =IF(COUNTIF($C$2:C2,C2)>1, "Duplicate","Original")
        
        Copy this formula all the way down column D (the "Duplicate_check" 
column)
      * Highlight column D (the "Duplicate_check" column) and copy it, and then select Paste > Paste Special, and then select the radio button "Values", and then click OK.
      * Highlight column D again, click Ctrl-f, and in the "Find and Replace" dialog box, type "Duplicate in the "Find what:" textbox, and then click on the "Find All" button
      * The records that are duplicates of the two columns that you are checking will be returned, with the value "Duplicate" in column D (the "Duplicate_check" column)
   * Using a count column and pivot table
      * Copy the file you are interested in checking for duplicate entries, and use this copy for your analysis
      * Concatenate the columns you are checking; assuming there is a header row, and you are using the first two columns for the comparison, create a new column, label it "Names", and then enter the formula


=A2&" "&B2


in cell C2 (assuming the new column you created is the third column "C"); copy this formula all the way down column C (the "Names" column)
      * Next, create another column next to the column you created (e.g., column "D"), label it "Counts", and in cell D2 enter the value "1" and copy this value all the way down column D (the "Counts" column)
      * Highlight both column C (the "Names" column) and column D (the "Count" column), click on Insert > PivotTable, and then click on OK
      * In the resulting PivotTable, click the checkbox for the "Names" and "Count" fields in the PivotTable Field List in the top right, which will add both of these fields to the Row Labels column in the bottom right; drag the "Count" field from the Row Labels column to the Values column to the right of it
      * Click on the cell immediately below the "Count of Count" header cell, then under PivotTable Tools > Options, select sort Z->A to display the rows that have counts greater than "1" -- these are the rows with duplicate columns
* Formula to sum the positive numbers in a column
   * =SUMIF(C2:C132,">0")
* Formula to sum the negative numbers in a column
   * =SUMIF(C2:C132,"<0")
* Formulae to sum the numbers in one column based on the value in another column; the column to be summed is column C, and the string to be matched is in column B, with the first formula using a multi-character wildcard (all strings beginning with "Billing") and the second formula matching on the entire field value
   * =SUMIF(B:B,"Billing*",C:C)
   * =SUMIF(B:B,"Payment - Thank you",C:C)


Text editors
                Terminal text editors
Vim


* Vim/gVim -- full-screen text editor (based on the vi editor) written in C and Vim script
   * Open file: vim file_name
   * Two modes: input mode (a or i), and command mode (ESC [M-])
      * Within command mode there is submode called colon mode (:)
        Command mode
   * Move to the previous line: k
   * Move to the next line: j
   * Move forward one character: l
   * Move backwards one character: h
   * Go to the beginning of the current line: 0
   * Go to the beginning of the next line: +
   * Go to the beginning of the previous line: -
   * Go to the end of the current line: $
   * Go forward one full screen: ^f
   * Go forward one half screen: ^d
   * Go backwards one full screen: ^b
   * Go backwards one half screen: ^u
   * Go backwards one word: b
   * Go forward one word: w
   * Go to the beginning of the buffer: 1G or [[
   * Go to the end of the buffer: G or ]]
   * Move to the beginning of the paragraph: {
   * Move to the end of the paragraph: }
   * Delete the character under the cursor: x
   * Delete entire line: dd
   * Delete two lines: 2dd
   * Delete everything from the cursor to the end of the line: D
   * Delete from the cursor to the end of the word: dw
   * Undo previous action: u
   * Redo changes that were undone: ^r
   * Copy line where cursor is: yy
   * Copy to the right of the cursor: y$
   * Copy to the left of the cursor: y0
   * Paste to the right of the cursor: p
   * Paste to the left of the cursor: P
   * Cut (delete) and paste, and copy (yank) and paste
      * Position the cursor where you want to begin cutting
      * Press v to select characters, or uppercase V to select whole lines, or Ctrl-v to select rectangular blocks
      * Move the cursor to the end of what you want to cut
      * Press d to cut, or y to copy
      * Move to where you would like to paste
      * Press P to paste before the cursor, or p to paste after
   * Indent document: gg=G
   * Display number of lines words, and characters (bytes): g then ^g
   * Save file and quit: ZZ
        Command mode >> Colon mode
   * Open help menu: :help
   * Save file: :w
   * Save to another file: :w file_name
   * Open a file: :vi file_name
   * Insert file: :r file_name
   * Insert file read-only: :view file_name
   * Quit file: :q
   * Save file and quit: :x or :wq
   * Quit without saving: :q!
   * Search for text: :/
   * Search and replace all occurrences in the document: :%s/FindMe/ReplaceMe/g
   * Search and replace first occurrence on each line: :%s/FindMe/ReplaceMe/
   * Search and replace all occurrences from line 10 to line 50: :10,50s/FindMe/ReplaceMe/g
   * Count number of occurrences of a pattern: :%s/pattern//gn
   * Browse file directory: :n .
   * Display line numbers: :set number or :set nu
   * Hide line numbers: :set nonumber or :set nonu
   * Display current row and column number position of cursor (status bar): set ruler
   * Hide current row and column number position of cursor (status bar) set noruler
   * Display hidden (invisible) characters: :set list
   * Hide hidden (invisible) characters: :set nolist
   * Turn off auto-indent when pasting code: :set paste
   * Turn off paste mode and turn auto-indenting back on: :set nopaste
   * Turn on syntax highlighting: syn on
   * Turn off syntax highlighting: syn off
   * Turn on search highlighting: :set hlsearch
   * Turn off search highlighting until the next search: :noh
   * Turn off search highlighting for entire vim session: :set nohlsearch
   * Turn on spell-checking: :set spell
      * move to the next misspelled word: ]s
      * move to the previous misspelled word: [s
      * view spelling suggestions for a misspelled word: z=
   * Turn off spell-checking: :set nospell
   * Display current character encoding: :set encoding? or :set enc
   * Set the character encoding to UTF-8: :set encoding=utf-8 | set fileencoding=utf-8
   * Display the file format option: :set ff? ffs? [[ffs=unix,dos --> Unix; ffs=dos,unix --> Windows]
   * Display current file format: :set fileformat? or :set ff?
   * Change file format from Windows/DOS (CRLF) to Unix (LF): set fileformat=unix | update 
   * Change file format from Unix to Windows/DOS: set fileformat=dos | update
   * Change line endings from mixture of CRLF and LF to Unix LF file format: :update | e ++ff=dos | setlocal ff=unix | w
   * Change line endings from mixture of CRLF and LF to Windows CRLF file format: :update | e ++ff=dos | w
   * Display current file type: :set filetype? or :set ft
   * Set file type as XML: :set filetype=xml or :setf xml
   * Set code indentation of four spaces: :set smartindent | set tabstop=4 | set shiftwidth=4 | set expandtab
   * Suspend running Vim and get to shell: Ctrl-z
   * Return from shell back to Vim: fg
   * To start a new shell from Vim: :sh or :!bash
   * To kill the shell and return to Vim: Ctrl-d or exit
   * Apply shell command to buffer text: :% !shell-command
   * Write buffer text to shell command: :%w !shell-command
   * Format and indent (pretty print) XML
      * :% !xmllint --format --encode utf-8 -
      * :% !XMLLINT_INDENT='        ' xmllint --format --encode utf-8 -
      * (made alias command XMLFORMAT)
   * Check if XML is well formed
      * :%w !xmllint --noout -
      * :%w !xmllint --noout -; if [ $? -eq 0 ]; then echo "Well formed"; else echo "Not well formed"; fi
      * :%w !xmllint --noout % & if ERRORLEVEL 1 (echo Not well formed) else (echo Well formed)
      * (made alias command XMLWELLFORMED)
   * Validate DC XML record
      * :%w !xmllint --noout --schema http://www.openarchives.org/OAI/2.0/oai_dc.xsd -
      * (made alias command XMLVALIDATEDC)
   * Validate MODS XML record
      * :%w !xmllint --noout --schema http://www.loc.gov/standards/mods/mods.xsd -
      * (made alias command XMLVALIDATEMODS)
   * Validate TEI XML record
      * :%w !java -jar "C:\Program Files (x86)\jing\bin\jing.jar" -t -e utf-8 https://www.tei-c.org/release/xml/tei/custom/schema/relaxng/tei_all.rng %
      * :%w !java -jar "/Users/brundin/Applications/Jing/bin/jing.jar" -t -e utf-8 https://www.tei-c.org/release/xml/tei/custom/schema/relaxng/tei_all.rng %; if [ $? -eq 0 ]; then echo "Validated"; else echo "Not validated"; fi
      * : %w !java -jar "C:\Program Files (x86)\Jing\bin\jing.jar" -t -e utf-8 https://www.tei-c.org/release/xml/tei/custom/schema/relaxng/tei_all.rng % & if ERRORLEVEL 1 (echo Not validated) else (echo Validated)
      * (made alias command XMLVALIDATETEI)
nano


* GNU nano -- full-screen text editor (clone of the Pico editor) written in C
   * Open file: nano file_name
   * Go to end of file: ^w and then ^v; or M-/
   * Go to beginning of file: ^w and then ^y; or M-\
   * Save file: ^o
   * Insert text file: ^r
   * Browse for a file: ^r and then ^t
   * Justify a paragraph: ^j
   * Copy text: M-^
   * Cut text: ^k
   * Paste text: ^u
   * Mark text: ^^ or ^6 or M-a
   * Copy and paste: ^^ to mark start of text
   * Cut and paste: ^^ to mark start of text, ^k to cut text, and ^u to paste text
   * Exit file: ^x
   * Search for text: ^w
   * Continue the search: M-w
   * Search and replace: ^\; or M-r
   * Move forward a page: ^v
   * Move backward a page: ^y
   * Spell check the document: ^t
   * Help menu: ^g
   * Move forward a character: ^f
   * Move back a character: ^b
   * Move to next line: ^n
   * Move to previous line: ^p
   * Move to the beginning of the line: ^a
   * Move to the end of the line: ^e
   * Refresh the display: ^l
   * Delete character: ^d
   * Mark cursor position as beginning of selected text: ^^
   * Insert tab: ^i
   * Report current cursor position: ^c
   * Justify the entire file: M-j [Alt-j or Esc-j]
   * Count the number of words, lines, and characters: M-d [Alt-d or Esc-d]
   * Toggle continuous current line/column display on and off: M-c [Alt-c or Esc-c]
   * Toggle syntax highlighting on and off: M-y [Alt-y or Esc-y]
   * Jump to a specific line number: M-g
   * Undo last operation: ^u
   * Redo the last undone operation: ^e
   * Turn on multiple buffers: M-f [Alt-f or Esc-f]
   * Navigate between open buffers: M-< [M-,] and M-> [M-.]
Emacs


* GNU Emacs -- full-screen text editor [^ is CTRL, M is ESC] written in C and Emacs Lisp
   * Open file: emacs file_name
   * Move to the previous line: ^p
   * Move to the next line: ^n
   * Move forward one character: ^f
   * Move backwards one character: ^b
   * Go to the beginning of the line: ^a
   * Go to the end of the line: ^e
   * Go forward one screen: ^v
   * Go backwards one screen: M-v
   * Go backwards one word: M-b
   * Go forward one word: M-f
   * Go to the beginning of the buffer: M-<
   * Go to the end of the buffer: M->
   * Move to the beginning of the paragraph: M-{
   * Move to the end of the paragraph: M-}
   * Delete the character to the left of the cursor: DEL
   * Delete the character to the right of the cursor: ^d
   * Delete everything from the point to the end of the line: ^k
   * Delete the line, only if the cursor is at the beginning of the line: ^k ^k
   * Delete the word to the left of the point: M-DEL
   * Retrieve deleted text: ^y
   * Mark one end of a region: ^space
   * Cut text: ^w
   * Copy text: M-w
   * Paste (or yank) text: ^y
   * Search for pattern: ^s
   * Search and replace: M-x
   * Save file: ^x ^s
   * Save to another file: ^x ^w
   * Find a file: ^x ^f
   * Insert a file: ^x i
   * Find alternate file: ^x ^v
   * Find file read-only: ^x ^r
   * Split the buffer the cursor is currently in into two: ^x 2
   * Close all buffers except for the one the cursor is currently in: ^x 1
   * Close the buffer the cursor is currently in: ^x 0
   * Move the cursor to a different buffer: ^x o
   * Exit Emacs: ^x ^c
   * Save changes and quit Emacs: ^c ^c
   * Abort the current command: ^g
   * Refresh the screen and center text vertically around the cursor: ^l
   * Repeat keyboard character or Emacs command [X] n times, i.e., Repeat X n times: M-n-X
   * Create a blank line: ^o
   * Undo changes: ^_
   * Play Tetris: M-x tetris (from command line can also type "emacs -q --no-splash -f tetris")
   * Play Pong: M-x pong -- Player 1: Left arrow and Right arrow, or number 4 and number 6; Player 2: Up arrow and Down arrow, or number 2 and number 8 (from command line can also type "emacs -q --no-splash -f pong")
   * Directory location to see what games are available: /usr/share/emacs/24.3/lisp/play


Notepad++


* Notepad++ -- text editor and source code editor for Microsoft Windows written in C++
   * Run menu
      * To create a permanent Run command
         * Click on the Run menu item
         * Type in command, then click on Save, then type in a name for the command, and then click on Save again
      * Format and indent XML (XMLStarlet)
         * cmd /c cd /d  "$(CURRENT_DIRECTORY)" && xml fo --indent-spaces 4 "$(FULL_CURRENT_PATH)" > temp.xml && copy temp.xml "$(FULL_CURRENT_PATH)" && del temp.xml
      * Validate entity (xmllint)
         * cmd /k "cd /d "$(CURRENT_DIRECTORY)" && xmllint --noout --relaxng "http://cwrc.ca/schemas/entities.rng" "$(FULL_CURRENT_PATH)" 2>&1 & pause & exit"
      * Validate entity (XMLStarlet)
         * cmd /k "cd /d "$(CURRENT_DIRECTORY)" && xml val --net --err --relaxng "http://cwrc.ca/schemas/entities.rng" "$(FULL_CURRENT_PATH)" 2>&1 & pause & exit"
      * Validate MODS (xmllint)
         * cmd /k "cd /d "$(CURRENT_DIRECTORY)" && xmllint --noout --schema "http://www.loc.gov/standards/mods/mods.xsd" "$(FULL_CURRENT_PATH)" 2>&1 & pause & exit"
      * Validate MODS (XMLStarlet)
         * cmd /k "cd /d "$(CURRENT_DIRECTORY)" && xml val --net --err --xsd "http://www.loc.gov/standards/mods/mods.xsd" "$(FULL_CURRENT_PATH)" 2>&1 & pause & exit"
   * Macro menu
      * To create a Macro
         * Use the NPPExec plugin, go to Execute..., and select <temporary script>
         * Type in script, and then Save, then type in a name for the script, and then select Save again 
         * Then go to Advanced Options in NPPExec, and then select "Associated script" in the bottom left, then select the script you created, and then click on the Add/Modify button
         * Use the "Move up" button if you want to move the script higher up in the list
         * Check the "Place to the Macros submenu" checkbox
         * The script will now be available from the Macro menu
      * Format and indent JavaScript (Python)
         * cd  "C:\Program Files (x86)\scripts"
python jsbeautifier.py "$(FULL_CURRENT_PATH)"
      * Format and indent JSON (PHP)
         * cd  "C:\Program Files (x86)\scripts"
cmd /c type "$(FULL_CURRENT_PATH)" | php json_pp.php
      * Format and indent JSON (Python)
         * cd  "C:\Program Files (x86)\scripts"
python json_pp.py "$(FULL_CURRENT_PATH)"
      * Format and indent XML (Tidy)
         * cd /d "$(CURRENT_DIRECTORY)"
tidy -xml -utf8 --add-xml-decl yes -indent --indent-spaces 4 -quiet "$(FULL_CURRENT_PATH)"
      * Format and indent XML (xmllint)
         * cd /d "$(CURRENT_DIRECTORY)"
cmd /c set XMLLINT_INDENT=    & xmllint --format --encode utf-8 "$(FULL_CURRENT_PATH)"
      * Format and indent XML (XMLStarlet)
         * cd /d "$(CURRENT_DIRECTORY)"
xml fo --indent-spaces 4 --encode utf-8 "$(FULL_CURRENT_PATH)"
      * Check if well-formed XML (xmllint)
         * cd /d "$(CURRENT_DIRECTORY)"
cmd /c xmllint --noout "$(FULL_CURRENT_PATH)" & if ERRORLEVEL 1 (echo Not well formed) else (echo Well formed)
      * Validate entity (xmllint)
         * cd /d "$(CURRENT_DIRECTORY)
xmllint --noout --relaxng "http://cwrc.ca/schemas/entities.rng" "$(FULL_CURRENT_PATH)"
      * Validate entity (XMLStarlet)
         * cd /d "$(CURRENT_DIRECTORY)"
xml val --net --err --relaxng "http://cwrc.ca/schemas/entities.rng" "$(FULL_CURRENT_PATH)"
      * Validate MODS (xmllint)
         * cd /d "$(CURRENT_DIRECTORY)"
xmllint --noout --schema "http://www.loc.gov/standards/mods/mods.xsd" "$(FULL_CURRENT_PATH)"
      * Validate MODS (XMLStarlet)
         * cd /d "$(CURRENT_DIRECTORY)"
xml val --net --err --xsd "http://www.loc.gov/standards/mods/mods.xsd" "$(FULL_CURRENT_PATH)"
      * Validate DC (xmllint)
         * cd /d "$(CURRENT_DIRECTORY)"
xmllint --noout --schema "http://www.openarchives.org/OAI/2.0/oai_dc.xsd" "$(FULL_CURRENT_PATH)"
      * Validate DC (XMLStarlet)
         * cd /d "$(CURRENT_DIRECTORY)"
xml val --net --err --xsd "http://www.openarchives.org/OAI/2.0/oai_dc.xsd" "$(FULL_CURRENT_PATH)"
      * Validate TEI (Jing)
cd /d "$(CURRENT_DIRECTORY)"
cmd /c java -jar "C:\Program Files (x86)\Jing\bin\jing.jar" -t -e utf-8 https://www.tei-c.org/release/xml/tei/custom/schema/relaxng/tei_all.rng "$(FULL_CURRENT_PATH)" & if ERRORLEVEL 1 (echo Not validated) else (echo Validated)
   * Plugins
      * AnalyzePlugin
      * Compare
      * Converter
      * DSpellCheck
      * JSLint
      * JSON Viewer
      * JSTool
      * LineFilter2
      * NppCalc
      * NppExec
      * NppExport
      * NppFTP
      * Plugin Manager
      * Search+
      * SQL
      * TextFX
      * XML Tools
      * XPatherizerNPP


TextWrangler


* TextWrangler -- text editor and source code editor for Mac OS X written in Objective-C and Objective-C++
   * Scripts that operate (filter) on the window text are located in the directory "/Users/brundin/Library/Application Support/TextWrangler/Text Filters", and are accessed via the "Text > Apply Text Filter" menu option -- there are 6 scripts in this directory:
      * FormatIndentJavaScript[Python].py
         * this is a copy of the Python script jsbeautifier.py, written by  Einar Lielmanis (backup copy is in my GPU account)
      * FormatIndentJSON[PHP].php
         * #!/usr/bin/php
<?php
$data = file_get_contents('php://stdin');
$json = json_decode($data);
if ($json == null) {
         // Problem decoding json
return $data;
}
echo json_encode($json, JSON_PRETTY_PRINT);
      * FormatIndentJSON[Python].py
#!/usr/bin/python
import fileinput
import json
print json.dumps( json.loads(''.join([line.strip() for line in fileinput.input()])), sort_keys=True, indent=4)
      * FormatIndentXML[Tidy].sh
#!/bin/sh
tidy -xml -utf8 --add-xml-decl yes -indent --indent-spaces 4 -quiet -
      * FormatIndentXML[xmllint].sh
         * #!/bin/sh
           XMLLINT_INDENT='    ' xmllint --format --encode utf-8 -
      * FormatIndentXML[XMLStarlet].sh
         * #!/bin/sh
xml fo --indent-spaces 4 --encode utf-8 -
   * Scripts that the window text are sent to as standard input are located in the directory "/Users/brundin/Library/Application Support/TextWrangler/Scripts", and are accessed via the scripts icon menu option which is to the left of the "Help" menu option -- there are 7 scripts in this directory:
      * Check if well-formed XML (xmllint).sh
#!/bin/sh
xmllint --noout $BB_DOC_PATH; if [ $? -eq 0 ]; then echo "Well formed"; else echo "Not well formed"; fi
      * validateEntity[xmllint].sh
         * #!/bin/sh
xmllint --noout --relaxng http://cwrc.ca/schemas/entities.rng $BB_DOC_PATH
      * validateEntity[XMLStarlet].sh
         * #!/bin/sh
xml val --net --err --relaxng http://cwrc.ca/schemas/entities.rng $BB_DOC_PATH
      * validateMODS[xmllint].sh
         * #!/bin/sh
xmllint --noout --schema http://www.loc.gov/standards/mods/mods.xsd $BB_DOC_PATH
      * validateMODS[XMLStarlet].sh
         * #!/bin/sh
xml val -net --err --xsd http://www.loc.gov/standards/mods/mods.xsd $BB_DOC_PATH
      * validateDC[xmllint].sh
         * #!/bin/sh
xmllint --noout --schema http://www.openarchives.org/OAI/2.0/oai_dc.xsd $BB_DOC_PATH
      * validateTEI[Jing].sh
         * #!/bin/sh
java -jar "/Users/brundin/Applications/Jing/bin/jing.jar" -t -e utf-8 http://www.tei-c.org/release/xml/tei/custom/schema/relaxng/tei_all.rng $BB_DOC_PATH; if [ $? -eq 0 ]; then echo "Validated"; else echo "Not validated"; fi


        XML tools
                Oxygen XML Editor


* Oxygen XML Editor -- XML editor and XSLT/XQuery debugger and profiler written in Java (implements XPath 3.0, XSLT 2.0, and XQuery 3.0)
   * validation messages
      * valid document: "Document is valid."
      * invalid document: "Validation - failed."
* XPath queries
   * query to retrieve all elements in the CWRC entry schema
      * //text/ancestor::element[1]/@name
   * query to retrieve all elements in the MODS schema
      * //xs:element/@name
   * query to retrieve all attributes in the MODS schema
      * //xs:attribute/@name
   * query to retrieve elements that can contain text in the MODS schema (query is incomplete, so returned number of elements that can contain text is lower than what it actually is -- need to finish constructing this query)
      * //xs:attribute[@type='xs:string']/ancestor::xs:complexType[1][@name = preceding-sibling::xs:element[1]/@type]/preceding-sibling::xs:element[1]/@name | //xs:attribute[@type='xs:string']/ancestor::xs:element[1]/@name
   * query to retrieve the Orlando events by EID number that have a <TITLE> element that does not have an @TITLETYPE attribute
      * /events/FREESTANDING_EVENT[*//TITLE[not(@TITLETYPE)]]/@EID
* Oxygen XML Editor can be used to validate a collection of MODS records in a single file; to form the file, and process the validation error file, perform the following steps
   * Concatenate all the MODS files into one file
      * cat *_mods.xml > mods_records.xml
   * Delete this string (the XML declaration statement) which occurs for every MODS record from the document
      * <?xml version="1.0" encoding="utf-8"?>
   * Add this string back to the top of the document
   * Add the following <modsCollection> element right below the XML declaration statement
      * <modsCollection xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.loc.gov/mods/v3" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mods="http://www.loc.gov/mods/v3" xsi:schemaLocation="http://www.loc.gov/mods/v3  http://www.loc.gov/standards/mods/mods.xsd">
   * Add a closing </modsCollection> tag at the bottom of the document
   * Delete blank lines in the document using a regular expression
      * ^\s*[\r\n]
   * Run validation on the MODS collection file
   * Copy and paste the text from the Errors window pane into a new "errors.txt" text file
   * Process this "errors.txt" text file
      * Use grep to filter out the lines with the errors, which is in the "Description: " field
         * grep "Description: " errors.txt > errors-desc.txt
      * Open the "errors-desc.txt" file in Notepad++ or TextWrangler, and find and replace from the beginning of the line to the first blank space, using the following regular expression in the find field; repeat this operation
         * ^[^\s]*\s
      * Delete blank lines
         * ^\s*[\r\n]
      * Save the file, and from the command line, sort and retrieve the unique error types
         * cat errors-desc.txt | sort | uniq > errors-final.txt


XMLStarlet


* XMLStarlet -- command line XML/XSLT toolkit written in C (implements XPath 1.0 and XSLT 1.0)
   * validation messages
      * valid document: "valid"
      * invalid document: "invalid"
   * command to generate all XPaths in a document
      * xml el -u abdyma-w.sgm > output.txt
   * command to do an XPath query on the element <place> with the value "San Francisco"
      * xml sel -t -c "/root/record/place[text()='San Francisco']" places.xml
   * command to do an XPath query on the element <title> with the value "December Days" in a directory of XML files with a namespace
      * xml sel -t -c "//_:title[text()='December Days']" *
      * Note: if there is a default namespace, then need to use "_" as the prefix before each node element; for example, use "/_:node" instead of "/node"; or could use the -N option to define the namespace
   * command to do an XPath query and return all PINFLUENCESHER elements' text on one line
      * xml sel --text --net -t -c "//PINFLUENCESHER/descendant-or-self::text()" abdyma-w.sgm
   * command to do an XPath query and return each PINFLUENCESHER element's text on a separate line
      * xml sel --text --net -t -v "//PINFLUENCESHER[descendant-or-self::text()][not(.//PINFLUENCESHER)]" abdyma-w.sgm
      * xml sel --text --net -t -m "//PINFLUENCESHER[descendant-or-self::text()][not(.//PINFLUENCESHER)]" -c "." -n abdyma-w.sgm
   * command to do an XPath query and return all the //dc:title element text node values in a directory of DC files
      * xml sel -N dc="http://purl.org/dc/elements/1.1/" -t -c "//dc:title[1]/text()" -n * > library-names.txt
   * command to do an XPath query and return all the //mods:title element text node values in a directory of MODS files
      * xml sel -N mods="http://www.loc.gov/mods/v3" -t -c "//mods:title[1]/text()" -n * > library-names.txt
   * command to do an XPath query and return all the //titleStmt/title element text node values in a directory of TEI files
      * xml sel -T -t -c "/_:TEI/_:teiHeader/_:fileDesc/_:titleStmt/_:title/text()" -n * > filenames-librarynames.tsv
   * command to do an XPath query and return all the filenames and //titleStmt/title element text node values in a directory of TEI files (the two field values are separated by a TAB character) -- the second command reverses the two fields, so the TEI title value is first, followed by the filename
      * xml sel -T -t -f -o "$(printf \\t)" -c "/_:TEI/_:teiHeader/_:fileDesc/_:titleStmt/_:title/text()" -n * > filenames-librarynames.tsv
      * xml sel -T -t -c "/_:TEI/_:teiHeader/_:fileDesc/_:titleStmt/_:title/text()" -o "$(printf \\t)" -f -n * > filenames-librarynames.tsv
   * command to do an XPath query and parse the output of a Solr query with records containing PID fields and dc.title fields, returning a list of PIDs parsed from the Solr query XML input file
      * xml sel -T -t -v "/response/result/doc/str[@name='PID']/text()" input-solr_query.xml > pids.txt
   * command to do an XPath query and return the names of all elements in an XML file
      * xml sel -T -t -m "//*" -v "name()" -n filename.xml | sort | uniq > elements.txt
   * command to do an XPath query and return the names of all attributes in an XML file (note: //@* works with most files, but not extremely large files [a memory error is thrown]; //*/@* works with all files, including huge files)
      * xml sel -T -t -m "//*/@*" -v "name()" -n filename.xml | sort | uniq > attributes.txt
   * command to do an XPath query and return all elements in an XML Schema (XSD) (namespace is defined in the root element)
      * xml sel -t -v "//xs:element/@name" mods.xsd | sort | uniq > mods-elements.txt
      * xml sel -N mrb="http://www.w3.org/2001/XMLSchema" -t -v "//mrb:element/@name" mods.xsd | sort | uniq > mods-elements.txt
      * xml sel -t -v "//*[local-name()='element']/@name" mods.xsd | sort | uniq > mods-elements.txt
      * xml sel -t -v "//*[local-name()='element' and namespace-uri()='http://www.w3.org/2001/XMLSchema']/@name" mods.xsd | sort | uniq > mods-elements.txt
   * command to do an XPath query and return all elements in a RELAX NG schema that can contain text content (i.e., all elements with a text node) (namespace is the default namespace defined in the root element)
      * xml sel -t -v "//_:text/ancestor::_:element[1]/@name" cwrc_entry.rng | sort | uniq > elements-text.txt
      * xml sel -t -v "//DEFAULT:text/ancestor::DEFAULT:element[1]/@name" cwrc_entry.rng | sort | uniq > elements-text.txt
      * xml sel -N mrb="http://relaxng.org/ns/structure/1.0" -t -v "//mrb:text/ancestor::mrb:element[1]/@name" cwrc_entry.rng | sort | uniq > elements-text.txt
      * xml sel -t -v "//*[local-name()='text']/ancestor::*[local-name()='element'][1]/@name" cwrc_entry.rng | sort | uniq > elements-text.txt
      * xml sel -t -v "//*[local-name()='text' and namespace-uri()='http://relaxng.org/ns/structure/1.0']/ancestor::*[local-name()='element' and namespace-uri()='http://relaxng.org/ns/structure/1.0'][1]/@name" cwrc_entry.rng | sort | uniq > elements-text.txt
   * command to count number of elements in an XML file
      * xml sel -N mods="http://www.loc.gov/mods/v3" -t -v "count(/mods:modsCollection/mods:mods)" title_mods.xml
      * xml sel -t -v "count(//titleInfo)" title_mods.xml
   * command to count all nodes in an XML file
      * xml sel -t -f -o " " -v "count(//node())" title_mods.xml
   * command to count number of records that have two <recordInfo> elements
      * xml sel -t -v "count(/mods:modsCollection/mods:mods/mods:recordInfo[position()=2])" title_mods.xml
   * command to delete all @type attributes from an XML file:
      * xml ed --delete //@type title_mods.xml
   * command to add @type attribute with value of "alternative" to the element <titleInfo> in an XML file
      * xml ed -i /mods/titleInfo -t attr -n type -v alternative title_mods.xml
   * command to apply XSLT 1.0 stylesheet to XML input file
      * xml tr stylesheet_file.xsl _file.xml
   * command to batch process a collection of MODS records using an XSLT 1.0 stylesheet (run the command in the source directory of MODS files)
      * for %FILE in (*) do xml tr ..\transforms\cleanup_mods.xsl %FILE > ..\mods-rev\%FILE
      * for FILE in *; do xml tr ../transforms/cleanup_mods.xsl $FILE > ../mods-rev/$FILE; done
   * command to format and indent (pretty print) four spaces an XML file encoded in UTF-8
      * xml fo -s 4 -e utf-8 input-file.xml > output-file.xml
      * xml fo --indent-spaces 4 --encode utf-8 input-file.xml > output-file.xml
   * command to format and indent (pretty print) a directory of MODS or DC records
      * for %FILE in (*) do xml fo --indent-spaces 4 --encode utf-8 %FILE > ..\mods-new\%FILE
      * for FILE in *; do xml fo --indent-spaces 4 --encode utf-8 $FILE > ../mods-new/$FILE; done
   * command to check if an XML file is well formed
      * xml val --well-formed --err mods.xml
   * * command to validate an XML file (or files -- use *) against an XSD schema, and send the output to both the screen and an output file
      * xml val --net -e -s mods.xsd title_mods.xml 2>&1 | tee output.txt
      * xml val --net --err --xsd mods.xsd title_mods.xml 2>&1 | tee output.txt
   * * command to validate an XML file (or files -- use *) against a RELAX NG schema, and send the output to both the screen and an output file
      * xml val --net -e -r entities.rng persons.xml 2>&1 | tee output.txt
      * xml val --net --err --relaxng entities.rng persons.xml 2>&1 | tee output.txt
   * command to validate hundreds of thousands of files if you are getting an "argument list too long" error message, and send the output to both the screen and an output file
      * find ./eccji -type f -exec xml val --net --err --xsd mods.xsd {} \; | tee output.txt
   * command to batch validate a directory of MODS records
      * xml val --net --err --xsd http://www.loc.gov/standards/mods/v3/mods.xsd *_mods.xml 2>&1 | tee xmlstarlet_mods.txt
   * command to batch validate a directory of DC records
      * xml val --net --err --xsd http://www.openarchives.org/OAI/2.0/oai_dc.xsd *_dc.xml 2>&1 | tee xmlstarlet_dc.txt
   * note: to validate merged title entities, need to use vim to change the <modsCollectionDescription> element to the following (taken from the biblifo_MODS.xml file; also remember to change closing </modsCollectionDescription> element to </modsCollection>):
<modsCollection xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xmlns="http://www.loc.gov/mods/v3" 
xmlns:xlink="http://www.w3.org/1999/xlink" 
xmlns:mods="http://www.loc.gov/mods/v3" 
xsi:schemaLocation="http://www.loc.gov/mods/v3 
http://www.loc.gov/standards/mods/mods.xsd">
   * note: to validate merged person entities, need to add the XML processing instruction with the path to the schema like the following:
<?xml-model href="http://cwrc.ca/schemas/entities.rng" type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0"?>
   * commands to restructure birth and death dates in person entities
      * rename fromDate to dateSingle
         * xml ed -L -r "/cwrc/entity/person/description/existDates/dateRange/fromDate" -v "dateSingle" persons.xml
      * rename toDate to dateSingle
         * xml ed -L -r "/cwrc/entity/person/description/existDates/dateRange/toDate" -v "dateSingle" persons.xml
      * delete dateRange
         * xml ed -L -u /cwrc/entity/person/description/existDates/dateRange/.. -x "dateRange/node()" persons.xml
   * command to convert an XML file into a CSV file
      * xml sel -T -t -m /root/record -v "concat(element1,',',element2,',',element3)" -n input.xml > output.csv
   * the following shell script “process-rels-ext.sh” will add the @xmlns:cwrc attribute to the <rdf:RDF> element, and add a new element <cwrc:hasSchema> with attribute @rdf:resource as a child element of the <rdf:Description> element (so copy this code below into a file called “process-rels-ext.sh”):
for FILE in ./rels-ext/*; do
xml ed -L -i '//rdf:RDF' -t 'attr' -n 'xmlns:cwrc' -v 'http://cwrc.ca/ns/cw#' $FILE
xml ed -L -s '//rdf:Description' -t 'elem' -n 'cwrc:hasSchema' -v '' $FILE
xml ed -L -i '//cwrc:hasSchema' -t 'attr' -n 'rdf:resource' -v 'info:fedora/cwrc:13d0b3bf-4d37-4ead-bb05-c91260233532' $FILE
done
   * the following shell script "change-rels-ext.sh" will batch change the //cwrc:hasSchema/@rdf:resource attribute value from the existing schema object value of cwrc:dcdb82a0-ac09-4686-87f5-8f215d703bff to the new schema object value of cwrc:13d0b3bf-4d37-4ead-bb05-c91260233532 in a directory called “rels-ext” containing RELS-EXT datastream files
#!/bin/sh
for FILE in ./rels-ext/*; do
xml ed -L -u "//cwrc:hasSchema[@rdf:resource='info:fedora/cwrc:dcdb82a0-ac09-4686-87f5-8f215d703bff']/@rdf:resource" -v 'info:fedora/cwrc:13d0b3bf-4d37-4ead-bb05-c91260233532' $FILE
done
   * see the file edit-XMLStarlet.cmd for additional examples of using the "ed" (edit) command


xmllint


* xmllint -- command line XML tool written in C (implements XPath 1.0)
   * validation messages
      * valid document: "validates"
      * invalid document: "fails to validate" (or sometimes "parser error")
   * command to do an XPath query on the element <place> with the value "San Francisco"
      * xmllint --xpath "/root/record/place[text()='San Francisco']" place.xml
   * command to do an XPath query on the element <title> with the value "December Days" in a directory of XML files with a namespace
      * xmllint --xpath "//*[name()='title'][text()='December Days']" *
   * command to do an XPath query and return all PINFLUENCESHER elements' text on one line
      * xmllint --xpath "//PINFLUENCESHER/descendant-or-self::text()" abdyma-w.sgm > output.txt
   * command to do an XPath query and return each PINFLUENCESHER element's text on a separate line
      * sed "s/<\/PINFLUENCESHER>/\n<\/PINFLUENCESHER>/g" abdyma-w.sgm | xmllint --xpath "//PINFLUENCESHER/descendant-or-self::text()" -
   * command to do an XPath query and return all elements in an XML Schema (XSD) (namespace is defined in the root element)
      * xmllint --xpath "//*[local-name()='element']/@name" mods.xsd | tr " " "\n" | sed "s/name=\"//g;s/\"$//g;/^$/d" | sort | uniq > mods-elements.txt
      * xmllint --xpath "//*[local-name()='element' and namespace-uri()='http://www.w3.org/2001/XMLSchema']/@name" mods.xsd | tr " " "\n" | sed "s/name=\"//g;s/\"$//g;/^$/d" | sort | uniq > mods-elements.txt
      * gecho -e "setns mrb=http://www.w3.org/2001/XMLSchema\ncat //mrb:element/@name" | xmllint --shell mods.xsd | grep "name=" | sed "s/name=\"//g;s/\"$//g;/^$/d" | gsort | uniq > mods-elements.txt (GnuWin32 version)
      * echo -e 'setns mrb=http://www.w3.org/2001/XMLSchema\ncat //mrb:element/@name' | xmllint --shell mods.xsd | grep 'name=' | sed 's/name=\"//g;s/\"$//g;/^$/d' | sort | uniq > mods-elements.txt (Unix version)
   * command to do an XPath query and return all elements in a RELAX NG schema that can contain text content (i.e., all elements with a text node) (namespace is the default namespace defined in the root element)
      * xmllint --xpath "//*[local-name()='text']/ancestor::*[local-name()='element'][1]/@name" cwrc_entry.rng | tr " " "\n" | sed "s/name=\"//g;s/\"$//g;/^$/d" | sort > elements-text.txt
      * xmllint --xpath "//*[local-name()='text' and namespace-uri()='http://relaxng.org/ns/structure/1.0']/ancestor::*[local-name()='element' and namespace-uri()='http://relaxng.org/ns/structure/1.0'][1]/@name" cwrc_entry.rng | tr " " "\n" | sed "s/name=\"//g;s/\"$//g;/^$/d" | sort > elements-text.txt
      * gecho -e "setns mrb=http://relaxng.org/ns/structure/1.0\ncat //mrb:text/ancestor::mrb:element[1]/@name" | xmllint --shell cwrc_entry.rng | grep "name=" | sed "s/name=\"//g;s/\"$//g;/^$/d" | gsort > elements-text.txt (GnuWin32 version)
      * echo -e 'setns mrb=http://relaxng.org/ns/structure/1.0\ncat //mrb:text/ancestor::mrb:element[1]/@name' | xmllint --shell cwrc_entry.rng | grep 'name=' | sed 's/name=\"//g;s/\"$//g;/^$/d' | sort > elements-text.txt (Unix version)
   * command to format and indent (pretty print) an XML file encoded in UTF-8
      * xmllint --format --encode utf-8 input-file.xml > output-file.xml
   * command to format and indent (pretty print) four spaces an XML file encoded in UTF-8
      * Unix machines: XMLLINT_INDENT='    ' xmllint --format --encode utf-8 input-file.xml > output-file.xml
      * within Vim on Unix machines: % !XMLLINT_INDENT='    ' xmllint --format --encode utf-8 -
      * Windows machines: set XMLLINT_INDENT=    & xmllint --format --encode utf-8 input-file.xml > output-file.xml
   * command to format and indent (pretty print) a directory of MODS or DC records
      * for FILE in *; do xmllint --format --encode utf-8 $FILE > ../mods-new/$FILE; done
      * for FILE in *; do XMLLINT_INDENT='        ' xmllint --format --encode utf-8 $FILE > ../mods-new/$FILE; done (Unix machines)
      * for %FILE in (*) do set XMLLINT_INDENT=           & xmllint --format --encode utf-8 %FILE > ..\mods-new\%FILE (Windows machines)
   * command to check if an XML file is well formed
      * xmllint --noout mods.xml
   * * command to validate an XML file (or files -- use *) against an XSD schema, and send the output to both the screen and an output file
      * xmllint --noout --schema mods.xsd biblifo.xml 2>&1 | tee output.txt
   * * command to validate an XML file (or files -- use *) against a RELAX NG schema, and send the output to both the screen and an output file
      * xmllint --noout         --relaxng workflow.rng sample-workflow.xml 2>&1 | tee output.txt
   * command to validate a huge XML file (or files -- use *) against an XSD schema, and send the output to both the screen and an output file
      * xmllint --noout --huge --stream --schema mods.xsd title_mods.xml 2>&1 | tee output.txt
   * command to batch validate a directory of MODS records
      * xmllint --noout --schema http://www.loc.gov/standards/mods/v3/mods.xsd *_mods.xml 2>&1 | tee xmllint_mods.txt
   * command to batch validate a directory of DC records
      * xmllint --noout --schema http://www.openarchives.org/OAI/2.0/oai_dc.xsd *_dc.xml 2>&1 | tee xmllint_dc.txt


xsltproc


* xsltproc -- command line XSLT processor written in C (implements XSLT 1.0)
   * command to run an XSLT 1.0 transform on an XML file
      * xsltproc transform.xsl input.xml > output.xml
   * command to run an XSLT 1.0 transform on an XML file encoded in UTF-8
      * xsltproc --encoding utf-8 biblifo2mods.xsl biblifo.xml > biblifo_mods.xml
   * command to batch process a collection of MODS records using an XSLT 1.0 stylesheet (run the command in the source directory of MODS files)
      * for %FILE in (*) do xsltproc ..\transforms\cleanup_mods.xsl %FILE > ..\mods-rev\%FILE
      * for FILE in *; do xsltproc ../transforms/cleanup_mods.xsl $FILE > ../mods-rev/$FILE; done


Saxon-HE


* Saxon-HE (Home Edition) -- XSLT and XQuery processor written in Java (implements XPath 3.0, XSLT 2.0, and XQuery 3.0)
   * Can use the Saxon-HE (Home Edition) XSLT processor to apply XSLT 2.0 and XQuery 3.0 transforms, and XPath 2.0 and XQuery 3.0 queries (Note: Saxon Home Edition is not schema-aware, so can only do transformations and queries, not validations -- need Saxon-EE [Enterprise Edition] to run validations)
   * command to apply XSLT stylesheet to split a large XML file into individual records using Java and the Saxon parser
      * java -Xmx1024m -jar "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" title_mods.xml split_modsCollection.xsl
   * command to apply XSLT stylesheet to transform XML file
      * java -Xmx1024m -jar "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" biblifo.xml biblifo2mods.xsl > biblifo_mods.xml
   * command to apply a standalone XSLT stylesheet to transform a CSV file to an XML file:
      * java -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Transform -o:c:/users/brundin/temp/geonames/output.xml -it:main csv2xml.xsl
   * command to batch process a collection of MODS records using an XSLT 2.0 stylesheet (run the command in the source directory of MODS files)
      * for %FILE in (*) do do java -Xmx1024m -jar "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" %FILE ..\transforms\fix_mods.xsl > ..\mods-fix\%FILE
      * for FILE in *; do java -Xmx1024m -jar "/Users/brundin/Applications/Saxon-HE/saxon9he.jar" $FILE ../transforms/fix_mods.xsl > ../mods-fix/$FILE; done
   * command to perform an XPath query (XQuery) on the element <place> with value "San Francisco"
      * java -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Query -s:"places.xml" -qs:"declare option saxon:output 'omit-xml-declaration=yes'; /root/record/place[text()='San Francisco']"
      * java -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Query -s:"places.xml" -qs:"/root/record/place[text()='San Francisco']" !method=text
   * command to do an XPath query on the element <title> with the value "December Days" on an XML file with a namespace
      * java -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Query -s:"title000001.xml" -qs:"//*[name()='title'][text()='December Days']"
      * Note: can only run an XPath query on one source file, can't run it on multiple source files
   * command to do an XPath query and return all PINFLUENCESHER elements' text on one line
      * java -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Query -s:"abdyma-w.sgm" -qs:"declare option saxon:output 'omit-xml-declaration=yes'; //PINFLUENCESHER/descendant-or-self::text()"
   * command to do an XPath query and return each PINFLUENCESHER element's text on a separate line
      * sed "s/<\/PINFLUENCESHER>/\n<\/PINFLUENCESHER>/g" abdyma-w.sgm | java -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Query -s:"-" -qs:"declare option saxon:output 'omit-xml-declaration=yes'; //PINFLUENCESHER/descendant-or-self::text()"
   * command to do an XPath query and return the names of all elements in an XML file
      * java -Xmx4096m -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Query -s:"filename.xml" -qs:"declare option saxon:output 'omit-xml-declaration=yes'; //*/name()" | tr " " "\n" | gsort | uniq > elements.txt (Windows version)
      * java -Xmx4096m -cp "/Users/brundin/Applications/Saxon-He/saxon9he.jar" net.sf.saxon.Query -s:"/Users/brundin/Desktop/filename.xml" -qs:"declare option saxon:output 'omit-xml-declaration=yes'; //*/name()" | tr ' ' '\n' | sort | uniq > attributes.txt (Unix version)
   * command to do an XPath query and return the names of all attributes in an XML file
      * java -Xmx4096m -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Query -s:"ECCJI.mgxml" -qs:"declare option saxon:output 'omit-xml-declaration=yes'; //@*/name()" | tr " " "\n" | gsort | uniq > attributes.txt (Windows version)
      * java -Xmx4096m -cp "/Users/brundin/Applications/Saxon-He/saxon9he.jar" net.sf.saxon.Query -s:"/Users/brundin/Desktop/ECCJI.mgxml" -qs:"declare option saxon:output 'omit-xml-declaration=yes'; //@*/name()" | tr ' ' '\n' | sort | uniq > eccji-attributes.txt (Unix version)
   * command to do an XPath query and return all elements in an XML Schema (XSD) (namespace is defined in the root element)
      * java -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Query -s:"mods.xsd" -qs:"//xs:element/@name" -wrap | tr "\/>" "\n" | grep "name=" | sed "s/<result.*name=\"//g;s/\"//g" | gsort | uniq > mods-elements.txt (Windows version)
      * java -cp "/Users/brundin/Applications/Saxon-HE/saxon9he.jar" net.sf.saxon.Query -s:"mods.xsd" -qs:"//xs:element/@name" -wrap | tr "\/>" "\n" | grep "name=" | sed "s/<result.*name=\"//g;s/\"//g" | sort | uniq > mods-elements.txt (Unix version)
      * java -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Query -s:"mods.xsd" -qs:"//*[local-name()='element']/@name" -wrap | tr "\/>" "\n" | grep "name=" | sed "s/<result.*name=\"//g;s/\"//g" | gsort | uniq > mods-elements.txt (Windows version)
      * java -cp "/Users/brundin/Applications/Saxon-HE/saxon9he.jar" net.sf.saxon.Query -s:"mods.xsd" -qs:"//*[local-name()='element']/@name" -wrap | tr "\/>" "\n" | grep "name=" | sed "s/<result.*name=\"//g;s/\"//g" | sort | uniq > mods-elements.txt (Unix version)
      * java -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Query -s:"mods.xsd" -qs:"//*[local-name()='element' and namespace-uri()='http://www.w3.org/2001/XMLSchema']/@name" -wrap | tr "\/>" "\n" | grep "name=" | sed "s/<result.*name=\"//g;s/\"//g" | gsort | uniq > mods-elements.txt (Windows version)
      * java -cp "/Users/brundin/Applications/Saxon-HE/saxon9he.jar" net.sf.saxon.Query -s:"mods.xsd" -qs:"//*[local-name()='element' and namespace-uri()='http://www.w3.org/2001/XMLSchema']/@name" -wrap | tr "\/>" "\n" | grep "name=" | sed "s/<result.*name=\"//g;s/\"//g" | sort | uniq > mods-elements.txt (Unix version)
   * command to do an XPath query and return all elements in a RELAX NG schema that can contain text content (i.e., all elements with a text node) (namespace is the default namespace defined in the root element)
      * java -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Query -s:"cwrc_entry.rng" -qs:"//*[local-name()='text']/ancestor::*[local-name()='element'][1]/@name" -wrap | tr "\/>" "\n" | grep "name=" | sed "s/<result.*name=\"//g;s/\"//g" | gsort (Windows version)
      * java -cp "/Users/brundin/Applications/Saxon-HE/saxon9he.jar" net.sf.saxon.Query -s:"cwrc_entry.rng" -qs:"//*[local-name()='text']/ancestor::*[local-name()='element'][1]/@name" -wrap | tr "\/>" "\n" | grep "name=" | sed "s/<result.*name=\"//g;s/\"//g" | sort (Unix version)
      * java -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Query -s:"cwrc_entry.rng" -qs:"//*[local-name()='text']/ancestor::*[local-name()='element' and namespace-uri()='http://relaxng.org/ns/structure/1.0'][1]/@name" -wrap | tr "\/>" "\n" | grep "name=" | sed "s/<result.*name=\"//g;s/\"//g" | gsort (Windows version)
      * java -cp "/Users/brundin/Applications/Saxon-HE/saxon9he.jar" net.sf.saxon.Query -s:"cwrc_entry.rng" -qs:"//*[local-name()='text']/ancestor::*[local-name()='element' and namespace-uri()='http://relaxng.org/ns/structure/1.0'][1]/@name" -wrap | tr "\/>" "\n" | grep "name=" | sed "s/<result.*name=\"//g;s/\"//g" | sort (Unix version)


XML Calabash


* XML Calabash -- command line utility written in Java to run an XProc (an XML pipeline language) pipeline file (implements XSLT 2.0; invokes Saxon-HE for the transformations)
   * command to run an XProc pipeline file using the Java JAR ("-jar") argument method
      * java -jar "C:\Program Files (x86)\XML_Calabash\xmlcalabash-1.1.15-97.jar" pipeline_file.xpl (Windows version)
      * java -jar "/Users/brundin/Applications/XML_Calabash/xmlcalabash-1.1.15-97.jar" pipeline_file.xpl (Unix version)
   * command to run an XProc pipeline file using the Java CLASSPATH ("-cp" or "-classpath") argument method (need argument that is the main class)
      * java -cp "C:\Program Files (x86)\XML_Calabash\xmlcalabash-1.1.15-97.jar;lib\*" com.xmlcalabash.drivers.Main pipeline_file.xpl (Windows version)
      * java -cp "/Users/brundin/Applications/XML_Calabash/xmlcalabash-1.1.15-97.jar:lib/*" com.xmlcalabash.drivers.Main pipeline_file.xpl (Unix version)
   * command to run an XProc pipeline file and change the input and output locations
      * java -jar "C:\Program Files (x86)\XML_Calabash\xmlcalabash-1.1.15-97.jar" -i source=input_dir/pipeline_file.xpl -o result=/output_dir/output_file.xml input_dir/pipeline_file.xpl
   * command to run an XProc pipeline file and allocate 1 GB for the maximum size of Java heap memory using the "-Xmx" flag
      * java -Xmx1024m -jar "C:\Program Files (x86)\XML_Calabash\xmlcalabash-1.1.15-97.jar" pipeline_file.xpl
   * command to batch process a collection of MODS records using an XSLT 2.0 stylesheet
      * java -jar "C:\Program Files (x86)\XML_Calabash\xmlcalabash-1.1.15-97.jar" fix_mods.xpl


Xalan-Java


* Xalan-Java -- XSLT processor written in Java (implements XSLT 1.0; XSLT processor that is part of the Islandora stack)
   * command to run an XSLT 1.0 transform on an XML file using the Java JAR ("-jar") argument method
      * java -jar /usr/local/fedora/tomcat/webapps/fedoragsearch/WEB-INF/lib/xalan.jar -IN mods-test.xml -XSL cleanup_mods.xsl
   * command to run an XSLT 1.0 transform on an XML file using the Java CLASSPATH ("-cp" or "-classpath") argument method (need argument that is the main class)
      * java -cp /usr/local/fedora/tomcat/webapps/fedoragsearch/WEB-INF/lib/*: org.apache.xalan.xslt.Process -IN mods-test.xml -XSL cleanup_mods.xsl


Xerces Java


* Xerces Java -- command line parser and RELAX NG (RNG) and XML Schema (XSD 1.1) validator written in Java; this version provides a wrapper around Xerces in a single JAR file (note: the MADS schema is written in XSD 1.1)
   * command to validate an XML file against an XML Schema, with the path to the schema provided
      * java -jar xerces2-j.jar -if mads-record.xml -su http://www.loc.gov/standards/mads/mads.xsd
   * command to validate an XML file against an XML Schema, with the path to the schema read from the xsi:schemaLocation attribute in the root element of the input XML file
      * java -jar xerces2-j.jar -if mads-record.xml


Jing


* Jing -- command line RELAX NG (RNG) and XML Schema (XSD 1.1) validator written in Java (includes Xerces2 Java, and Saxon parsers as well; Jing supports W3C XML Schema by using a wrapper around the Xerces2 Java parser; note: the MADS schema is written in XSD 1.1)
   * validation messages
      * valid document: no message or output if the document is valid
      * invalid document: "fatal:"
   * command to validate an XML file (or files -- use *) against a RELAX NG schema, and send the output to both the screen and an output file
      * java -jar "C:\Program Files (x86)\Jing\bin\jing.jar" -t -e utf-8 workflow.rng workflow_record.xml | tee output.txt
   * command to validate an XML file (or files -- use *) against an XML Schema, and send the output to both the screen and an output file
      * java -jar "C:\Program Files (x86)\Jing\bin\jing.jar" -t -e utf-8 http://www.loc.gov/standards/mods/v3/mods.xsd mods-record.xml | tee output.txt
   * command to batch validate a directory of TEI records
      * java -jar "C:\Program Files (x86)\Jing\bin\jing.jar" -t -e utf-8 https://www.tei-c.org/release/xml/tei/custom/schema/relaxng/tei_all.rng *_tei.xml 2>&1 | tee jing_tei.txt


Trang


* Trang -- command line RELAX NG converter written in Java (RNG => XSD)
   * command to convert a RELAX NG (.rng) schema to an XML Schema (.xsd)
      * java -jar "C:\Program Files (x86)\Trang\trang.jar" -I rng -i encoding=utf-8 -O xsd -o encoding=utf-8 -o indent=4 workflow.rng workflow.xsd


Sun Multi Schema Validator (MSV)


* Sun Multi Schema Validator (MSV) -- command line XML Schema (XSD) and RELAX NG (RNG) validator written in Java (includes Xerces Java, Crimson, and OracleV2 parsers as well)
* (can batch validate hundreds of thousands of files with this application; also is extremely fast; but if validating a single file with many validation errors, it will exit after a certain threshold of number of errors is reached)
   * validation messages
      * valid document: "the document is valid."
      * invalid document: "Fatal error" and then "the document is NOT valid."
   * command to validate an XML file (or files -- use *) against an XSD schema, and send the output to both the screen and an output file
      * java -jar "C:\Program Files (x86)\MSV\msv.jar" -maxerror mods.xsd title_mods.xml | tee output.txt
   * command to validate an XML file (or files -- use *) against a RELAX NG schema, and send the output to both the screen and an output file
      * java -jar "C:\Program Files (x86)\MSV\msv.jar" -maxerror workflow.rng workflow_record.xml | tee output.txt
   * command to batch validate a directory of MODS records
      * java -jar "C:\Program Files (x86)\MSV\msv.jar" -maxerror http://www.loc.gov/standards/mods/v3/mods.xsd *_mods.xml | tee msv_mods.txt
   * command to batch validate a directory of DC records
      * java -jar "C:\Program Files (x86)\MSV\msv.jar" -maxerror http://www.openarchives.org/OAI/2.0/oai_dc.xsd *_mods.xml | tee msv_mods.txt


Sun RELAX NG Converter (rngconv)


* Sun RELAX NG Converter (rngconv) -- command line RELAX NG converter written in Java (XSD => RNG) (includes MSV parser as well)
   * command to convert an XML Schema (.xsd) to a RELAX NG (.rng) schema
      * java -jar "C:\Program Files (x86)\rngconv\rngconv.jar" myschema.xsd > result.rng 


Tidy


* Tidy -- command line utility tool written in C to check and fix HTML/XHTML/XML files
   * command to format and indent (pretty print) four spaces an HTML file
      * tidy --tidy-mark no --indent yes --indent-spaces 4 --wrap 0 --quiet yes input-file.html > output-file.html
   * command to format and indent (pretty print) four spaces an XML file encoded in UTF-8
      * tidy -xml -utf8 --add-xml-decl yes -indent --indent-spaces 4 -quiet input-file.xml > output-file.xml
   * command to check if an XML file is well formed
      * tidy -xml -errors input-file.xml
   * command to format and indent (pretty print) a directory of MODS or DC records
      * for %FILE in (*) do tidy -xml -utf8 --add-xml-decl yes -indent --indent-spaces 4 -quiet %FILE > ..\mods-new\%FILE
      * for FILE in *; do tidy -xml -utf8 --add-xml-decl yes -indent --indent-spaces 4 -quiet $FILE > ../mods-new/$FILE


xml_grep


* xml_grep -- command line utility tool written in compiled Perl to query an XML file
   * command to do an XPath query and return each PINFLUENCESHER element's text on a separate line
      * xml_grep --text_only --cond //PINFLUENCESHER abdyma-w.sgm > output.txt


xml_pp


* xml_pp -- command line utility tool written in compiled Perl to format and indent (pretty print) an XML file
   * command to format and indent (pretty print) an XML file encoded in UTF-8
      * xml_pp -s indented -e utf8 input-file.xml > output-file.xml
      * xml_pp -s record -e utf8 input-file.xml > output-file.xml


        Utilities
cURL


* cURL -- utility, libcurl library, and APIs written in C for downloading network data
   * command to download file and save with the same filename as on the remote server
      * curl -O http://www.ualberta.ca/~rbrundin/index.html
   * command to download file and save with a different filename
      * curl -o mrb.html http://www.ualberta.ca/~rbrundin/index.html
   * command to have Solr update the index for a particular Fedora object using that object's PID
      * curl -s -m 15 -u fedoraAdmin:Fedora_Admin_password_here "http://localhost:8080/fedoragsearch/rest?operation=updateIndex&action=fromPid&value=PID_value_here"


Wget


* Wget -- utility written in C for downloading network data
   * command to download file and save with the same filename as on the remote server
      * wget http://www.ualberta.ca/~rbrundin/index.html
   * command to download file and save with a different filename
      * wget -O mrb.html http://www.ualberta.ca/~rbrundin/index.html
   * command to recursively download a directory (although this particular example does not have any subdirectories; -U flag is a workaround for when one gets a 403 Forbidden error)
      * wget -U "Mozilla" -r -e robots=off -nH --no-parent http://www.missmaggie.org/geospy_files/
   * command to download LC MODS schema documentation
      * wget -r --level=0 -E --ignore-length -x -k -p -erobots=off -np -N http://www.loc.gov/standards/mods/userguide/
   * command to download Dublin Core schema documentation
      * wget -r --level=0 -E --ignore-length -x -k -p -erobots=off -np -N http://dublincore.org/documents/dcmi-terms/
   * note: include an index.html meta refresh redirect page at root of each documentation directory
      * example HTML index.html file at the root of the MODS schema documentation directory
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
<title>LC MODS schema documentation</title>
<meta http-equiv="REFRESH" content="0;url=./www.loc.gov/standards/mods/userguide/index.html">
</head>
<body>
Redirecting page.
</body>
</html>


cloc


* cloc (Count Lines of Code) is a utility written in Perl that reports the number of files and lines of code (broken out by blank lines, comment lines, and programmatic source code lines) by programming language for a given directory tree or directory trees; it is available as a Windows or Unix binary (in compiled Perl), or as a Perl script; found cloc to be much superior to another utility called SLOCCount
* Commands to run the cloc utility
   * cloc --quiet --out=output-file.txt directory-name
   * perl cloc.pl --quiet --out=output-file.txt directory-name
* Commands to print out the number of files and the number of lines of code in a GitHub repository -- use the following commands from the Git shell command prompt
   * cloc $(git ls-files)
   * git diff --shortstat 4b825dc642cb6eb9a060e54bf8d69288fbee4904
   * git ls-files | wc -l
   * wc -l $(git ls-files)


Zorba


* Zorba -- query processor written in C++ that processes XQuery and JSONiq syntaxes (implements XPath 3.0, XSLT 1.0, and XQuery 3.0)
* Zorba available on Dell Precision laptop CWRCLAB04, MacBook Pro laptop cwrcmac02, cwrc-apps-01.srv.ualberta.ca (beta.cwrc.ca), and cwrc-dev-01.srv.ualberta.ca
* command to do an XPath query and return the names of all elements in an XML file
   * zorba --context-item filename.xml -r -q "//*/name()" | tr " " "\n" | sort | uniq > elements.txt
* command to do an XPath query and return the names of all attributes in an XML file
   * zorba --context-item filename.xml -r -q "//@*/name()" | tr " " "\n" | sort | uniq > attributes.txt
* command to run an XQuery using the Zorba command line utility
   * zorba --context-item input_file.xml --as-files --query xquery_file.xquery --output-file output_file.xml
   * zorba --context-item input_file.xml -f -q xquery_file.xquery -o output_file.xml
* XQuery command to return the number of objects associated with each of the schema collection objects -- input XML files are the FOXML objects of each of the schema collection objects
   * xquery for $item in /obj/RELS-EXT_DS//*:hasSchema/@*:resource let $z := $item/data() group by $z return concat($z , " count: " , count($item))
* command that DGi uses in the $command variable in the Drupal islandora_xquery module
   * /usr/local/bin/zorba --as-files --context-item "/tmp/drupal/ds.xml" --output-file "/tmp/drupal/zorba.out" --query "/tmp/drupal/query.xq"
* Zorba command line utility documentation
   * http://www.zorba.io/documentation/latest/zorba/cli
* code temporarily inserted into the Drupal islandora_xquery module to display the Zorba command contained in the $command variable 
   * watchdog("Zorba command: ", $command);


Gmvault


* Gmvault -- Gmail e-mail backup utility program written in Python
   * command to update backup
      * gmvault sync --no-compression brundin@ualberta.ca
   * command to back up new e-mails from the past week
      * gmvault sync -t quick --no-compression brundin@ualberta.ca
   * command to restore Gmail e-mails to another Gmail account
      * gmvault restore new_brundin@ualberta.ca


Workflow
Activities
Notes on using the Fedora Resource Index Query Service and the Fedora RI REST API to query the Mulgara triplestore


* Fedora Commons is written in Java
* URL to the Fedora Resource Index Query Service: http://beta.cwrc.ca:8080/fedora/risearch
* Fedora Resource Index search documentation page: http://fedorarepository.org/sites/fedorarepository.org/files/documentation/3.2.1/Resource%20Index%20Search.html
* Mulgara iTQL command reference page: http://docs.mulgara.org/itqlcommands/index.html
* Sample iTQL queries: https://wiki.dlib.indiana.edu/display/INF/Fedora+Resource+Index
* URL Decoder/Encoder: http://meyerweb.com/eric/tools/dencoder/
* recursion, iTQL walk function page: http://fedora-commons.1317035.n2.nabble.com/fcrepo-user-Recursion-in-a-SPARQL-Query-td5843564.html
* Directory path to the iTQL documentation file on the cwrc-apps-07.srv.ualberta.ca server: /data/README.dir/README-cwrc-apps.cmd.txt
* Types of queries: can perform either tuple queries or triple queries
   * a tuple query is one that returns a list of named values (i.e., entities [subject, predicate, or object])
   * a triple query is one that returns a list of RDF statements (i.e., triples)
* Query languages:
   * tuple query languages supported include SPARQL and iTQL
   * triple query languages supported include SPARQL, iTQL, and SPO (subject-predicate-object)
* Fedora Resource Index (Mulgara triplestore) documentation: https://wiki.duraspace.org/display/FEDORA34/Triples+in+the+Resource+Index
* object predicates
   * info:fedora/fedora-system:def/model#createdDate
   * info:fedora/fedora-system:def/view#lastModifiedDate
   * info:fedora/fedora-system:def/model#state
   * info:fedora/fedora-system:def/model#ownerId
   * info:fedora/fedora-system:def/model#label
   * http://purl.org/dc/elements/1.1/title
   * http://purl.org/dc/elements/1.1/identifier
   * info:fedora/fedora-system:def/model#hasModel
   * info:fedora/fedora-system:def/relations-external#isMemberOfCollection
   * info:fedora/fedora-system:def/relations-external#isMemberOf
   * info:fedora/fedora-system:def/model#isContractorOf
   * info:fedora/fedora-system:def/model#isDeploymentOf
   * info:fedora/fedora-system:def/model#definesMethod
* datastream predicates:
   * info:fedora/fedora-system:def/view#disseminates
   * info:fedora/fedora-system:def/view#disseminationType
   * info:fedora/fedora-system:def/view#mimeType
   * info:fedora/fedora-system:def/view#lastModifiedDate
   * info:fedora/fedora-system:def/model#state
   * info:fedora/fedora-system:def/view#isVolatile
* subject tuple
   * object: info:fedora/$PID
   * datastream: info:fedora/$PID/$DSID
* aliases
Alias                        URI Prefix
fedora-rels-ext                info:fedora/fedora-system:def/relations-external#
dc                        http://purl.org/dc/elements/1.1/
mulgara                http://mulgara.org/mulgara#
fedora-model                info:fedora/fedora-system:def/model#
fedora-view                info:fedora/fedora-system:def/view#
test                        http://example.org/terms#
rdf                        http://www.w3.org/1999/02/22-rdf-syntax-ns#
xml-schema                http://www.w3.org/2001/XMLSchema#
fedora                        info:fedora/
* Notes:
   * count command queries will run many orders of magnitude faster if the query is formulated with the count function statement as part of the query, rather than relying on using the "format" option with the value "count" in the POST or GET method
   * don't URL encode PIDs when using the Fedora Resource Index Query Service GUI, so use colon rather than %3A between namespace and unique identifier part when using PIDs
   * to filter iTQL queries to just "active" objects, in the "where" clause use the Boolean "and" to add the following triple statement to the qualifying where clause:
      * and $item <info:fedora/fedora-system:def/model#state> <fedora-model:Active>
   * a tuple query will return a single subject tuple instance of a particular constraint on a subject tuple, whereas a triple query will return every subject tuple instance of a particular constraint on a subject tuple -- use the appropriate tuple query or triple query, depending on your particular problem space
   * useful RI query commands that can be used in cleanup and data integrity operations are identified below by the parenthetical phrase "(cleanup and data integrity operation)" at the end of the statement explaining the command that precedes each command


Various iTQL, SPARQL, and SPO tuple and triple query commands:


        iTQL tuple commands
* iTQL tuple command to provide paths from the root to all descendants to express the structure of the entire graph (tree) (use the walk function) (note: will retrieve all objects except book and critical edition page objects, which express their relationship to their parent via the isMemberOf and isPageOf predicates, rather than the isMemberOfCollection predicate)
select $parent $child from <#ri> 
where walk ( 
    $child 
    <fedora-rels-ext:isMemberOfCollection> 
    <info:fedora/islandora:root> 
  and 
    $child 
    <fedora-rels-ext:isMemberOfCollection> 
    $parent 
)
* iTQL tuple command to provide path from a known descendant up to the root
select $child $parent from <#ri> 
where walk ( 
    <info:fedora/demo:grandchild2b> 
    <fedora-rels-ext:isMemberOfCollection> 
    $parent 
  and 
    $child 
    <fedora-rels-ext:isMemberOfCollection> 
    $parent 
)
* iTQL tuple command to display the DC titles (which is the same as the Fedora labels) of the top-level collections (the child collections of the root collection)
select $title from <#ri>
where (
  $object <dc:title> $title
  and
  (
    $object <fedora-rels-ext:isMemberOfCollection>
      <info:fedora/islandora:root>
  )
)
order by $title;
* iTQL tuple command to display the DC titles (which is the same as the Fedora labels) as well as the DC identifiers (which is the same as the Fedora PIDs) of the top-level collections (the child collections of the root collection)
select $title $identifier from <#ri>
where (
  $object <dc:title> $title
  and
  $object <dc:identifier> $identifier
  and
  (
    $object <fedora-rels-ext:isMemberOfCollection>
      <info:fedora/islandora:root>
  )
)
order by $title;
* iTQL tuple command to display the PIDs for all Fedora objects in the repository
select $object
from <#ri>
where $object <info:fedora/fedora-system:def/model#hasModel>
<info:fedora/fedora-system:FedoraObject-3.0>
order by $object;
* iTQL tuple command to display the list of PIDs that belong to a particular collection
select $object
from <#ri>
where $object <info:fedora/fedora-system:def/relations-external#isMemberOfCollection>
<info:fedora/cwrc:collectionPIDhere>
* iTQL tuple command to display the titles (labels) and PIDs (identifiers) for all videos (islandora:sp_videoCModel) in the repository
select ?lab ?identifier from <#ri> where
?obj  <info:fedora/fedora-system:def/model#hasModel> <info:fedora/fedora-system:FedoraObject-3.0>
 and
?obj <dc:identifier> $identifier
 and
?obj  <info:fedora/fedora-system:def/model#label> ?lab
and 
?obj <info:fedora/fedora-system:def/model#hasModel>
     <info:fedora/islandora:sp_videoCModel>
order by ?lab
* iTQL tuple command to display all possibly malformed inactive objects (cleanup and data integrity operation)
select $object
from <#ri>
where $object <fedora-model:state> <info:fedora/fedora-system:def/model#Inactive>;
* iTQL tuple command to display all possibly malformed deleted objects (cleanup and data integrity operation)
select $object
from <#ri>
where $object <fedora-model:state> <info:fedora/fedora-system:def/model#Deleted>;
* iTQL tuple command to display objects by createdDate
select ?col ?obj ?mod from <#ri> where
?obj  <info:fedora/fedora-system:def/model#hasModel>
<info:fedora/fedora-system:FedoraObject-3.0>
 and
?obj  <info:fedora/fedora-system:def/model#createdDate> ?mod
and
?obj  <fedora-rels-ext:isMemberOfCollection> ?col
order by ?mod desc;
* iTQL tuple command to display objects by lastModifiedDate
select ?col ?obj ?mod from <#ri> where
?obj  <info:fedora/fedora-system:def/model#hasModel>
<info:fedora/fedora-system:FedoraObject-3.0>
 and
?obj  <info:fedora/fedora-system:def/view#lastModifiedDate> ?mod
and
?obj  <fedora-rels-ext:isMemberOf> ?col
order by ?mod desc;
* iTQL tuple command to display objects with a certain CModel that were created during a certain date span
select $object from <#ri>                                              
where $object <info:fedora/fedora-system:def/model#hasModel> <info:fedora/cwrc:citationCModel>
and $object <info:fedora/fedora-system:def/model#createdDate> $created
and $created <mulgara:after>'2012-11-01T00:00:00.000Z'^^<http://www.w3.org/2001/XMLSchema#dateTime> in <#xsd>
and $created <mulgara:before>'2012-12-04T00:00:00.000Z'^^<http://www.w3.org/2001/XMLSchema#dateTime> in <#xsd>;
* iTQL tuple command to display the number of objects that were created before a certain date
select count (
select $object from <#ri>                                              
where $object <info:fedora/fedora-system:def/model#hasModel> <info:fedora/fedora-system:FedoraObject-3.0>
and $object <info:fedora/fedora-system:def/model#createdDate> $created
and $created <mulgara:before>'2015-10-01T00:00:00.000Z'^^<http://www.w3.org/2001/XMLSchema#dateTime> in <#xsd>)
from <#ri>
where $object <info:fedora/fedora-system:def/model#hasModel>
<info:fedora/fedora-system:FedoraObject-3.0>;
* iTQL tuple command to display the number of objects by owner ID
select $ownerId
count(select $item from <#ri>
where $item <info:fedora/fedora-system:def/model#ownerId> $ownerId)
from <#ri>
where $item <info:fedora/fedora-system:def/model#ownerId> $ownerId
having $k0 <http://mulgara.org/mulgara#occursMoreThan> '0.0'^^<http://www.w3.org/2001/XMLSchema#double>
order by $k0 desc;
* iTQL tuple command to display the PIDs (identifiers) for all objects with RELS-EXT datastreams (i.e., display the PIDs for all objects that have a datastream with the dissemination type [datastream label, datastream ID, or DSID] of "RELS-EXT")
select $object
from <#ri>
where
$object <info:fedora/fedora-system:def/model#hasModel>    
     <info:fedora/fedora-system:FedoraObject-3.0>
  and
$object <fedora-view:disseminates> $ds
  and
$ds <info:fedora/fedora-system:def/view#disseminationType>
     <info:fedora/*/RELS-EXT>;
* iTQL tuple command to display all malformed objects that do not have a DC datastream (cleanup and data integrity operation)
select $object from <#ri>
where
$object <info:fedora/fedora-system:def/model#hasModel>    
     <info:fedora/fedora-system:FedoraObject-3.0>
minus ($object <fedora-view:disseminates> $ds1 in <#ri>
and $ds1 <fedora-view:disseminationType> <info:fedora/*/DC> in <#ri>)
* iTQL tuple command to display all malformed objects that do not have a RELS-EXT datastream (cleanup and data integrity operation)
select $object from <#ri>
where
$object <info:fedora/fedora-system:def/model#hasModel>    
     <info:fedora/fedora-system:FedoraObject-3.0>
minus ($object <fedora-view:disseminates> $ds1 in <#ri>
and $ds1 <fedora-view:disseminationType> <info:fedora/*/RELS-EXT> in <#ri>)
* iTQL tuple command to display all malformed objects that do not have a DC title element (cleanup and data integrity operation)
select $object
from <#ri>
where 
$object <info:fedora/fedora-system:def/model#hasModel>    
     <info:fedora/fedora-system:FedoraObject-3.0>
minus ($object <http://purl.org/dc/elements/1.1/title> $collection);
* iTQL tuple command to display all possibly malformed objects that do not have either an isMemberOfCollection statement or an isMemberOf statement in the RELS-EXT datastream (cleanup and data integrity operation)
select $object
from <#ri>
where 
$object <info:fedora/fedora-system:def/model#hasModel>
     <info:fedora/fedora-system:FedoraObject-3.0>
minus ($object <fedora-rels-ext:isMemberOfCollection> $collection)
minus ($object <fedora-rels-ext:isMemberOf> $collection);
* iTQL tuple command to display all malformed objects that have both an isMemberOfCollection statement and an isMemberOf statement in the RELS-EXT datastream (cleanup and data integrity operation)
select $object
from <#ri>
where 
$object <fedora-rels-ext:isMemberOfCollection> $collection
and
$object <fedora-rels-ext:isMemberOf> $collection;
* iTQL tuple command to display all malformed objects that have a pageCModel but don’t have a TN datastream (cleanup and data integrity operation)
select $object $title from <#ri>
where ($object <fedora-model:hasModel> <info:fedora/islandora:pageCModel>
and $object <dc:title> $title
and $object <fedora-model:state> <info:fedora/fedora-system:def/model#Active>)
minus ($object <fedora-view:disseminates> $ds1 in <#ri>
and $ds1 <fedora-view:disseminationType> <info:fedora/*/TN> in <#ri>)
* iTQL tuple command to display the number of RDF triple statements with Fedora objects as the object
select count ( select $subject $predicate $object from <#ri>  where $subject 
$predicate $object and $subject <fedora-model:hasModel> 
<info:fedora/fedora-system:FedoraObject-3.0> ) from <#ri> where $subject 
$predicate $object; 
* iTQL tuple command to display the total number of RDF triples
select count ( select $subject $predicate $object from <#ri>  where $subject 
$predicate $object ) from <#ri> where $subject 
$predicate $object;
* iTQL tuple command to display the number of objects by state (Active [A], Inactive [I], or Deleted [D])
select $state
count(select $item from <#ri>
where $item <info:fedora/fedora-system:def/model#state> $state
and $item  <info:fedora/fedora-system:def/model#hasModel>
<info:fedora/fedora-system:FedoraObject-3.0>)
from <#ri>
where $item <info:fedora/fedora-system:def/model#state> $state
having $k0 <http://mulgara.org/mulgara#occursMoreThan> '0.0'^^<http://www.w3.org/2001/XMLSchema#double>
order by $state;
* iTQL tuple command to display the number of objects by CModel (Content Model)
select $cmodel
count(select $item from <#ri>
where $item <info:fedora/fedora-system:def/model#hasModel> $cmodel)
from <#ri>
where $item <info:fedora/fedora-system:def/model#hasModel> $cmodel
having $k0 <http://mulgara.org/mulgara#occursMoreThan> '0.0'^^<http://www.w3.org/2001/XMLSchema#double>
order by $k0 desc;
* iTQL tuple command to display the number of datastreams by MIME type (media type)
select $target
count (select $object
from <#ri>
where $object <info:fedora/fedora-system:def/view#mimeType> $target)
from <#ri>
where $object <info:fedora/fedora-system:def/view#mimeType> $target
having $k0 <http://mulgara.org/mulgara#occursMoreThan> '0.0'^^<http://www.w3.org/2001/XMLSchema#double>
order by $target;
* iTQL tuple command to display the number of datastreams by dissemination type (datastream label, datastream ID, or DSID) (and this is also the number of objects by dissemination type, as no object has two of the same dissemination type)
select $target
count (select $object
from <#ri>
where $object <info:fedora/fedora-system:def/view#disseminationType> $target)
from <#ri>
where $object <info:fedora/fedora-system:def/view#disseminationType> $target
having $k0 <http://mulgara.org/mulgara#occursMoreThan> '0.0'^^<http://www.w3.org/2001/XMLSchema#double>
order by $target;
* iTQL tuple command to display the number of datastreams by volatility (isVolatile is true or false based on the control group the datastream belongs to -- true if R [Redirected content] or E [Externally referenced content], false if M [Managed content] or X [inline XML])
select $target
count (select $object
from <#ri>
where $object <info:fedora/fedora-system:def/view#isVolatile> $target)
from <#ri>
where $object <info:fedora/fedora-system:def/view#isVolatile> $target
having $k0 <http://mulgara.org/mulgara#occursMoreThan> '0.0'^^<http://www.w3.org/2001/XMLSchema#double>
order by $target;
* iTQL tuple command to display the total number of Fedora objects
select count (
select $object from <#ri>
where $object <info:fedora/fedora-system:def/model#hasModel>
<info:fedora/fedora-system:FedoraObject-3.0> )
from <#ri>
where $object <info:fedora/fedora-system:def/model#hasModel>
<info:fedora/fedora-system:FedoraObject-3.0>;
* iTQL tuple command to display the total number of Fedora objects (change the results to "count"; note: this is way slower than the above command!)
select $object from <#ri>
where $object <info:fedora/fedora-system:def/model#hasModel>
<info:fedora/fedora-system:FedoraObject-3.0>;
* iTQL tuple command to display the total number of Fedora datastreams
select count (
select $object $datastream from <#ri>
where $object <info:fedora/fedora-system:def/view#disseminates>
$datastream )
from <#ri>
where $object <info:fedora/fedora-system:def/view#disseminates>
$datastream;
* iTQL tuple command to display the total number of Fedora datastreams (change the results to "count"; note: this is way slower than the above command!)
select $object $datastream from <#ri>
where $object <info:fedora/fedora-system:def/view#disseminates>
$datastream;
* iTQL tuple command to display the PIDs of Fedora objects that will not be indexed by Solr (because of the conditional statement directives in the foxmlToSolr.xslt XSLT file); the two versions below retrieve the same set of Fedora objects, which are objects with a state of “Inactive” or “Deleted”, and CModel objects -- CModel objects all have a datastream dissemination type of “DS-COMPOSITE-MODEL”
   * includes objects with a state of “Inactive” or “Deleted”, and objects with a hasModel value of “fedora-system:ContentModel-3.0”, which are Content Model (CModel) Fedora objects (Need to remove duplicate entries!  Also need to remove duplicate entries!  E.g., "islandora:OACCModel" appears twice -- it is retrieved by the "has model" "fedora-system:ContentModel-3.0" conditional statement, and it is also retrieved by the "state" "Inactive" conditional statement.  Can use AWK [awk '!seen[$0]++' input_file] or uniq [cat input_file | sort | uniq] commands to remove duplicate lines.  There is a cURL version of this command below.
query=select $object
from <#ri>
where
$object <fedora-model:state>
         <info:fedora/fedora-system:def/model#Inactive>
or
$object <fedora-model:state>
         <info:fedora/fedora-system:def/model#Deleted>
or
$object <info:fedora/fedora-system:def/model#hasModel>
         <info:fedora/fedora-system:ContentModel-3.0>;
   * includes objects with a state of "Inactive" or "Deleted", and objects with a disseminationType of "DS-COMPOSITE-MODEL" or "METHODMAP" (Need to remove the dissemination type values from the end of the returned strings for the entry rows with dissemination types.  Also need to remove duplicate entries!  E.g., "islandora:OACCModel" appears twice -- it is retrieved by the "dissemination type" "DS-COMPOSITE-MODEL" conditional statement, and it is also retrieved by the "state" "Inactive" conditional statement.  Can use AWK [awk '!seen[$0]++' input_file] or uniq [cat input_file | sort | uniq] commands to remove duplicate lines.  There is a cURL version of this command below.)
select $object
from <#ri>
where
(
 $object
        <fedora-model:state>
        <info:fedora/fedora-system:def/model#Deleted>
or
 $object
        <fedora-model:state>
        <info:fedora/fedora-system:def/model#Inactive>
or
 $object
        <fedora-view:disseminationType>
        <info:fedora/*/METHODMAP>
or
 $object
        <fedora-view:disseminationType>
        <info:fedora/*/DS-COMPOSITE-MODEL>
)
order by $object;


        SPO (subject-predicate-object) triple commands
* SPO triple command to display all RDF triples (note: iTQL tuple command above to find all RDF triples is way faster!)
   * Click on the Find Triples tab
   * Change the Response to "Count"
   * Change the Limit dropdown menu value to "Unlimited"
   * Then use this query (three asterisks separated by spaces):
* * *
* SPO triple command to display all triples with the predicate hasModel for a certain target CModel object
   * Click on the Find Triples tab
   * Change the Response to "Count"
   * Change the Limit dropdown menu value to "Unlimited"
   * Then use the query * [the hasModel predicate] [the target PID], e.g., for the criticaleditionschemaCModel as a target, use this query,
* <info:fedora/fedora-system:def/model#hasModel> <info:fedora/islandora:criticaleditionschemaCModel>
* SPO triple command using cURL and Unix processing commands to display all possibly malformed objects with more than one isMemberOfCollection statement in the REL-EXT datastream (cleanup and data integrity operation)
   * curl -s "http://beta.cwrc.ca:8080/fedora/risearch?type=triples&lang=spo&format=n-triples&dt=on&query=*%20%3Cfedora-rels-ext%3AisMemberOfCollection%3E%20*" > spo-output.txt
   * process file with sed commands to trim so just subject tuple consisting of PID is left in the file
   * cat spo-output.txt | sort | uniq -c | sort -rn | more
* SPO triple command using cURL and Unix processing commands to display all possibly malformed objects with more than one isMemberOf statement in the REL-EXT datastream (cleanup and data integrity operation)
   * curl -s "http://beta.cwrc.ca:8080/fedora/risearch?type=triples&lang=spo&format=n-triples&dt=on&query=*%20%3Cfedora-rels-ext%3AisMemberOf%3E%20*" > spo-output.txt
   * process file with sed commands to trim so just subject tuple consisting of PID is left in the file
   * cat spo-output.txt | sort | uniq -c | sort -rn | more


        SPARQL tuple commands
* SPARQL tuple command to get all Fedora objects (to display the total number of Fedora objects, change results to "count")
SELECT ?obj 
FROM <#ri>
WHERE {
?obj <info:fedora/fedora-system:def/model#hasModel> <info:fedora/fedora-system:FedoraObject-3.0> .
}
* SPARQL tuple command to get all Fedora datastreams (to display the total number of Fedora datastreams, change results to "count")
SELECT ?obj ?ds
FROM <#ri>
WHERE {
?obj <info:fedora/fedora-system:def/view#disseminates> ?ds .
}
* SPARQL tuple command to show all properties where islandora:root is the subject
SELECT ?p ?o
FROM <#ri>
WHERE {
 <info:fedora/islandora:root> ?p ?o }
* SPARQL tuple command to show all properties where islandora:root is the object
SELECT ?s ?p
FROM <#ri>
WHERE {
 ?s ?p <info:fedora/islandora:root> }
* SPARQL tuple command to display all objects with a PDF datastream
SELECT ?pid
FROM <#ri>
WHERE {
 ?dss <fedora-view:disseminationType> <info:fedora/*/PDF> .
 ?pid <info:fedora/fedora-system:def/view#disseminates>
?dss
}
* SPARQL tuple command to display all objects with DC titles
PREFIX dc: <http://purl.org/dc/elem
ents/1.1/>
SELECT ?object ?title where { ?object dc:title ?title }
* SPARQL tuple command to display objects directly linked to a collection via an isMemberOfCollection relationship (can delete ?obj from the SELECT statement to retrieve just the collection container objects, or can delete ?col from the SELECT statement to just retrieve the linked objects)
SELECT ?obj ?col WHERE 
{ ?obj <info:fedora/fedora-system:def/relations-external#isMemberOfCollection> ?col }
* SPARQL tuple command to display objects directly linked to a book or critical edition object collection via an isMemberOf relationship (can delete ?obj from the SELECT statement to retrieve just the collection container objects, or can delete ?col from the SELECT statement to just retrieve the linked objects)
SELECT ?obj ?col WHERE 
{ ?obj <info:fedora/fedora-system:def/relations-external#isMemberOf> ?col }
* SPARQL tuple command to display orphaned Fedora objects linked to deleted parent collection objects via an isMemberOfCollection relationship (can delete ?obj from the SELECT statement to retrieve just the deleted parent collection container objects, or can delete ?collection from the SELECT statement to just retrieve the orphaned linked objects) (cleanup and data integrity operation)
SELECT ?obj ?collection
FROM <#ri>
WHERE {
  ?obj <fedora-rels-ext:isMemberOfCollection> ?collection .
  OPTIONAL { ?collection <fedora-model:hasModel> ?model . }
  FILTER(!bound(?model))
}
* SPARQL tuple command to display orphaned Fedora objects linked to deleted parent book or critical edition objects via an isMemberOf relationship (can delete ?obj from the SELECT statement to retrieve just the deleted parent book or critical edition container objects, or can delete ?collection from the SELECT statement to just retrieve the orphaned linked objects) (cleanup and data integrity operation)
SELECT ?obj ?collection
FROM <#ri>
WHERE {
  ?obj <fedora-rels-ext:isMemberOf> ?collection .
  OPTIONAL { ?collection <fedora-model:hasModel> ?model . }
  FILTER(!bound(?model))
}


Useful cURL commands to query the RI Service using iTQL commands
* Number of Fedora objects
   * curl -s "http://localhost:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20count%20(%0Aselect%20%24object%20from%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23hasModel%3E%0A%3Cinfo%3Afedora%2Ffedora-system%3AFedoraObject-3.0%3E%20)%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23hasModel%3E%0A%3Cinfo%3Afedora%2Ffedora-system%3AFedoraObject-3.0%3E%3B%0A" | tail -1
* Number of Fedora datastreams
   * curl -s "http://localhost:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20count%20(%0Aselect%20%24object%20%24datastream%20from%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fview%23disseminates%3E%0A%24datastream%20)%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fview%23disseminates%3E%0A%24datastream%3B%0A" | tail -1
* Number of Mulgara triplestore (Fedora Resource Index) RDF triples
   * curl -s "http://localhost:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20count%20(%20select%20%24subject%20%24predicate%20%24object%20from%20%3C%23ri%3E%20%20where%20%24subject%20%0A%24predicate%20%24object%20)%20from%20%3C%23ri%3E%20where%20%24subject%20%0A%24predicate%20%24object%3B" | tail -1
* Number of Fedora objects by state (Active [A], Inactive [I], or Deleted [D])
   * curl -s "http://localhost:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24state%0Acount(select%20%24item%20from%20%3C%23ri%3E%0Awhere%20%24item%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23state%3E%20%24state%0Aand%20%24item%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23hasModel%3E%0A%3Cinfo%3Afedora%2Ffedora-system%3AFedoraObject-3.0%3E)%0Afrom%20%3C%23ri%3E%0Awhere%20%24item%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23state%3E%20%24state%0Ahaving%20%24k0%20%3Chttp%3A%2F%2Fmulgara.org%2Fmulgara%23occursMoreThan%3E%20%270.0%27%5E%5E%3Chttp%3A%2F%2Fwww.w3.org%2F2001%2FXMLSchema%23double%3E%0Aorder%20by%20%24state%3B" | sed 1d | sed "s/info:fedora\/fedora-system:def\/model#//g" | sort -t, -k2,2nr -k1,1
* Number of Fedora objects by CModel (Content Model)
   * curl -s "http://localhost:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24cmodel%0Acount(select%20%24item%20from%20%3C%23ri%3E%0Awhere%20%24item%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23hasModel%3E%20%24cmodel)%0Afrom%20%3C%23ri%3E%0Awhere%20%24item%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23hasModel%3E%20%24cmodel%0Ahaving%20%24k0%20%3Chttp%3A%2F%2Fmulgara.org%2Fmulgara%23occursMoreThan%3E%20%270.0%27%5E%5E%3Chttp%3A%2F%2Fwww.w3.org%2F2001%2FXMLSchema%23double%3E%0Aorder%20by%20%24cmodel%3B%0A" | sed 1d | sed "s/info:fedora\///g" | sort -t, -k2,2nr -k1,1
* Number of Fedora objects by owner ID
   * curl -s "http://localhost:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24ownerId%0Acount(select%20%24item%20from%20%3C%23ri%3E%0Awhere%20%24item%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23ownerId%3E%20%24ownerId)%0Afrom%20%3C%23ri%3E%0Awhere%20%24item%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23ownerId%3E%20%24ownerId%0Ahaving%20%24k0%20%3Chttp%3A%2F%2Fmulgara.org%2Fmulgara%23occursMoreThan%3E%20%270.0%27%5E%5E%3Chttp%3A%2F%2Fwww.w3.org%2F2001%2FXMLSchema%23double%3E%0Aorder%20by%20%24k0%20desc%3B%0A" | sed 1d | sort -t, -k2,2nr -k1,1
* Number of Fedora datastreams by volatility (isVolatile is true or false based on the control group the datastream belongs to -- true if R [Redirected content] or E [Externally referenced content], false if M [Managed content] or X [inline XML])
   * curl -s "http://localhost:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24target%0Acount%20(select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fview%23isVolatile%3E%20%24target)%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fview%23isVolatile%3E%20%24target%0Ahaving%20%24k0%20%3Chttp%3A%2F%2Fmulgara.org%2Fmulgara%23occursMoreThan%3E%20%270.0%27%5E%5E%3Chttp%3A%2F%2Fwww.w3.org%2F2001%2FXMLSchema%23double%3E%0Aorder%20by%20%24target%3B%0A" | sed 1d
* Number of Fedora datastreams by MIME type (media type)
   * curl -s "http://localhost:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24target%0Acount%20(select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fview%23mimeType%3E%20%24target)%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fview%23mimeType%3E%20%24target%0Ahaving%20%24k0%20%3Chttp%3A%2F%2Fmulgara.org%2Fmulgara%23occursMoreThan%3E%20%270.0%27%5E%5E%3Chttp%3A%2F%2Fwww.w3.org%2F2001%2FXMLSchema%23double%3E%0Aorder%20by%20%24target%3B" | sed 1d | sort -t, -k2,2nr -k1,1
* Number of Fedora datastreams by dissemination type (datastream label, datastream ID, or DSID) (and this is also the number of objects by dissemination type, as no object has two of the same dissemination type)
   * curl -s "http://localhost:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24target%0Acount%20(select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fview%23disseminationType%3E%20%24target)%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fview%23disseminationType%3E%20%24target%0Ahaving%20%24k0%20%3Chttp%3A%2F%2Fmulgara.org%2Fmulgara%23occursMoreThan%3E%20%270.0%27%5E%5E%3Chttp%3A%2F%2Fwww.w3.org%2F2001%2FXMLSchema%23double%3E%0Aorder%20by%20%24target%3B%0A" | sed 1d | sed "s/info:fedora\/\*\///g" | sort -t, -k2,2nr -k1,1
* Output the number of Fedora datastreams per Fedora object as a ranked distribution in descending order
   * curl -s "http://beta.cwrc.ca:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24datastream%20from%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fview%23disseminates%3E%0A%24datastream%0A" | sed 1d | sed "s/info:fedora\///g;s/\/.*//g" | sort | uniq -c | sort -t, -k1,1nr -k2,2 | sed -e 's/^[ \t]*//;s/[ \t]*$//' | sed 's/ /,/g' > output_ds_per_object.csv
   * curl -s "http://beta.cwrc.ca:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24datastream%20from%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fview%23disseminates%3E%0A%24datastream%0A" | sed 1d | sed "s/info:fedora\///g;s/\/.*//g" | gsort | uniq -c | gsort -t, -k1,1nr -k2,2 | sed -e "s/^[ \t]*//;s/[ \t]*$//" | sed "s/ /,/g" > output_ds_per_object.csv (GnuWin32 version)
* Frequency distribution for the number of Fedora datastreams per Fedora object
   * curl -s "http://beta.cwrc.ca:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24datastream%20from%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fview%23disseminates%3E%0A%24datastream%0A" | sed 1d | sed "s/info:fedora\///g;s/\/.*//g" | sort | uniq -c | sort -nr | sed -e 's/^[ \t]*//;s/[ \t]*$//' | awk '{print $1}' | uniq -c | sed -e 's/^[ \t]*//;s/[ \t]*$//' | awk ' { t = $1; $1 = $2; $2 = t; print; } ' | sort -n | sed 's/ /,/g' | awk -F, 'BEGIN {print "Number of datastreams,Number of objects"} {s+=$2; print} END {print "TOTAL,"s}' > freq_dist_ds_per_object.csv
   * curl -s "http://beta.cwrc.ca:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24datastream%20from%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fview%23disseminates%3E%0A%24datastream%0A" | sed 1d | sed "s/info:fedora\///g;s/\/.*//g" | gsort | uniq -c | gsort -nr | sed -e "s/^[ \t]*//;s/[ \t]*$//" | awk "{print $1}" | uniq -c | sed -e "s/^[ \t]*//;s/[ \t]*$//" | awk " { t = $1; $1 = $2; $2 = t; print; } " | gsort -n | sed "s/ /,/g" | awk -F, "BEGIN {print \"Number of datastreams,Number of objects\"} {s+=$2; print} END {print \"TOTAL,\"s}" > freq_dist_ds_per_object.csv (GnuWin32 version)
* Output the PIDs for all Fedora objects
   * curl -s "http://localhost:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23hasModel%3E%0A%3Cinfo%3Afedora%2Ffedora-system%3AFedoraObject-3.0%3E%0Aorder%20by%20%24object%3B" | sed "1d" | sed "s/info:fedora\///g" | sort | tee fedora_pids.txt
   * number of Fedora objects
      * cat fedora_pids.txt | wc -l
   * number of EMiC Fedora objects
      * cat fedora_pids.txt | sort | grep "emic" | wc -l
* Number of Fedora objects that will not be indexed as per directives (condition statements) in the foxmlToSolr.xslt file
   * curl -s "http://localhost:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%20%0A%24object%20%3Cfedora-model%3Astate%3E%0A%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23Inactive%3E%0Aor%0A%24object%20%3Cfedora-model%3Astate%3E%0A%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23Deleted%3E%0Aor%0A%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23hasModel%3E%0A%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3AContentModel-3.0%3E%3B" | sed "1d" | sed "s/info:fedora\///g" | sort | uniq | wc -l
   * curl -s "http://localhost:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%0A(%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-model%3Astate%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23Deleted%3E%0Aor%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-model%3Astate%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23Inactive%3E%0Aor%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-view%3AdisseminationType%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2F*%2FMETHODMAP%3E%0Aor%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-view%3AdisseminationType%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2F*%2FDS-COMPOSITE-MODEL%3E%0A)%0Aorder%20by%20%24object%3B" | sed "1d" | sed "s/info:fedora\///g;s/\/METHODMAP//g;s/\/DS-COMPOSITE-MODEL//g" | sort | uniq | wc -l
* Output the PIDs for all Fedora objects that will not be indexed as per directives (condition statements) in the foxmlToSolr.xslt file
   * curl -s "http://localhost:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%20%0A%24object%20%3Cfedora-model%3Astate%3E%0A%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23Inactive%3E%0Aor%0A%24object%20%3Cfedora-model%3Astate%3E%0A%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23Deleted%3E%0Aor%0A%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23hasModel%3E%0A%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3AContentModel-3.0%3E%3B" | sed "1d" | sed "s/info:fedora\///g" | sort | uniq | tee pids.txt
   * curl -s "http://localhost:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%0A(%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-model%3Astate%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23Deleted%3E%0Aor%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-model%3Astate%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23Inactive%3E%0Aor%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-view%3AdisseminationType%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2F*%2FMETHODMAP%3E%0Aor%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-view%3AdisseminationType%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2F*%2FDS-COMPOSITE-MODEL%3E%0A)%0Aorder%20by%20%24object%3B%0A" | sed "1d" | sed "s/info:fedora\///g;s/\/METHODMAP//g;s/\/DS-COMPOSITE-MODEL//g" | sort | uniq | tee no_solr_pids.txt
* Output the PIDs for all Fedora objects, but filtering out Fedora objects with the following characteristics: objects with a state of "Inactive" or "Deleted"; objects with a CModel of "fedora-system:ContentModel-3.0" or "islandora:collectionCModel"; and objects with a last modified date before the provided date of 2014-06-11T19:38:02.49Z
   * curl -s "http://roam.macewan.ca:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%20%0A(%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23hasModel%3E%0A%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3AFedoraObject-3.0%3E%0Aand%0A%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fview%23lastModifiedDate%3E%20%24modified%0Aand%0A%24modified%20%3Cmulgara%3Aafter%3E%272014-06-11T19%3A38%3A02.49Z%27%5E%5E%3Chttp%3A%2F%2Fwww.w3.org%2F2001%2FXMLSchema%23dateTime%3E%20in%20%3C%23xsd%3E)%0Aminus%20(%24object%20%3Cfedora-model%3Astate%3E%0A%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23Inactive%3E)%0Aminus%20(%24object%20%3Cfedora-model%3Astate%3E%0A%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23Deleted%3E)%0Aminus%20(%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23hasModel%3E%0A%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3AContentModel-3.0%3E)%0Aminus%20(%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23hasModel%3E%0A%20%20%20%20%20%3Cinfo%3Afedora%2Fislandora%3AcollectionCModel%3E)%3B" | sed "1d" | sed "s/info:fedora\///g" | sort -n -k1.4 | uniq | tee pids.txt


Useful HTTP browser commands to query the RI Service using iTQL commands


* Total number of Fedora objects
   * http://beta.cwrc.ca:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20count%20(%0Aselect%20%24object%20from%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23hasModel%3E%0A%3Cinfo%3Afedora%2Ffedora-system%3AFedoraObject-3.0%3E%20)%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23hasModel%3E%0A%3Cinfo%3Afedora%2Ffedora-system%3AFedoraObject-3.0%3E%3B%0A
* Number of Fedora objects that will not be indexed as per directives (condition statements) in the foxmlToSolr.xslt file
   * http://beta.cwrc.ca:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20count%20(%0Aselect%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%0A(%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-model%3Astate%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23Deleted%3E%0Aor%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-model%3Astate%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23Inactive%3E%0Aor%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-view%3AdisseminationType%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2F*%2FMETHODMAP%3E%0Aor%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-view%3AdisseminationType%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2F*%2FDS-COMPOSITE-MODEL%3E%0A)%0Aorder%20by%20%24object%20)%0Afrom%20%3C%23ri%3E%0Awhere%0A(%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-model%3Astate%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23Deleted%3E%0Aor%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-model%3Astate%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Fmodel%23Inactive%3E%0Aor%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-view%3AdisseminationType%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2F*%2FMETHODMAP%3E%0Aor%0A%20%24object%0A%20%20%20%20%20%20%20%20%3Cfedora-view%3AdisseminationType%3E%0A%20%20%20%20%20%20%20%20%3Cinfo%3Afedora%2F*%2FDS-COMPOSITE-MODEL%3E%0A)%0Aorder%20by%20%24object%3B


Notes on using the Fedora Search Interface and the Fedora REST API to query the Fedora datastore


* URL to the Fedora Search Interface: http://beta.cwrc.ca:8080/fedora/objects
* Old deprecated Fedora Search Interface: http://beta.cwrc.ca:8080/fedora/search
* Documentation on the Fedora search interface: https://wiki.duraspace.org/display/FEDORA34/Basic+Search
* Documentation on the Fedora REST API: https://wiki.duraspace.org/display/FEDORA38/REST+API
* Article on using the Fedora REST API: https://wsampson.wordpress.com/2011/03/23/handling-results-in-fedoras-rest-api/
* Instead of the Fedora REST API, can use the Islandora Tuque PHP Library
* Fedora datastore -- composed of
   * Fedora object store -- contains FOXML objects
   * Fedora datastream store -- contains managed content datastreams
* Fedora REST API queries the FOXML objects in the Fedora object store, and can also return datastreams in the Fedora datastream store
* Note: Web browser GET methods and cURL commands that query the Fedora REST API need to use the username "fedoraAdmin" with the fedoraAdmin's password
* Fedora query to return 100 Fedora object PIDs in the repository (note: maxResults can’t return more than 100 results)
   * http://beta.cwrc.ca:8080/fedora/objects?terms=*&pid=true&maxResults=100&resultFormat=xml
* Fedora query to return the FOXML object for the root BIBLIFO collection container object
   * http://beta.cwrc.ca:8080/fedora/objects/islandora:abda880b-55d8-4d4a-b08a-e919e77e9a1b/objectXML
   * curl -s -u fedoraAdmin:passwordHere -X GET "http://beta.cwrc.ca:8080/fedora/objects/islandora:abda880b-55d8-4d4a-b08a-e919e77e9a1b/objectXML"
* Fedora query to download the MODS datastream for a BIBLIFO object
   * curl -s -u fedoraAdmin:passwordHere -X GET "http://beta.cwrc.ca:8080/fedora/objects/islandora:75a7614e-fd23-431a-ae36-6f521fa83990/datastreams/MODS/content" > MODS_output.xml
* Fedora query command to use a cURL iTQL query to obtain a list of PIDs in a collection, and then use cURL to download the TEI and MODS datastreams respectively -- used to download the TEI and MODS datastreams from book objects from a collection in the Spanish Civil War project (the collection is "Prisoner of War Letters"; PID is the following: islandora:3ed517db-86e0-4b84-b589-1b050e406dec) (Note: this command also uses the Fedora Resource Index API to obtain the list of PIDs)
   * curl -s "http://cwrc-apps-01.srv.ualberta.ca:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Frelations-external%23isMemberOfCollection%3E%0A%3Cinfo%3Afedora%2Fislandora%3A3ed517db-86e0-4b84-b589-1b050e406dec%3E" | sed '1d;s/info:fedora\///g' >> pids.txt; mkdir TEI_files; cd TEI_files; for PID in `cat ../pids.txt`; do curl -s -u fedoraAdmin:passwordHere -X GET "http://cwrc-apps-01.srv.ualberta.ca:8080/fedora/objects/$PID/datastreams/TEI/content" > $PID"_tei.xml"; mv $PID"_tei.xml" `echo $PID"_tei.xml" | sed s/:/_/`; done; cd ..
   * curl -s "http://cwrc-apps-01.srv.ualberta.ca:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Frelations-external%23isMemberOfCollection%3E%0A%3Cinfo%3Afedora%2Fislandora%3A3ed517db-86e0-4b84-b589-1b050e406dec%3E" | sed '1d;s/info:fedora\///g' >> pids.txt; mkdir MODS_files; cd MODS_files; for PID in `cat ../pids.txt`; do curl -s -u fedoraAdmin:passwordHere -X GET "http://cwrc-apps-01.srv.ualberta.ca:8080/fedora/objects/$PID/datastreams/MODS/content" > $PID"_mods.xml"; mv $PID"_mods.xml" `echo $PID"_mods.xml" | sed s/:/_/`; done; cd ..
* Bash shell script that contain a Fedora query wrapped in two nested Bash for loops to change the Fedora labels for a set Fedora objects
   * Shell script “rename-label.sh” code is the following:
        for PID in `cat pids.txt`; do
            for LIBRARY_NAME in `cat library-names.txt`; do
        curl -s -u fedoraAdmin:passwordHere -X PUT "http://beta.cwrc.ca:8080/fedora/objects/$PID?label=$LIBRARY_NAME"
    done
done


Notes on using the Islandora search and browse interface to query the Islandora repository


* Retrieve a particular object via its PID -- example of an object in the BIBLIFO collection
   * http://beta.cwrc.ca/islandora/object/islandora%3A75a7614e-fd23-431a-ae36-6f521fa83990
* Download the MODS datastream from the above BIBLIFO collection object
   * http://beta.cwrc.ca/islandora/object/islandora%3A75a7614e-fd23-431a-ae36-6f521fa83990/datastream/MODS/download
* Search and retrieve all objects that are viewable via the Islandora search interface
   * http://beta.cwrc.ca/islandora/search
* Search for all objects that are members of the BIBLIFO collection object
   * http://beta.cwrc.ca/islandora/search/RELS_EXT_isMemberOfCollection_uri_mt%3A%28abda880b-55d8-4d4a-b08a-e919e77e9a1b%29
* Walk the CEWW root collection using the ancestor_ms Solr field to retrieve all nested objects in the CEWW root collection
   * http://beta.cwrc.ca/islandora/search/*?type=edismax&cp=islandora:dbdbd5af-fc3a-4f91-9be8-9ad8499fa8af
* Command to use Bag-It, cURL, to download a set of objects in a collection, and then copy out the TEI and MODS datastreams respectively, and rename the files sequentially (e.g., TEI0001.xml, etc.) -- used to download a Bag-It ZIP archive of book objects from a collection in the Spanish Civil War project (the collection is "Prisoner of War Letters"; PID is the following: islandora:3ed517db-86e0-4b84-b589-1b050e406dec) and copy out the TEI (filename is "TEI.bin") and MODS (filename is "MODS.xml") datastreams
   * curl http://cwrc-apps-01.srv.ualberta.ca/islandora/object/islandora%3A3ed517db-86e0-4b84-b589-1b050e406dec/manage/bagit && wget http://cwrc-apps-01.srv.ualberta.ca/sites/default/files/private/BagItPHP/Bag-islandora_3ed517db_86e0_4b84_b589_1b050e406dec.zip; mkdir Bag-It; cd Bag-It; unzip ../Bag-*.zip; cd ..; mkdir TEI_files; find ./Bag-* -name TEI.bin | awk 'BEGIN{ a=1 }{ printf "cp \"%s\" ./TEI_files/TEI%04d.xml\n", $0, a++ }' | bash
   * curl http://cwrc-apps-01.srv.ualberta.ca/islandora/object/islandora%3A3ed517db-86e0-4b84-b589-1b050e406dec/manage/bagit && wget http://cwrc-apps-01.srv.ualberta.ca/sites/default/files/private/BagItPHP/Bag-islandora_3ed517db_86e0_4b84_b589_1b050e406dec.zip; mkdir Bag-It; cd Bag-It; unzip ../Bag-*.zip; cd ..; mkdir MODS_files; find ./Bag-* -name MODS.xml | awk 'BEGIN{ a=1 }{ printf "cp \"%s\" ./MODS_files/MODS%04d.xml\n", $0, a++ }' | bash
* Command to use Bag-It, cURL, and Wget to download a set of objects in a collection, and then copy out the TEI and MODS datastreams respectively, and rename the files using the parent directory name of each file (this assumes each parent directory name is unique; in this case, the directory name is the PID of the object) -- used to download a Bag-It ZIP archive of book objects from a collection in the Spanish Civil War project (the collection is "Prisoner of War Letters"; PID is the following: islandora:3ed517db-86e0-4b84-b589-1b050e406dec) and copy out the TEI (filename is "TEI.bin") and MODS (filename is "MODS.xml") datastreams
   * curl http://cwrc-apps-01.srv.ualberta.ca/islandora/object/islandora%3A3ed517db-86e0-4b84-b589-1b050e406dec/manage/bagit && wget http://cwrc-apps-01.srv.ualberta.ca/sites/default/files/private/BagItPHP/Bag-islandora_3ed517db_86e0_4b84_b589_1b050e406dec.zip; mkdir Bag-It; cd Bag-It; unzip ../Bag-*.zip; cd ..; mkdir TEI_files; for name in */*/*/*/TEI.bin; do cp "$name" "TEI_files/$(basename -- "$(dirname -- "$name")")_tei.xml"; done
   * curl http://cwrc-apps-01.srv.ualberta.ca/islandora/object/islandora%3A3ed517db-86e0-4b84-b589-1b050e406dec/manage/bagit && wget http://cwrc-apps-01.srv.ualberta.ca/sites/default/files/private/BagItPHP/Bag-islandora_3ed517db_86e0_4b84_b589_1b050e406dec.zip; mkdir Bag-It; cd Bag-It; unzip ../Bag-*.zip; cd ..; mkdir MODS_files; for name in */*/*/*/MODS.xml; do cp "$name" "MODS_files/$(basename -- "$(dirname -- "$name")")_mods.xml"; done
* Command to use a cURL iTQL query to obtain a list of PIDs in a collection, and then use cURL to download the TEI and MODS datastreams respectively -- used to download the TEI and MODS datastreams from book objects from a collection in the Spanish Civil War project (the collection is "Prisoner of War Letters"; PID is the following: islandora:3ed517db-86e0-4b84-b589-1b050e406dec) (Note: this command also uses the Fedora Resource Index API to obtain the list of PIDs)
   * curl -s "http://cwrc-apps-01.srv.ualberta.ca:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Frelations-external%23isMemberOfCollection%3E%0A%3Cinfo%3Afedora%2Fislandora%3A3ed517db-86e0-4b84-b589-1b050e406dec%3E" | sed '1d;s/info:fedora\///g' >> pids.txt; mkdir TEI_files; cd TEI_files; for PID in `cat ../pids.txt`; do curl http://cwrc-apps-01.srv.ualberta.ca/islandora/object/$PID/datastream/TEI/download > $PID"_tei.xml"; mv $PID"_tei.xml" `echo $PID"_tei.xml" | sed s/:/_/`; done; cd ..
   * curl -s "http://cwrc-apps-01.srv.ualberta.ca:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Frelations-external%23isMemberOfCollection%3E%0A%3Cinfo%3Afedora%2Fislandora%3A3ed517db-86e0-4b84-b589-1b050e406dec%3E" | sed '1d;s/info:fedora\///g' >> pids.txt; mkdir MODS_files; cd MODS_files; for PID in `cat ../pids.txt`; do curl http://cwrc-apps-01.srv.ualberta.ca/islandora/object/$PID/datastream/MODS/download > $PID"_mods.xml"; mv $PID"_mods.xml" `echo $PID"_mods.xml" | sed s/:/_/`; done; cd ..


Notes on using the Solr Admin UI and the Solr REST API to query the Apache Solr index


* Apache Solr is written in Java
* URL to the Solr Admin UI: http://beta.cwrc.ca:8080/solr/admin/
* Solr Admin UI documentation portal: https://cwiki.apache.org/confluence/display/solr/Using+the+Solr+Administration+User+Interface
* Common Solr query parameters: https://wiki.apache.org/solr/CommonQueryParameters
* List of "fl" fields can search on: https://16884266100116878211.googlegroups.com/attach/7d3ac0008a38453b/html_file1%20-%20FGS%20Record%20-%20berklee.202%20-%20How%20High%20The%20Moon.html?part=0.1&view=1&vt=ANaJVrFAc5gw2YWfgG1bbVDryI1fJUgO9b5kYGzU7OGBxENo_1d_GpyhiQ8L1fJ7lPhQ3paprmAEHXWSWi-U_9DGX13pcMFV2_Fmaq49A8a3SjJywRdTYvc
* Solr Luke request handler (LukeRequestHandler)
   * Documentation on the Luke request handler
      * https://wiki.apache.org/solr/LukeRequestHandler
   * Luke request handler query to output all Solr index data
      * http://beta.cwrc.ca:8080/solr/admin/luke
   * Luke request handler query to output all Solr index data, with XSLT stylesheet to visualize Luke output
      * http://beta.cwrc.ca:8080/solr/admin/luke?wt=xslt&tr=luke.xsl
   * AJAXified schema browser to visualize Luke output
      * http://beta.cwrc.ca:8080/solr/admin/schema.jsp
   * Luke request handler to return the top 100 terms from the "dc.description" field
      * http://beta.cwrc.ca:8080/solr/admin/luke?fl=dc.description&numTerms=100
   * Luke request handler to return a document using the document's PID
      * http://beta.cwrc.ca:8080/solr/admin/luke?id=islandora%3A75a7614e-fd23-431a-ae36-6f521fa83990
* Notes:
   * useful Solr commands that can be used in cleanup and data integrity operations are identified below by the parenthetical phrase "(cleanup and data integrity operation)" at the end of the statement explaining the command that precedes each command
   * there are 25,560 indexed Solr fields 
* Solr query to return all Solr fields (Solr field names) from the Solr index
   * http://roam.macewan.ca:8080/solr/select?q=*:*&wt=csv&rows=0&facet
* cURL command to return all Solr fields
   * curl -s "http://roam.macewan.ca:8080/solr/select?q=*:*&wt=csv&rows=0&facet" | sed s/,/\\n/g | sort > solr-fields.txt
* cURL command to return all MODS Solr fields
   * curl -s "http://roam.macewan.ca:8080/solr/select?q=*:*&wt=csv&rows=0&facet&fl=mods_*" | sed s/,/\\n/g | sort > mods-solr-fields.txt
* Solr query to return all MODS Solr field names
   * http://roam.macewan.ca:8080/solr/select?q=*:*&wt=csv&rows=0&facet&fl=mods_*
* Solr query to return all searchable Solr documents (note: that is asterisk-colon-asterisk below)
   * *:*
* Solr query to return the Solr document for the BIBLIFO root collection container object using its PID
   * http://cwrc-apps-01.srv.ualberta.ca:8080/solr/select/?q=PID:*abda880b-55d8-4d4a-b08a-e919e77e9a1b&version=2.2&start=0&rows=10&indent=on
   * curl -s "http://cwrc-apps-01.srv.ualberta.ca:8080/solr/select/?q=PID:*abda880b-55d8-4d4a-b08a-e919e77e9a1b&version=2.2&start=0&rows=10&indent=on"
* Solr query to return all BIBLIFO collection objects’ Solr documents -- will return 3,445 documents for the example query below; use the RELS_EXT_isMemberOfCollection_uri_ms key, then colon (":"), then asterisk ("*"), then PID (minus the namespace and colon) for the isMemberOfCollection BIBLIFO object
   * http://beta.cwrc.ca:8080/solr/select/?q=RELS_EXT_isMemberOfCollection_uri_ms:*abda880b-55d8-4d4a-b08a-e919e77e9a1b&version=2.2&start=0&rows=99999&indent=on
   * aside: the islandora_collection_search Drupal module uses the Solr ancestor_ms field (which contains the object ancestors of the object in question, all the way up to the root) to query an entire collection (so since it is using the ancestor_ms field, and not the RELS_EXT_isMemberOfCollection_uri_ms field, it can go deeper than one level)
      * http://beta.cwrc.ca/islandora/search/*?type=edismax&cp=islandora:abda880b-55d8-4d4a-b08a-e919e77e9a1b
* Solr query to return all Solr documents for the page objects from the "Mother and Wives of Winnipeg" book object -- will return 3 Solr documents for the example query below; use the RELS_EXT_isMemberOf_uri_ms key (could also use the RELS_EXT_isPageOf_uri_ms key), then colon (":"), then asterisk ("*"), then PID (minus the namespace and colon) for the book object or critical edition object (isMemberOf page object)
   * http://beta.cwrc.ca:8080/solr/select/?q=RELS_EXT_isPageOf_uri_ms:*569bb17a-6f7a-4fff-a4d4-1d4500731a1f&version=2.2&start=0&rows=10&indent=on
* Solr query to return all Solr documents without the ancestors_ms field (cleanup and data integrity operation)
   * http://beta.cwrc.ca:8080/solr/select?q=-ancestors_ms%3A*&start=0&rows=99999&version=1.2&wt=json&json.nl=map&sort=PID+asc&fl=PID+ancestors_ms+isMemberOfCollection_uri_mt+RELS_EXT_isMemberOfCollection_uri_ms+RELS_EXT_isMemberOf_uri_ms+RELS_EXT_hasModel_uri_ms+fgs_label_s&indent=true
* Solr query to return all Solr documents without the dc.title field (cleanup and data integrity operation)
   * http://beta.cwrc.ca:8080/solr/select?q=-dc.title%3A*&start=0&rows=99999&version=1.2&wt=json&json.nl=map&sort=PID+asc&fl=PID+dc.title+isMemberOfCollection_uri_mt+RELS_EXT_isMemberOfCollection_uri_ms+RELS_EXT_isMemberOf_uri_ms+RELS_EXT_hasModel_uri_ms+fgs_label_s&indent=true
* Solr query to query the ancestors_ms field to return all the dc.title field entries for the CEWW top-level collection, but omitting the Solr documents for the page objects (the objects with an isMemberOf relationship to their parent)
   * http://beta.cwrc.ca:8080/solr/select?q=ancestors_ms%3A*dbdbd5af-fc3a-4f91-9be8-9ad8499fa8af%20AND%20-RELS_EXT_isMemberOf_uri_ms%3A[*%20TO%20*]&start=0&rows=99999&version=1.2&wt=json&json.nl=map&fl=dc.title&indent=true
* Solr query to return all the ECCJI MODS article records that have the string pattern "'S " in the dc.title field, and the returned records have the fields PID and dc.title in XML format (3,168 ECCJI MODS article records have this pattern)
   * http://beta.cwrc.ca:8080/solr/select/?q=dc.title%3A*%27S*%20%20AND%20RELS_EXT_isMemberOfCollection_uri_ms:*404b485e-0969-45c8-9fe2-dcb46b174c0f&version=2.2&start=0&rows=999999&indent=on&fl=PID+dc.title
* Solr query to return all of the CEWW person entity records, so searching for the string "ceww" in the cwrc_entity_project_id_ms field in the person_2015_05_22 collection, and the returned records have the fields PID, dc.title, and cwrc_entity_project_id_ms in XML format (843 CEWW person entities are returned)
   * http://beta.cwrc.ca:8080/solr/select/?q=cwrc_entity_project_id_ms%3A*ceww*%20%20AND%20RELS_EXT_isMemberOfCollection_uri_ms:*person_2015-05-22&version=2.2&start=0&rows=999999&indent=on&fl=PID+dc.title
* Solr query to return all the Solr documents that have the schema object PID of "cwrc:teiSchema" (note: removed the namespace "cwrc" and colon ":" from the beginning of the PID in the Solr query) in the RELS_EXT_hasSchema_uri_ms Solr field, and the returned records have the fields PID and dc.title in XML format
   * http://beta.cwrc.ca:8080/solr/select/?q=RELS_EXT_hasSchema_uri_ms:*teiSchema&version=2.2&start=0&rows=999999&indent=on&fl=PID+dc.title
* Solr query to return in XML format the PID and dc.title of all Solr documents that have the CModel object PID of “cwrc:documentCModel” (note: removed the namespace "cwrc" and colon ":" from the beginning of the PID in the Solr query) in the RELS_EXT_hasModel_uri_ms Solr field
   * http://beta.cwrc.ca:8080/solr/select/?q=RELS_EXT_hasModel_uri_ms:*documentCModel&version=2.2&start=0&rows=999999&indent=on&fl=PID+dc.title
* Lynx browser command to query Solr
   * Note: need to use parentheses to use Boolean operators, e.g., "(term1 OR term2)"
   * Lynx "g" ("go to") command to query "abdy NOT maria"
      * http://localhost:8080/solr/select?sort=%22fgs.label%22+asc&facet=true&facet.mincount=2&facet.limit=20&facet.field=&fq=hasDatastream%3APERSON&fq=rels.isViewableByUser%3A%22anonymous%22+OR+rels.isViewableByRole%3A%22anonymous+user%22+OR+%28%28%2A%3A%2A+-rels.isViewableByUser%3A%5B%2A+TO+%2A%5D%29+AND+%28%2A%3A%2A+-rels.isViewableByRole%3A%5B%2A+TO+%2A%5D%29%29&version=1.2&wt=json&json.nl=map&q=fgs.label%3A(abdy%20NOT%20maria)&start=0&rows=100
* cURL command to return the number of internal Solr document identifiers (Solr maxDoc attribute) (beta server's xmllint utility is old, and doesn't have "--xpath" option -- therefore second command won't work)
   * curl -s "http://localhost:8080/solr/admin/mbeans?stats=true" | xmllint --format - | grep '<int name=\"maxDoc\">' -A1 | head -n 1 | sed -e 's/^[ \t]*//;s/[ \t]*$//'
   * curl -s "http://localhost:8080/solr/admin/stats.jsp" | xmllint --xpath "/response/lst[2]/lst[1]/lst[1]/lst[1]/int[2][@name='maxDoc'][1]/text()" -
   * curl -s "http://localhost:8080/solr/admin/mbeans?stats=true" | xml sel -T -t -v "/response/lst[2]/lst[1]/lst[1]/lst[1]/int[2][@name='maxDoc'][1]/text()" - 2>/dev/null
* cURL command to return the number of searchable Solr documents (Solr numDocs attribute) (beta server's xmllint utility is old, and doesn't have "--xpath" option -- therefore second command won't work)
   * curl -s "http://localhost:8080/solr/admin/mbeans?stats=true" | xmllint --format - | grep '<int name=\"numDocs\">' -A1 | head -n 1 | sed -e 's/^[ \t]*//;s/[ \t]*$//'
   * curl -s "http://localhost:8080/solr/admin/mbeans?stats=true" | xmllint --xpath "/response/lst[2]/lst[1]/lst[1]/lst[1]/int[1][@name='numDocs'][1]/text()" -
   * curl -s "http://localhost:8080/solr/admin/mbeans?stats=true" | xml sel -T -t -v "/response/lst[2]/lst[1]/lst[1]/lst[1]/int[1][@name='numDocs'][1]/text()" - 2>/dev/null
   * curl -s "http://localhost:8080/solr/select?q=*:*&fl=*" | tidy -xml -wrap 0 2>/dev/null | grep numFound | sed s"/<result name=\"response\" numFound=\"//g;s/\" start=\"0\">//g"
* Solr query to output various statistics, including maxDoc (number of internal Solr document identifiers), and numDocs (number of searchable Solr documents)
   * http://devlibir2.ca:8080/solr/admin/mbeans?stats=true
* Solr query to return the number of searchable Solr documents which is numFound
   * http://devlibir2.macewan.ca:8080/solr/select?q=*:*&version2.2&rows=0
* Wget command to return all Solr documents without the ancestors_ms field (cleanup and data integrity operation)
   * wget -O no_ancestors.txt 'http://beta.cwrc.ca:8080/solr/select?q=-ancestors_ms%3A*&start=0&rows=99999&version=1.2&wt=json&json.nl=map&sort=PID+asc&fl=PID+ancestors_ms+isMemberOfCollection_uri_mt+RELS_EXT_isMemberOfCollection_uri_ms+RELS_EXT_isMemberOf_uri_ms+RELS_EXT_hasModel_uri_ms+fgs_label_s&indent=true'
* Wget command to return all Solr documents without the dc.title field (cleanup and data integrity operation)
   * wget -O no_dc.title.txt 'http://beta.cwrc.ca:8080/solr/select?q=-dc.title%3A*&start=0&rows=99999&version=1.2&wt=json&json.nl=map&sort=PID+asc&fl=PID+dc.title+isMemberOfCollection_uri_mt+RELS_EXT_isMemberOfCollection_uri_ms+RELS_EXT_isMemberOf_uri_ms+RELS_EXT_hasModel_uri_ms+fgs_label_s&indent=true'
* cURL command to query the ancestors_ms field to return all the dc.title field entries for the CEWW top-level collection,  but omitting the page objects (the objects with an isMemberOf relationship to their parent)
   * curl -s "http://beta.cwrc.ca:8080/solr/select?q=ancestors_ms%3A*dbdbd5af-fc3a-4f91-9be8-9ad8499fa8af%20AND%20-RELS_EXT_isMemberOf_uri_ms%3A\[*%20TO%20*\]&start=0&rows=99999&version=1.2&wt=csv&fl=dc.title&indent=true" | sed '1d' | sort | tee ceww_titles.txt
   * curl -s "http://beta.cwrc.ca:8080/solr/select?q=ancestors_ms%3A*dbdbd5af-fc3a-4f91-9be8-9ad8499fa8af%20AND%20-RELS_EXT_isMemberOf_uri_ms%3A\[*%20TO%20*\]&start=0&rows=99999&version=1.2&wt=csv&fl=dc.title&indent=true" | sed "1d" | gsort | tee ceww_titles.txt (GnuWin32 version)
* cURL command to output the PIDs for all Solr documents that have the Solr field mods_titleInfo_title_ms and thus have MODS datastreams
   * curl -s "http://beta.cwrc.ca:8080/solr/select/?q=mods_titleInfo_title_ms%3A*&version=2.2&start=0&rows=999999&indent=on&fl=PID" | grep "str name=\"PID\"" | sed "s/<str name=\"PID\">//g;s/<\/str>//g" | sed -e 's/^[ \t]*//;s/[ \t]*$//' | sed "s/<\/doc>//g" | sort | tee solr_pids.txt
* cURL command to output the PIDs for all Solr documents
   * curl -s "http://localhost:8080/solr/select/?q=*:*&version=2.2&start=0&rows=999999&indent=on&fl=PID&wt=csv" | sed "1d" | sort | tee solr_pids.txt
   * curl -s "http://localhost:8080/solr/select/?q=*:*&version=2.2&start=0&rows=999999&indent=on&fl=PID" | grep "str name=\"PID\"" | sed "s/<str name=\"PID\">//g;s/<\/str>//g" | sed -e 's/^[ \t]*//;s/[ \t]*$//' | sed "s/<\/doc>//g" | sort | tee solr_pids.txt
      * number of Solr documents
         * cat solr_pids.txt | wc -l
      * number of EMiC Solr documents
         * cat solr_pids.txt | sort | grep "emic" | wc -l
* commands to determine the total size of all Fedora objects in the repository -- beware!  7.20 GB download!  (entire Solr index)
   * curl -s "http://localhost:8080/solr/select/?q=*:*&version=2.2&start=0&rows=999999&indent=on" | tee cwrc-size.xml
   * grep "fedora_datastream_latest_.*_SIZE_ms" cwrc-size.xml | sed 's/<arr name="fedora_datastream_latest_.*_SIZE_ms"><str>//g;s/<\/str><\/arr>//g' | sed -e 's/^[ \t]*//;s/[ \t]*$//' | awk '{s+=$1} END {print s/(1024*1024*1024) " GB"}'
* commands to determine the total size of all EMiC Fedora objects in the repository -- 0.91 GB download from the Solr index
   * curl -s "http://localhost:8080/solr/select/?q=PID:*emic*&version=2.2&start=0&rows=999999&indent=on" | tee emic-size.xml
   * grep "fedora_datastream_latest_.*_SIZE_ms" emic-size.xml | sed 's/<arr name="fedora_datastream_latest_.*_SIZE_ms"><str>//g;s/<\/str><\/arr>//g' | sed -e 's/^[ \t]*//;s/[ \t]*$//' | awk '{s+=$1} END {print s/(1024*1024*1024) " GB"}'
* cURL command to output the PIDs for all Solr documents, but filtering out Fedora objects with the following characteristics: objects with a CModel of "islandora:collectionCModel"; and objects with a last modified date before the provided date of 2014-06-11T19:38:02.49Z
   * curl -s "http://roam.macewan.ca:8080/solr/select/?q=*:*%20AND%20-RELS_EXT_hasModel_uri_s:*collectionCModel&fq=fgs_lastModifiedDate_dt:\[2014-06-11T19:38:02.49Z%20TO%20NOW\]&version=2.2&start=0&rows=999999&indent=on&fl=PID&wt=csv" | sed "1d" | sort -n -k1.4 | uniq | tee pids.txt


Procedures to selectively re-index in Solr unindexed Fedora objects


* Steps to determine Fedora objects that should have been indexed by Solr but were not indexed by Solr
   * If need to determine if any Fedora objects should have been indexed by Solr but weren't, calculate the number of Fedora objects, then the number of Solr documents, then the number of Fedora objects that will be omitted from Solr indexing due to conditional logic contained in foxmlToSolr.xslt, and then subtract the latter two totals from the first total
      * 1. Number of Fedora objects -- see above for command
      * 2. Number of Solr documents -- see above for command
      * 3. Number of Fedora objects that will be omitted from Solr indexing -- see above for command
      * Number of Fedora objects that should have been indexed by Solr but weren't
         * = 1 - (2 + 3)
   * If the above figure is not 0, and therefore need to determine the PIDs of the Fedora objects that should have been indexed but weren't, generate the PIDs for all Fedora objects and sort and save this result, then generate the PIDs for all Solr documents and sort and save this result, then run a "comm -23" on those two results and sort and then save this result, then generate the PIDs for the Fedora objects that won't be indexed and sort and save this result, and then "comm -23" the first "comm -23" result with this last saved result for Fedora objects that won't be indexed
      * 1. PIDs for all Fedora objects -- see above for command
      * 2. PIDs for all Solr documents -- see above for command
      * 3. PIDs of all Fedora objects not in Solr
         * comm -23 1.command-result 2.command-result | sort > 3.command-result 
      * 4. PIDs for Fedora objects that will not ever be indexed by Solr -- see above for command
      * 5. PIDs of Fedora objects that should have been indexed by Solr, but were not
         * comm -23 3.command-result 4.command-result | sort > 5.command-result
   * If need to re-index the Fedora objects that were identified as not indexed in Solr but they should have been indexed in Solr, run this command to have Solr update the index for a particular Fedora object using that object's PID
      * curl -s -m 15 -u fedoraAdmin:Fedora_Admin_password_here "http://localhost:8080/fedoragsearch/rest?operation=updateIndex&action=fromPid&value=PID_value_here"
      * shell script to batch re-index a set of Solr documents using a list of PIDs obtained from a file -- the  shell script file name is called "re-index.sh"
#/bin/bash


pids_file="ceww-pids.txt"
total_pids=`wc -l < $pids_file`
pid_count=0
echo "### Begin the batch re-indexing of" $total_pids "Solr documents."
echo "**********************************************************************"
while read pid; do
    pid_count=`expr $pid_count + 1`
    echo "Processing Solr document number" $pid_count "of" $total_pids "Solr documents . . ."
    curl -s -m 15 -u fedoraAdmin:fedoraAdminPasswordHere "http://localhost:8080/fedoragsearch/rest?operation=updateIndex&action=fromPid&value=$pid" 2>&1 >/dev/null
echo "     Solr document number" $pid_count "of" $total_pids "with pid"
echo "    " $pid
echo "     has been re-indexed in the Solr index . . ."
done < $pids_file
echo "**********************************************************************"
echo "The batch re-indexing of" $total_pids "Solr documents is completed."


Procedures to batch delete a set of Fedora objects from a list of PIDs


* To delete Fedora objects
   * Two phases: (1) iTQL tuple select query to obtain list of object PIDs from a collection (put list of object PIDs in a file), and (2) run bash script to delete objects based on list of object PIDs
      * (1) iTQL tuple query to select all PIDs in a specified collection as well as the collection container object itself 
* itql - CSV - unlimited
* itql is under Language, CSV is under Response, unlimited is under Limit, and in the Advanced pane the "Fake Media-Types" is checked (default)
* URL to iTQL query page on dev: https://cwrc-dev-01.srv.ualberta.ca:8443/fedora/risearch
* URL to iTQL query page on beta: http://beta.cwrc.ca:8080/fedora/risearch
* query statement:
select $identifier from <#ri>
where (
  $object <dc:identifier> $identifier
  and
  (
    $object <fedora-rels-ext:isMemberOfCollection>
      <info:fedora/islandora:4ae6cb3e-9ac3-48a3-9078-9c12ec29227f>
    or
    $object <dc:identifier>
      'islandora:4ae6cb3e-9ac3-48a3-9078-9c12ec29227f'
  )
  and
  $object <fedora-model:state>
    <info:fedora/fedora-system:def/model#Active>
)
order by $identifier
      * (2) shell script to run to delete objects by list of PIDs: 
         * script located here: https://github.com/cwrc/CWRC-Misc/blob/master/migration/scripts/fedora-purge-by-list.sh
         * when run the shell script, need to provide
            * path to file, e.g., file:///home/brundin/list-pids.txt
            * target repository: localhost:8080
            * username: fedoraAdmin
            * password: fedoraAdmin’s password
            * press "y" in response to prompt if you want to delete Fedora objects


Procedures to obtain the number and file size of Fedora objects in a collection


Procedures to obtain the number of Fedora objects for, and the file size of, a particular collection -- will use the CEWW top-level collection as an example throughout; note: could also query, return, and parse the FOXML object to calculate the size of the object, but querying, returning, and parsing the Solr document for that Fedora object to determine the size has certain advantages (e.g., much fewer cURL queries required, smaller overall download sizes, slightly easier ability to parse appropriate size values)
* Total number of Fedora objects in a collection (including descendants)
   * Run an iTQL query using the Mulgara walk function with the count function which will count all the Fedora object descendants of the collection (using the isMemberOfCollection predicate) except the children of book objects and critical edition objects (which both use both the isMemberOf and isPageOf predicates to indicate their parent, rather than the isMemberOfCollection predicate).  Record this number. (3379)
select $child from <#ri> 
where walk ( 
    $child 
    <fedora-rels-ext:isMemberOfCollection> 
    <info:fedora/islandora:dbdbd5af-fc3a-4f91-9be8-9ad8499fa8af> 
  and 
    $child 
    <fedora-rels-ext:isMemberOfCollection> 
    $parent 
)
  or
    $child
    <dc:identifier>
    'islandora:dbdbd5af-fc3a-4f91-9be8-9ad8499fa8af';
      * The book objects belong to the islandora:bookCModel and islandora:iaBookCModel, and the critical edition objects belong to the islandoracriticalEditionCModel; there are also some "wrapper" or container critical edition objects, including the islandora:criticalEditionContainerCModel (4 objects), the islandora:criticalApparatusCModel (3 objects), and the islandora:criticaleditionschemaCModel (1 object) -- these all use the isMemberOfCollection predicate to establish the relationship with their parent, and they also only account for a handful of objects that use these other critical edition CModels, so don't need to worry about these objects.
      * The child objects of the islandora:bookCModel and islandora:iaBookCModel belong to the islandora:pageCModel, and as noted, they establish their relationship to their parent book model via the isMemberOf and isPageOf predicates.  The child objects of the islandora:criticalEditionCModel establish their relationship to their parent critical edition objects also via the isMemberOf and isPageOf predicates.
   * Run this same query above, but instead of counting the Fedora objects, output and save the PIDs of all the objects in the collection -- except, as noted, it won't include the PIDs of the children of the book and critical edition objects.  Trim, sort, and save this result set.
   * Run an iTQL query to output and save the the PIDs of all CWRC repository objects that belong to either the islandora:bookCModel, the islandora:iaBookCModel, or the islandora:criticalEditionCModel (will use a "Boolean OR" iTQL operation).  Trim, sort, de-duplicate, and save this result set.
select $object from <#ri> where
$object <info:fedora/fedora-system:def/model#hasModel>
     <info:fedora/islandora:bookCModel>
  or
$object <info:fedora/fedora-system:def/model#hasModel>
     <info:fedora/islandora:iaBookCModel>
  or
$object <info:fedora/fedora-system:def/model#hasModel>
     <info:fedora/islandora:criticalEditionCModel>
order by $object;
   * Using the Unix "comm -12" (common) command, output and save the intersection (the PIDs that are in both result sets) of all the PIDs in the collection (that is, the objects in the collection with an isMemberOfCollection RELS-EXT value) and the PIDs of the repository objects that belong to either the book objects or the critical edition objects.  This will result in a list of PIDs for the book objects and critical edition objects in the collection of interest. (15 PIDs for book objects or critical edition objects)
   * For each of the PIDs in this last result set (the PIDS of the book objects or critical edition objects of the collection of interest), run an iTQL count query to count the child objects of each of these book or critical edition objects, using the isMemberOf predicate (could also use the isPageOf predicate, if you wish).  Record the totals of each of these operations.  (1+5+4+6+2+3+1+2+4+1+2+1+5+5+6 = 48 page objects)
select $object
from <#ri>
where $object <info:fedora/fedora-system:def/relations-external#isMemberOf>
<info:fedora/cwrc:8d92cfbc-a0c5-4eb9-a87e-9a8937472366>


bash script (reads in a file called PIDs.txt that contains the PIDs, one PID per line):


#/bin/bash
numPagesTotal=0
while read pid; do
    numPages=`curl -s "http://cwrc-apps-01.srv.ualberta.ca:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20count%20(%0Aselect%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Frelations-external%23isMemberOf%3E%0A%3Cinfo%3Afedora%2F"$pid"%3E)%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Frelations-external%23isMemberOf%3E%0A%3Cinfo%3Afedora%2F"$pid"%3E%3B" | tail -1`  
    numPagesTotal="$((numPagesTotal + numPages))"
    numPages=0
done < ./PIDs.txt
echo $numPagesTotal page objects


   * Add each of these child book or critical edition page object totals to the first total you calculated for the number of "isMemberOfCollection" objects.  This grand total is the total number of Fedora objects in the collection of interest. (3379 + 48 = 3,427 objects)
   * Since we know that all of the page objects are children of islandora:bookCModel objects that reside in the “Bibliographic Records -- Primary Sources Collection”, we can also run the following iTQL query to generate the PIDs, and count by changing the Response to Count, of the page objects in the CEWW collection
select $page
from <#ri>
where $object <info:fedora/fedora-system:def/relations-external#isMemberOfCollection>
<info:fedora/islandora:2d7da0e4-e719-4e52-8037-0c064132b4bf>
and
$page <info:fedora/fedora-system:def/relations-external#isMemberOf>
$object;


* Total file size used by Fedora objects in a collection (including descendants)
   * As above, run the appropriate commands to generate a list of PIDs for all objects in the collection (except for book page objects and critical edition page objects), and a list of PIDs for the book objects and critical edition objects in the collection of interest.
   * In addition, run an iTQL command to output the PIDs of all CWRC repository objects that belong to the islandora:collectionCModel.  Trim, sort, and save this result set.
select $object from <#ri> where
$object <info:fedora/fedora-system:def/model#hasModel>
     <info:fedora/islandora:collectionCModel>
order by $object;
   * Using the result set of PIDs for all the "isMemberOfCollection" objects in the collection, run the Unix "comm -12" command to get the intersection of the "isMemberOfCollection" collection object PIDs and the "islandora:collectionCModel" repository object PIDs.  This will result in the set of PIDs for objects in the collection that belong to the islandora:collectionCModel. (4 collection objects, including the collection's root container object)  Remove the initial "info:fedora/" string and the namespace and colon at the beginning of each PID.
   * Using the result set of PIDs for all the book or critical edition objects in the repository, run the Unix "comm -12" command to get the intersection of the book or critical edition repository object PIDs and the "isMemberOfCollection" objects in the collection PIDs.  This will result in the set of PIDs for book or critical edition objects in the collection.  (15 book or critical edition objects)  Remove the initial "info:fedora/" string and the namespace and colon at the beginning of each PID.
comm -12 repo-page-pids.txt ceww-pids.txt | sed "s/info:fedora\/islandora://g" > ceww-page-pids.txt
   * Run the appropriate cURL command to query Solr and download the results for the collection's root collection container object -- will use the Solr PID key in the query.  (1 query needed)
curl -s "http://cwrc-apps-01.srv.ualberta.ca:8080/solr/select/?q=PID:*dbdbd5af-fc3a-4f91-9be8-9ad8499fa8af&version=2.2&start=0&rows=10&indent=on" | tee collection-root-file.xml (1 query needed)
   * Run the appropriate cURL command to query Solr and download the results for each of the collection's islandora:collectionCModel PIDs -- will use the Solr RELS_EXT_isMemberOfCollection_uri_ms key in the query.  (4 queries needed)
curl -s "http://cwrc-apps-01.srv.ualberta.ca:8080/solr/select/?q=RELS_EXT_isMemberOfCollection_uri_ms:*2d7da0e4-e719-4e52-8037-0c064132b4bf&version=2.2&start=0&rows=999999&indent=on" | tee 16-file.xml
   * Run the appropriate cURL command to query Solr and download the results for each of the collection's islandora:bookCModel, islandora:iaBookCModel, and islandora:criticalEditionCModel objects (the list of PIDs for the book objects and critical edition objects -- remember to remove the namespace and colon from the beginning of each PID) --  will use the Solr RELS_EXT_isMemberOf_uri_ms key in the query.  (15 queries needed using the isMemberOf predicate)
curl -s "http://cwrc-apps-01.srv.ualberta.ca:8080/solr/select/?q=RELS_EXT_isMemberOf_uri_ms:*8d92cfbc-a0c5-4eb9-a87e-9a8937472366&version=2.2&start=0&rows=999999&indent=on" | tee 01-file.xml
   * Using the Unix "cat" command, concatenate all of the downloaded files from the two sets of operations above into one single file.
   * Using the appropriate Unix command, calculate the total file size of the collection of interest -- the default output is in gigabytes (the total in bytes is converted to gigabytes by dividing the total bytes in the AWK command by 1024*1024*1024), so for most collections (which won't be more than a gigabyte is size), change the command so that it outputs the total collection file size in megabytes (i.e., divide by 1024*1024).  This grand total is the total file size of the Fedora objects in the collection of interest.
grep "fedora_datastream_latest_.*_SIZE_ms" ceww-size.xml | sed 's/<arr name="fedora_datastream_latest_.*_SIZE_ms"><str>//g;s/<\/str><\/arr>//g' | sed -e 's/^[ \t]*//;s/[ \t]*$//' | awk '{s+=$1} END {print s/(1024*1024) " MB"}'


Determining number of objects, number of datastreams, and size of objects from FOXML objects and Solr documents


Notes:
* There are two approaches that can be used to calculate the measures: querying, downloading, and parsing FOXML objects from the collection; and querying, downloading, and parsing Solr documents from the collection.
* The PIDs that make up the collection can be obtained using the sequence of commands outlined above.  A Bash loop can then be used to query either the Fedora API or the Solr API to download each of the objects/documents represented by the PIDs.
* The unit of analysis, or unit of measurement, to define a datastream are entities that have the predicate properties of <info:fedora/fedora-system:def/view#disseminates> and <info:fedora/fedora-system:def/view#disseminationType>; this definition therefore excludes the AUDIT datastream, which also doesn't have any SIZE value information.
* The size of the object is technically the size of the sum of the datastreams of the object, which will be slightly less than the true size of the object (as the FOXML part of the object is not included).


Parsing the FOXML object:
* Root collection label name:
   * xmllint --xpath "/*[local-name()='digitalObject']/*[local-name()='datastream']/*[local-name()='datastreamVersion']/*[local-name()='xmlContent']/*[local-name()='dc']/*[local-name()='title']/text()" biblifo_foxml.xml
   * xml sel -T -N dc="http://purl.org/dc/elements/1.1/" -N oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" -t -v "/foxml:digitalObject/foxml:datastream/foxml:datastreamVersion/foxml:xmlContent/oai_dc:dc/dc:title" biblifo_foxml.xml
   * grep "<dc:title>" biblifo_foxml.xml | sed "s/<dc:title>//g;s/<\/dc:title>//g" | sed -e 's/^[ \t]*//;s/[ \t]*$//'
* Number of objects:
   * xmllint --xpath "count(/*[local-name()='digitalObject'])" biblifo_foxml.xml
   * xml sel -T -t -v "count(foxml:digitalObject)" biblifo_foxml.xml
   * grep "<foxml:digitalObject" biblifo_foxml.xml | wc -l
* Number of datastreams:
   * xmllint --xpath "count(/*[local-name()='digitalObject']/*[local-name()='datastream'][not(@ID='AUDIT')]/@ID)" biblifo_foxml.xml
   * xml sel -T -t -v "count(/foxml:digitalObject/foxml:datastream[not(@ID='AUDIT')]/@ID)" biblifo_foxml.xml
   * grep "<foxml:datastream ID=" biblifo_foxml.xml | grep -v "AUDIT" | wc -l
* Size of object in bytes:
   * xmllint --xpath "/*[local-name()='digitalObject']/*[local-name()='datastream']/*[local-name()='datastreamVersion']/@SIZE" biblifo_foxml.xml | tr " " "\n" | sed "s/SIZE=\"//g;s/\"//g" | awk '{s+=$1} END {print s " bytes"}'
   * xml sel -T -t -v "/foxml:digitalObject/foxml:datastream/foxml:datastreamVersion/@SIZE" biblifo_foxml.xml | awk '{s+=$1} END {print s " bytes"}'
   * grep "SIZE" biblifo_foxml.xml | sed "s/^.*SIZE=\"//g;s/\">//g" | awk '{s+=$1} END {print s " bytes"}'


Parsing the Solr document:
* Root collection label name:
   * xmllint --xpath "/response/result/doc/arr[@name[starts-with(.,'dc') and contains(.,'title')]]/str/text()" biblifo_solr.xml
   * xml sel -T -t -v "/response/result/doc/arr[@name[starts-with(.,'dc') and contains(.,'title')]]/str" biblifo_solr.xml
   * grep -A1 "<arr name=\"dc.title\">" biblifo_solr.xml | tail -1 | sed "s/<str>//g;s/<\/str>//g" | sed -e 's/^[ \t]*//;s/[ \t]*$//'
* Number of objects:
   * xmllint --xpath "count(/response/result/doc/str[@name='PID'])" biblifo_solr.xml
   * xml sel -T -t -v "count(/response/result/doc/str[@name='PID'])" biblifo_solr.xml
   * grep "<str name=\"PID\">" biblifo_solr.xml | wc -l
* Number of datastreams:
   * xmllint --xpath "count(/response/result/doc/arr/@name[starts-with(.,'fedora_datastream_version_') and contains(.,'_SIZE_ms')])" biblifo_solr.xml
   * xml sel -T -t -v "count(/response/result/doc/arr/@name[starts-with(.,'fedora_datastream_version_') and contains(.,'_SIZE_ms')])" biblifo_solr.xml
   * grep "fedora_datastream_version_.*_SIZE_ms" biblifo_solr.xml | wc -l
* Size of object in bytes:
   * xmllint --xpath "/response/result/doc/arr[@name[starts-with(.,'fedora_datastream_version_') and contains(.,'_SIZE_ms')]]/str" biblifo_solr.xml | tr -s "<\/str><str>" "\n" | awk '{s+=$1} END {print s " bytes"}'
   * xml sel -T -t -v "/response/result/doc/arr[@name[starts-with(.,'fedora_datastream_version_') and contains(.,'_SIZE_ms')]]/str" biblifo_solr.xml | awk '{s+=$1} END {print s " bytes"}'
   * xmllint --format biblifo_solr.xml | sed -n -e '/fedora_datastream_version_.*_SIZE_ms/,/<\/arr>/ p' | grep "<str>" | sed 's/<str>//g;s/<\/str>//g' | sed -e 's/^[ \t]*//;s/[ \t]*$//' | awk '{s+=$1} END {print s " bytes"}'


Notes on using Drush (Drupal shell) scripts


* Notes on Drush commands:
   * to use the correct version of PHP for the Drush commands on beta.cwrc.ca, run this command beforehand:
      * source /opt/rh/php54/enable
   * Remember to run the Drush commands from this directory path
      * /var/www/html/drupal7


* Steps to batch ingest CWRC entities (person, organization, and place entities)
* Uses the cwrc_migration_batch module
* Documentation and information about the Drush shell entity ingest script can be found on the cwrc-apps-07.srv.ualberta.ca server; use this command
   * vim -R /data/README.dir/README-cwrc-apps.cmd.txt
* Unzip person, organization, and place entity records in home directory tree on cwrc-dev-01.srv.ualberta.ca
   * E.g.,
      * /home/brundin/person_entities/data/
      * /home/brundin/organization_entities/data/
      * /home/brundin/place_entities/data/
* Directory on cwrc-dev-01.srv.ualberta.ca to run the drush commands is the following:
   * /var/www/html/drupal7/sites/all/modules/cwrc_migration_batch
* Directory on beta.cwrc.ca and cwrc-dev-05.srv.ualberta.ca to run the drush commands is the following:
   * /var/www/html/drupal7/sites/default/modules/cwrc_migration_batch
* drush commands to use for each type of entity (issued from the directory path above)
   * person entities
  drush -u 1  cwrc_migration_batch_ingest_entities \
  /home/brundin/person_entities/data/ \
  /xml/cwrc_entity_default_workflow.xml \
  cwrc:person_2015-02-19 \
  cwrc:person-entityCModel \
  'PERSON' \
  'Person XML Content' \
  '/../cwrc_entities/xslt/entity_to_dc.xsl' \
  > /tmp/cwrc:person_2015-02-19
   * organization entities
  drush -u 1  cwrc_migration_batch_ingest_entities \
  /home/brundin/organization_entities/data/ \
  /xml/cwrc_entity_default_workflow.xml \
  cwrc:organization_2015-02-19 \
  cwrc:organization-entityCModel \
  'ORGANIZATION' \
  'Organization XML Content' \
  '/../cwrc_entities/xslt/entity_to_dc.xsl' \
  > /tmp/cwrc:organization_2015-02-19
   * place entities
  drush -u 1  cwrc_migration_batch_ingest_entities \
  /home/brundin/place_entities/data/ \
  /xml/cwrc_entity_place_workflow.xml \
  cwrc:place_2015-02-19 \
  cwrc:place-entityCModel \
  'PLACE' \
  'Place XML Content' \
  '/../cwrc_entities/xslt/entity_to_dc.xsl' \
  > /tmp/cwrc:place_2015-02-19
   * TPatT person entities
  drush -u 1  cwrc_migration_batch_ingest_entities \
  /home/brundin/tpatt/data/ \
  /xml/tpatt-workflow.xml \
  tpatt:f0a796c3-d251-4996-8cba-b7ad79729259 \
  cwrc:person-entityCModel \
  'PERSON' \
  'Person XML Content' \
  '/../cwrc_entities/xslt/entity_to_dc.xsl' \
  > /tmp/tpatt-person-2017-09-17
   * Notes on the Drush commands:
      * to background the process before the command is run, append a space and an ampersand at the end of the command string, i.e., “ &“ 
      * to background the process while the command is running
         * after the command is run, one can return to the Unix command prompt by typing "Ctrl-z"
         * then, at the command prompt, type "bg" to background the running process
      * to foreground the process again, if you are in the same terminal session, type “fg”; otherwise, can run “ps aux | grep brundin” to see the processes that are associated with user brundin
      * note: instead of backgrounding the processing could use the nohup command preceding the above commands
         * nohup command string &
         * hit the Enter key again to return to the Unix prompt
         * can view standard output in the nohup file
            * tail -f nohup
         * If need to send standard input to script at startup, can run nohup like so:
            * ( echo y | nohup command string ) &
         * For multiple lines of input:
            * ( echo -e "input1\ninput2\ninput3" | nohup command string ) &
      * Note: alternatives to nohup are GNU Screen, tmux, and disown
         * Screen (GNU)
            * screen (starts another shell running within screen)
            * command string
            * Ctrl-a d (disconnect you from the screen session)
            * screen -RD (reconnect to the screen session)
         * tmux (BSD) (not installed on beta)
         * disown
            * first, background the process using “ &” after the command, or “Ctrl-z” after the command has run and then typing “bg”
            * disown (bash detaches the job and allows it to continue running)
               * jobs (shows the jobs running by brundin)
               * jobs -p (shows the PIDs of the processes associated with the jobs running by brundin)
               * pidof process_name (returns the PID of the named process, e.g., process name of “sleep”)
               * kill 32293 (kills the process with PID “32293”)
               * E.g., (can examine this example to see how to kill process that the job was detached from):
                  * sleep 1000 &
                  * jobs -p (returned 32293)
                  * disown
                  * jobs -p (returned nothing, because jobs was detached)
                  * pid of sleep (returned 32293)
                  * kill 32293 (killed process 322293)
                  * pidof sleep (returned nothing, as the process was killed)
      * to view the processes that are running, type "ps -F" -- can also run the "top" command
      * the different elements that make up the array in the above commands are the following:
         * cwrc_migration_batch_ingest_entities
         * path_to_directory
         * name_of_default_workflow_file
         * collection_pid
         * CModel_pid
         * content_datastream_dsid
         * content_datastream_label
         * entity_to_dc_xslt_path
         * logfile_path
      * the elements that will, or might, change from one ingest session to the next are the following: source directory path, destination or target PID, and output log file name in the tmp directory
* Log file information on cwrc-dev-01.srv.ualberta.ca
   * output log file recording entities ingested using the drush command can be viewed using the following command:
      * tail /tmp/cwrc:place_2015-02-19
      * tail -f /tmp/cwrc:place_2015-02-19 (output appended data as the file grows)
   * Fedora GSearch event log file
      * /usr/local/fedora/server/logs
      * tail -100 /usr/local/fedora/server/logs/fedoragsearch.daily.log
   * Apache Tomcat logs
      * /usr/local/fedora/tomcat/logs
         * example logs (use ls -latr to sort log files by date; use -f flag to continuously update log file):
            * localhost_access_log.2015-07-17.txt
            * catalina.2015-07-17.log
   * Apache HTTP Server log files (use ls -latr to sort log files by date -- access log files and error log files)
      * /var/log/httpd


* Drush commands to batch ingest MODS records (can be used to batch ingest any type of file, including two sets of files in the ZIP archive, such as PDF and XML files)
   * notes:
      * Uses the islandora_batch module
      * GitHub location: https://github.com/Islandora/islandora_batch
      * two sets of commands
         * first command creates the set of objects that will be ingested
         * second command ingests the objects
      * can only ingest a set that has a maximum of 9900 (actually, 9984) objects
      * change "parent" value to the PID of the collection you are ingesting the MODS records into
      * change "target" so that it points to source zip file
      * as noted above, zip file can't exceed 9984 MODS records -- use 9900 MODS records as maximum number
      * change "ingest_set" in second command so that it is correct -- it is the set number generated after creating a set using the first command -- this can be determined by viewing Islandora's Reports > Islandora Batch Sets page; ingest_set is an optional paramter -- if it is omitted, then everything in the Batch Que will be ingested, which usually will only be your set
      * the two commands are the following:
   *  drush -v -u 1 --uri=http://beta.cwrc.ca islandora_batch_scan_preprocess --content_models=cwrc:citationCModel --parent=cwrc:00083420-ff6d-4ee0-8fca-b173778f8cc2 --parent_relationship_pred=isMemberOfCollection --type=zip --namespace=cwrc --parent_relationship_uri=info:fedora/fedora-system:def/relations-external# --scan_target=/home/brundin/eccji.zip
   * drush -v -u 1 --uri=http://beta.cwrc.ca islandora_batch_ingest [--ingest_set=55]


* Drush command to batch search and replace a string in a set of Fedora objects
   * islandora_update_metadata module
   * Robertson Library, UPEI GitHub link: https://github.com/roblib/scripts/blob/master/drush/drupal7/islandora_update_metadata.drush.inc
   * MODS: /mods:mods/mods:titleInfo/mods:title string change; used Drush command, with replacement text in “replace.txt”(first line in replace.txt: MRB test ~ MRB test book’s), and PIDs in “pids.txt”:
      * drush -u 1 --uri=http://beta.cwrc.ca/islandora islandoraum /home/brundin/test/pids.txt /home/brundin/test/replace.txt MODS "//ns:title" http://www.loc.gov/mods/v3
   * DC: /dc:title string change; used Drush command, with replacement text in “replace.txt” (first line in replace.txt: MRB test ~ MRB test book’s), and PIDs in “pids.txt”:
      * drush -u 1 --uri=http://beta.cwrc.ca/islandora islandoraum /home/brundin/test/pids.txt /home/brundin/test/replace.txt DC "//ns:title" http://purl.org/dc/elements/1.1/


* Drush command to batch push TEI files to Fedora objects
   * islandora_datastream_crud module
   * GitHub repository: https://github.com/mjordan/islandora_datastream_crud
   * TEI files are named using the formulation namespace_rest-of-pid_CWRC.xml
   * drush islandora_datastream_crud_push_datastreams --user=brundin --datastreams_source_directory=/home/brundin/rsc/tei --datastreams_mimetype=application/xml --datastreams_label="TEI Document" --datastreams_crud_log=/home/brundin/rsc/logs/crud-push-tei.log


* Drush command to batch fetch the REL-EXT datastreams for Fedora objects that are identified by a file containing the PIDs of the Fedora objects
   * islandora_datastream_crud module
   * drush islandora_datastream_crud_fetch_datastreams --user=brundin --pid_file=/home/brundin/rsc/pids.txt --dsid=RELS-EXT --datastreams_directory=/home/brundin/rsc/rels-ext --datastreams_extension=rdf


Commands to install a Drupal module


* Drupal is a content-management framework written in PHP
* Commands to install a Drupal module
   * Commands to install a Drupal module from the Drupal Web site
      * ssh cwrc-dev-01.srv.ualberta.ca
cd /var/www/html/drupal7/sites/all/modules
sudo wget http://ftp.drupal.org/files/projects/pmgrowl-7.x-1.x-dev.tar.gz
sudo tar -xzvf pmgrowl-7.x-1.x-dev.tar.gz
   * Commands to install a Drupal module from Islandora's GitHub Web site
      * ssh cwrc-dev-01.srv.ualberta.ca
cd /var/www/html/drupal7/sites/all/modules
sudo git clone https://github.com/discoverygarden/cwrc_workflow.git


GET methods for various LODs and APIs


* Islandora REST API on beta
   * describe a BIBLIFO object
      * http://beta.cwrc.ca/islandora/rest/v1/object/islandora%3A75a7614e-fd23-431a-ae36-6f521fa83990
   * list the relationships of a BIBLIFO object
      * http://beta.cwrc.ca/islandora/rest/v1/object/islandora%3A75a7614e-fd23-431a-ae36-6f521fa83990/relationship?
   * describe the MODS datastream properties of a BIBLIFO object
      * http://beta.cwrc.ca/islandora/rest/v1/object/islandora%3A75a7614e-fd23-431a-ae36-6f521fa83990/datastream/MODS?content=false
   * download the MODS datastream from a BIBLIFO object
      * http://beta.cwrc.ca/islandora/rest/v1/object/islandora%3A75a7614e-fd23-431a-ae36-6f521fa83990/datastream/MODS?content=true
   * search query to return all searchable Solr documents (search queries use Solr syntax and Solr’s API endpoint)
      * http://beta.cwrc.ca/islandora/rest/v1/solr/*%3A*
   * Islandora query command to use a cURL iTQL query to obtain a list of PIDs in a collection, and then use cURL to download the TEI and MODS datastreams respectively -- used to download the TEI and MODS datastreams from book objects from a collection in the Spanish Civil War project (the collection is "Prisoner of War Letters"; PID is the following: islandora:3ed517db-86e0-4b84-b589-1b050e406dec) (Note: this command also uses the Fedora Resource Index API to obtain the list of PIDs)
      * curl -s "http://cwrc-apps-01.srv.ualberta.ca:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Frelations-external%23isMemberOfCollection%3E%0A%3Cinfo%3Afedora%2Fislandora%3A3ed517db-86e0-4b84-b589-1b050e406dec%3E" | sed '1d;s/info:fedora\///g' >> pids.txt; mkdir TEI_files; cd TEI_files; for PID in `cat ../pids.txt`; do curl -s "http://cwrc-apps-01.srv.ualberta.ca/islandora/rest/v1/object/$PID/datastream/TEI" > $PID"_tei.xml"; mv $PID"_tei.xml" `echo $PID"_tei.xml" | sed s/:/_/`; done; cd ..
      * curl -s "http://cwrc-apps-01.srv.ualberta.ca:8080/fedora/risearch?type=tuples&lang=itql&format=csv&dt=on&query=select%20%24object%0Afrom%20%3C%23ri%3E%0Awhere%20%24object%20%3Cinfo%3Afedora%2Ffedora-system%3Adef%2Frelations-external%23isMemberOfCollection%3E%0A%3Cinfo%3Afedora%2Fislandora%3A3ed517db-86e0-4b84-b589-1b050e406dec%3E" | sed '1d;s/info:fedora\///g' >> pids.txt; mkdir MODS_files; cd MODS_files; for PID in `cat ../pids.txt`; do curl -s "http://cwrc-apps-01.srv.ualberta.ca/islandora/rest/v1/object/$PID/datastream/MODS" > $PID"_mods.xml"; mv $PID"_mods.xml" `echo $PID"_mods.xml" | sed s/:/_/`; done; cd ..
* Islandora CWRC entities API on beta
   * person entities -- example search for "Austen, Jane"
      * http://cwrc-apps-01.srv.ualberta.ca/islandora/cwrc_entities/v1/search/person?query=Austen,%20Jane&limit=100&page=0
   * organization entities -- example search for "United Nations"
      * http://cwrc-apps-01.srv.ualberta.ca/islandora/cwrc_entities/v1/search/organization?query=United%20Nations&limit=100&page=0
   * title entities -- example search for "Cosmos"
      * http://cwrc-apps-01.srv.ualberta.ca/islandora/cwrc_entities/v1/search/title?query=Cosmos&limit=100&page=0
   * place entities -- example search for "Ottawa"
      * http://cwrc-apps-01.srv.ualberta.ca/islandora/cwrc_entities/v1/search/place?query=Ottawa&limit=100&page=0
* Virtual International Authority File (VIAF) API
   * person entities -- example search for "Jane Austen"
      * http://www.viaf.org/viaf/search?query=local.personalNames+all+%22Jane%20Austen%22&sortKeys=holdingscount&maximumRecords=10&startRecord=1&httpAccept=text/xml
         * a selected record: http://viaf.org/viaf/102333412
   * organization entities -- example search for "United Nations"
      * http://www.viaf.org/viaf/search?query=local.corporateNames+all+%22United%20Nations%22&sortKeys=holdingscount&maximumRecords=10&startRecord=1&httpAccept=text/xml
         * a selected record: http://viaf.org/viaf/131378718
   * title entities -- example search for "Cosmos"
      * http://www.viaf.org/viaf/search?query=local.uniformTitleWorks+=%22Cosmos%22&sortKeys=holdingscount&maximumRecords=10&startRecord=1&httpAccept=text/xml
         * a selected record: http://viaf.org/viaf/219468490
* GeoNames Web Services API
   * place entities -- example search for "Ottawa"
      * http://api.geonames.org/search?name_equals=Ottawa&username=brundin
         * a selected record: http://www.geonames.org/6094817
   * Sample GET URL string:
      * http://api.geonames.org/search?name_equals=London&adminCode1=08&country=CA&featureClass=P&type=XML&username=brundin
   * Sample GET URL string using unique GeoNames ID (e.g., London, ON: 6058560):
      * http://ws.geonames.org/get?geonameId=6058560&username=brundin
   * Sample GET URL string to get map version:
      * http://www.geonames.org/6058560/
* Google Maps Geocoding API
   * place entities -- example search for "Ottawa"
      * http://maps.googleapis.com/maps/api/geocode/xml?address=Ottawa
         * a selected record: https://www.google.ca/maps/place/Ottawa,%20ON,%20Canada
   * Sample GET URL string:
      * http://maps.googleapis.com/maps/api/geocode/xml?address=London,ON&sensor=false
   * Sample GET URL string (another version):
      * http://www.google.ca/maps/api/geocode/xml?address=London,%20ON
   * Sample GET URL string to get map version using place name:
      * https://www.google.ca/maps/place/London,%20ON
   * Sample GET URL string using API key (AIzaSyBnKE7aZSWodIiAUEGZ1LL4WPJmshtDUnc):
      * https://maps.googleapis.com/maps/api/geocode/xml?address=London,ON&sensor=false&key=AIzaSyBnKE7aZSWodIiAUEGZ1LL4WPJmshtDUnc
   * Sample GET URL string using unique place_id (e.g., London, ON: ChIJC5uNqA7yLogRlWsFmmnXxyg) and API key (AIzaSyBnKE7aZSWodIiAUEGZ1LL4WPJmshtDUnc):
      * https://maps.googleapis.com/maps/api/geocode/xml?place_id=ChIJC5uNqA7yLogRlWsFmmnXxyg&sensor=false&key=AIzaSyBnKE7aZSWodIiAUEGZ1LL4WPJmshtDUnc
   * Sample GET URL string to get map version using latitude and longitude
      * https://www.google.ca/maps/place/53.527304,%20-113.519790
* Natural Resources Canada’s (NRCan) Canadian Geographical Names Data Base (CGNDB) API
   * Sample GET URL strings:
      * http://www.nrcan.gc.ca/earth-sciences/api?geoname=London&match=exact&conciseCode=CITY,TOWN,VILG,HAM,UNP&statusCode=A&regionCode=35&order=geoname&output=xml
      * http://www.nrcan.gc.ca/earth-sciences/api?geoname=Edmonton&match=exact&conciseCode=CITY,TOWN,VILG,HAM,UNP&statusCode=A&regionCode=48&order=geoname&output=xml
* geocoder.ca API
   * Sample GET URL strings:
      * http://geocoder.ca/?&locate=London,ON&geoit=XML
      * http://geocoder.ca/?&locate=8331%20104%20Street,Edmonton,AB&geoit=XML


Notes on the CWRC XML Validation Service


* the CWRC XML Validation Service is written in Java
* Commands to run the CWRC XML Validation Service (also known as the CWRC XML Validator API)
   * command to run Apache Tomcat
      * Windows: in the directory C:\xampp\tomcat\bin, run this command
         * startup.bat
      * Mac: in the directory /usr/local/apache-tomcat/bin, run this command
         * sh startup.sh
   * after Apache Tomcat has been started, command to run the CWRC XML Validation Service
      * Windows: in the directory C:\xampp\tomcat, run this command
         * php batch_validation_local_version.php | tee %USERPROFILE%\Desktop\output.xml
      * Mac: in the directory /usr/local/apache-tomcat, run this command
         * php batch_validation_local_version.php | tee $HOME/Desktop/output.xml
* Notes:
   * validation messages
      * valid document: "<status>pass</status>"
      * invalid document: "<status>fail</status>"
   * the validation output file for the nightly Orlando Document Archive validation process can be found here:
      * http://medusa.arts.ualberta.ca/~penner/downloads/validation_orlando.txt
   * XPath queries to query the XML validation output file (output.xml) to return the names of the documents that have a <status> element value of "fail"
      * Oxygen
         * /validation/validation_item[validation-result/status/text()="fail"]/@id
      * XMLStarlet
         * xml sel -T -t -v "/validation/validation_item[validation-result/status/text()='fail']/@id" output.xml > documents.txt
      * xmllint
         * xmllint --xpath  "/validation/validation_item[validation-result/status/text()='fail']/@id" output.xml | tr " " "\n" | sed "s/id=\"//g;s/\"$//g;/^$/d" > documents.txt
      * Saxon-HE
         * java -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Query -s:"output.xml" -qs:"/validation/validation_item[validation-result/status/text()='fail']/@id" -wrap | tr " " "\n" | grep "id=" | sed "s/id=\"//;s/\".*$//g" > documents.txt (GnuWin32 version)
         * java -cp "/Users/brundin/Applications/Saxon-HE/saxon9he.jar" net.sf.saxon.Query -s:"output.xml" -qs:"/validation/validation_item[validation-result/status/text()='fail']/@id" -wrap | tr " " "\n" | grep "id=" | sed "s/id=\"//;s/\".*$//g" > documents.txt (Unix version)
      * BaseX
         * basex -i output.xml -q "/validation/validation_item[validation-result/status/text()='fail']/@id" | sed "s/id=\"//g;s/\"$//g" > documents.txt
      * Zorba
         * zorba --context-item output.xml -r -q "/validation/validation_item[validation-result/status[text()='fail']]/@id/string()" | tr " " "\n" > documents.txt
         * zorba --context-item output.xml -r -q "/validation/validation_item[validation-result/status[text()='fail']]/string(@id)" | tr " " "\n" > documents.txt
   * XPath queries to write various bits of validation error information from the XML validation output file (output.xml) to a CSV file (CSV fields are tab-delimited; the XMLStarlet Windows version uses a TAB environment variable that was created by pasting in the tab character as the value for a newly created TAB environment variable); the information consists of the following fields: document name, line number, column number, error message, element name, and XPath
      * Oxygen
         * string-join(/validation/validation_item/validation-result[status/text()='fail']/warning/(concat(ancestor::validation_item/@id,'&#x9;',line/text(),'&#x9;',column/text(),'&#x9;',message/text(),'&#x9;',element/text(),'&#x9;',path/text())), '&#xa;')
      * XMLStarlet
         * xml sel -T -t -m "/validation/validation_item/validation-result[status/text()='fail']/warning" -v "concat(ancestor::validation_item/@id,'%TAB%',line,'%TAB%',column,'%TAB%',message,'%TAB%',element,'%TAB%',path)" -n output.xml > validation-errors.csv (Windows version)
         * xml sel -T -t -m "/validation/validation_item/validation-result[status/text()='fail']/warning" -v "concat(ancestor::validation_item/@id,'$(printf \\t)',line,'$(printf \\t)',column,'$(printf \\t)',message,'$(printf \\t)',element,'$(printf \\t)',path)" -n output.xml > validation-errors.csv (Unix version)
      * Saxon-HE
         * java -cp "C:\Program Files (x86)\Saxon-HE\saxon9he.jar" net.sf.saxon.Query -s:"output.xml" -qs:"declare option saxon:output 'omit-xml-declaration=yes'; string-join(/validation/validation_item/validation-result[status/text()='fail']/warning/(concat(ancestor::validation_item/@id,'&#x9;',line/text(),'&#x9;',column/text(),'&#x9;',message/text(),'&#x9;',element/text(),'&#x9;',path/text())), '&#xa;')" > validation-errors.csv (Windows version)
         * java -cp "/Users/brundin/Applications/Saxon-HE/saxon9he.jar" net.sf.saxon.Query -s:"output.xml" -qs:"declare option saxon:output 'omit-xml-declaration=yes'; string-join(/validation/validation_item/validation-result[status/text()='fail']/warning/(concat(ancestor::validation_item/@id,'&#x9;',line/text(),'&#x9;',column/text(),'&#x9;',message/text(),'&#x9;',element/text(),'&#x9;',path/text())), '&#xa;')" > validation-errors.csv (Unix version)
      * BaseX
         * basex -i output.xml -q "string-join(/validation/validation_item/validation-result[status/text()='fail']/warning/(concat(ancestor::validation_item/@id,'&#x9;',line/text(),'&#x9;',column/text(),'&#x9;',message/text(),'&#x9;',element/text(),'&#x9;',path/text())), '&#xa;')" > validation-errors.csv
      * Zorba
         * zorba --context-item output.xml -r -q "string-join(/validation/validation_item/validation-result[status/text()='fail']/warning/(concat(ancestor::validation_item/@id,'&#x9;',line/text(),'&#x9;',column/text(),'&#x9;',message/text(),'&#x9;',element/text(),'&#x9;',path/text())), '&#xa;')" > validation-errors.csv
   * the WAR file cwrcxmlval-0.0.1-SNAPSHOT.war is obtained from the CWRC GitHub repository cwrc-validator located here: https://github.com/cwrc/cwrc-validator; after downloading and unzipping the ZIP file, the WAR file can be found in the "target" directory; copy this file to C:\xampp\tomcat\webapps and rename the file "validator.war"; the next time Apache Tomcat is run the validator servlet will be installed in C:\xampp\tomcat\webapps\validator 
   * the validation-mrb.php script is located in the C:\xampp\tomcat directory; run Apache Tomcat to use CWRC validator service locally; to run Apache Tomcat, go to the directory C:\xampp\tomcat\bin and type "startup.bat"; after running the PHP script from C:\xampp\tomcat directory, to shut down Apache Tomcat, go back in the directory C:\xampp\tomcat\bin and type "shutdown.bat"
   * if getting a javax/servlet/Servlet.class error when running Apache Tomcat, then delete the jar file servlet-api-2.5.jar from the directory C:\xampp\tomcat\webapps\validator\WEB-INF\lib, and then it will properly load the jar file servlet-api.jar and its associated dependencies from the directory C:\xampp\tomcat\lib
   * in validation-mrb.php script, will set directory path to XML files to be validated, directory path to schema (schemas are located in the C:\Users\brundin\Documents\GitHub\CWRC-Schema\schemas directory), and schema type (RNG_XML or XSD_XML); as note above, run the PHP script and the CWRC XML Validation Service (after starting Apache Tomcat) by typing "php validation-mrb.php | tee %USERPROFILE%\Desktop\output.xml" from the directory C:\xampp\tomcat


Notes on batch processing a directory of XML files


* Tools to batch query a directory of XML files
   * BaseX (XPath 3.0 and XQuery 3.0)
   * Saxon-HE (XPath 3.0 and XQuery 3.0)
   * xmllint (XPath 1.0)
   * XMLStarlet (XPath 1.0)
   * Zorba (XPath 3.0 and XQuery 3.0)
* Tools to batch format and indent (pretty print) a directory of XML files
   * Tidy
   * xmllint
   * XMLStarlet
* Tools to batch check if well formed a directory of XML files
   * Tidy
   * xmllint
   * XMLStarlet
* Tools to batch validate a directory of XML files (only enumerating utilities for XSD and RELAX NG languages -- not DTD or Schematron languages)
   * Jing (XML Schema [XSD 1.1] and RELAX NG [including TEI all])
   * Sun Multi Schema Validator (MSV) (XML Schema [XSD 1.0] and RELAX NG [except TEI all, due to a bug causing an infinite loop in the libxml2 library file called relaxng.c])
   * Xerces 2 Java (XML Schema [XSD 1.1] and RELAX NG [including TEI all])
   * xmllint (XML Schema [XSD 1.0] and RELAX NG [except TEI all, due to a bug causing an infinite loop in the libxml2 library file called relaxng.c])
   * XMLStarlet (XML Schema [XSD 1.0] and RELAX NG [except TEI all, due to a bug in the libxml2 library file called relaxng.c])
* Tools to batch transform a directory of XML files
   * BaseX (XSLT 2.0 and XQuery 3.0)
   * Saxon-HE (XSLT 2.0 and XQuery 3.0)
   * Xalan-Java (XSLT 1.0)
   * XML Calabash (XSLT 2.0; uses XProc)
   * XMLStarlet (XSLT 1.0)
   * xsltproc (XSLT 1.0)
   * Zorba (XSLT 1.0 and XQuery 3.0)


Notes on processing very large files (VLFs)


* Tools to process very large text files
   * Search and replace input file keys with index file values
      * AWK script search-replace.awk
      * Perl script search-replace.pl
   * Convert a CSV text file with a header to an XML file
      * AWK script csv2xml.awk
      * Perl script csv2xml.pl
      * XSLT stylesheet csv2xml.xsl
* Tools to process very large XML files
   * View files:
      * Oxygen (Large File Viewer option)
      * Vim
      * XML Copy Editor (Open Large Document option)
   * Edit files
      * BaseX (regular expression, XPath 3.0, and XQuery 3.0 editing)
      * sed (regular expression editing)
      * Oxygen (regular expression, XPath 3.0, and XQuery 3.0 editing)
      * Tidy (format and indent)
      * Vim (regular expression editing)
      * XML Copy Editor (regular expression and XPath 1.0 editing)
      * xml_pp (format and indent)
      * xmllint (format and indent)
      * XMLStarlet (format and indent, XPath 1.0 editing)
      * Zorba (regular expression, XPath 3.0, and XQuery 3.0 editing)
   * Query files
      * BaseX (regular expression, XPath 3.0, and XQuery 3.0 queries)
      * findstr (regular expression queries)
      * grep (regular expression queries)
      * Oxygen (regular expression, XPath 3.0, and XQuery 3.0 queries)
      * Saxon-HE (XPath 3.0, and XQuery 3.0 queries)
      * XML Copy Editor (regular expression and XPath 1.0 queries)
      * xml_grep (limited/specialized XPath 1.0 queries)
      * xmllint (XPath 1.0 queries)
      * XMLStarlet (XPath 1.0 queries)
      * Zorba (regular expression, XPath 3.0, and XQuery 3.0 queries)
   * Transform files
      * BaseX (XSLT 2.0 and XQuery 3.0 transforms)
      * Oxygen (XSLT 2.0 and XQuery 3.0 transforms)
      * Saxon-HE (XSLT 2.0 and XQuery 3.0 transforms)
      * Xalan-Java (XSLT 1.0 transforms)
      * XML Copy Editor (XSLT 1.0 transforms)
      * XMLStarlet (XSLT 1.0 transforms)
      * xsltproc (XSLT 1.0 transforms)
      * Zorba (XSLT 1.0 and XQuery 3.0 transforms)
   * Validate files
      * CWRC XML Validation Service (also known as CWRC XML Validator API [it's a Java Servlet application]) (RELAX NG and XML Schema)
      * Jing (RELAX NG and XML Schema; this appears to be the best utility for RELAX NG validation, e.g., for TEI documents)
      * Oxygen (RELAX NG and XML Schema)
      * Sun Multi Schema Validator (MSV) (RELAX NG and XML Schema; very fast and good at processing tens of thousands of documents)
      * Xerces2 Java (RELAX NG and XML Schema)
      * XML Copy Editor (RELAX NG and XML Schema)
      * xmllint (RELAX NG and XML Schema)
      * XMLStarlet (RELAX NG and XML Schema)


Steps to install the XMLStarlet utility in a user's home directory


* Steps to install the the XMLStarlet utility software in a user's home directory on a Unix-like operating system, in a full local hierarchy under a subdirectory called "local" (the local hierarchy gets created during the installation process)
   * To set up certain environment variables ($PATH, and $LD_LIBRARY_PATH -- $PERL5LIB is used to install the XML::Twig Perl module, and $XMLLINT_INDENT sets the indent at 4 spaces for the xmllint utility), add the following lines to the .profile or .bash_profile file found in the root of the home directory:


# User specific environment and startup programs


# MRB: Wed 10-Sep-2014: made $HOME/bin first (before $PATH), and added
# $HOME/local/bin


PATH=$HOME/bin:$HOME/local/bin:$PATH


export PATH


# MRB: Wed 10-Sep-2014: added XMLLINT_INDENT environment variable
XMLLINT_INDENT='    '; export XMLLINT_INDENT


# MRB: Mon 22-Sep-2014: added PERL5LIB environment variable
export PERL5LIB=$HOME/local/lib


# MRB: Wed 24-Sep-2014: added LD_LIBRARY_PATH environment variable
export LD_LIBRARY_PATH=$HOME/local/lib


# MRB: Wed 24-Sep-2014: set alias for xmlstarlet from "xml" to "xmlstarlet"
alias xmlstarlet='xml'


# MRB: Mon 20-Apr-2015: added MANPATH environment variable
MANPATH=$HOME/local/share/man:$MANPATH; export MANPATH


   * Create a directory in your home directory called "local"
   * In the directory "local", create a subdirectory called "src"; each tarball will be uncompressed and built in the "src" directory; XMLStarlet has some dependencies, and some of these dependencies have dependencies, so install the tarballs in the order outlined below (i.e., libiconv, libxml2, libxslt, xmlstarlet)
   * Uncompress, prepare (setup), build, and install the libiconv-1.14.tar.gz tarball
      * tar -xvzf libiconv-1.14.tar.gz (or gunzip -c libiconv-1.14.tar.gz | tar -xvf -)
      * cd  libiconv-1.14
      * ./configure --prefix=$HOME/local
      * make
      * make install
   * Uncompress, prepare (setup), build, and install the libxml2-2.9.1.tar.gz tarball
      * tar -xvzf libxml2-2.9.1.tar.gz (or gunzip -c libxml2-2.9.1.tar.gz | tar -xvf -)
      * cd libxml2-2.9.1
      * ./configure --prefix=$HOME/local
      * make
      * make install
   * Uncompress, prepare (setup), build, and install the libxslt-1.1.28.tar.gz tarball
      * tar -xvzf libxslt-1.1.28.tar.gz (or gunzip -c libxslt-1.1.28.tar.gz | tar -xvf -)
      * cd libxslt-1.1.28
      * ./configure --prefix=$HOME/local
      * make
      * make install
   * Uncompress, prepare (setup), build, and install the xmlstarlet-1.6.1.tar.gz tarball
      * tar -xvzf xmlstarlet-1.6.1.tar.gz (or gunzip -c xmlstarlet-1.6.1.tar.gz | tar -xvf -)
      * cd xmlstarlet-1.6.1
      * ./configure --prefix=$HOME/local
      * make
      * make install
   * Notes:
      * To include headers or link against libraries in $HOME/local, instead of using environment variables in your .profile file, can use the following:
         * ./configure --prefix=$HOME/local CFLAGS="-I$HOME/local/include" LDFLAGS="-L$HOME/local/lib"
      * Can also link to specific libraries using the following:
         * ./configure --prefix=$HOME/local --with-foolib=$HOME/local/lib
      * To uninstall a local application (i.e., a locally installed utility), uncompress the source tarball again (tar -xvzf file_name.tar.gz), cd into the unpacked directory, and then type the following:
         * ./configure --prefix=$HOME/local
         * make uninstall
      * To prepare (setup), build, and install a local Perl module (after uncompressing the Perl module tarball)
         * For Makefile.PL-based distributions:
            * perl Makefile.PL PREFIX=$HOME/local
            * (Or instead of the PREFIX option can use the INSTALL_BASE option:
            * perl Makefile.PL INSTALL_BASE=$HOME/local)
            * make
            * make test
            * make install
         * For Build.PL-based distributions:
            * perl Build.PL --install_base $HOME/local
            * ./Build
            * ./Build test
            * ./Build install


Commands to install the Noid utility software


* noid (nice opaque identifier) is a Perl module that creates minters (identifier generators)
* Commands to install the the Noid utility software
   * First, install the Berkeley DB system library software db (it is an embedded database, written in C); can obtain from Oracle (note: need to get an Oracle account in order to download the software)
      * wget http://download.oracle.com/otn/berkeley-db/db-6.0.20.tar.gz
      * tar -xzf db-6.0.20.tar.gz
      * cd db-6.0.20
      * cd build_unix
      * ../dist/configure
      * make
      * sudo make install
   * Second, install the Perl module BerkeleyDB.pm
      * wget http://search.cpan.org/CPAN/authors/id/P/PM/PMQS/BerkeleyDB-0.27.tar.gz
      * tar -xzf BerkeleyDB-0.27.tar.gz
      * cd BerkeleyDB-0.27
      * vi config.in     # then edit and save this file to change the INCLUDE and LIB settings:
         * INCLUDE = /usr/local/BerkeleyDB.6.0/include
         * LIB = /usr/local/BerkeleyDB.6.0/lib
      * LD_LIBRARY_PATH=/usr/local/BerkeleyDB.6.0/lib
      * export PERL5LIB
      * perl Makefile.PL
      * make
      * make test
      * sudo make install
   * Third, install the Perl module Noid.pm and other associated dependencies, as well as the script "noid"
      * wget http://search.cpan.org/CPAN/authors/id/J/JA/JAK/Noid-0.424.tar.gz
      * tar -xzf Noid-0.424.tar.gz
      * cd Noid-0.424
      * perl Makefile.PL
      * make
      * make test
      * sudo make install


Software used to transform a historical map


* To georeference and transform the John Arrowsmith 1854 map of British North America from a pseudoconic Bonne projection to a cylindrical WGS 1984 Web Mercator (Auxiliary Sphere) (i.e., EPSG:3857) projection, the following software programs were used:
   * QGIS -- GIS program (written in C++, Python, and Qt) to georeference and transform the map image; used 183 ground control points (could also use the ArcGIS program (written in C++) -- available in second floor Rutherford South computer lab, south room)
   * IrfanView -- image editor (written in C++) to make transparent the boundaries of the transformed map image (could also use the GIMP image editor [written in C and GTK+], the Pinta image editor [written in C# and GTK#], or the Paint.NET image editor [written in C# and C++])
   * pngquant -- command-line utility (written in C) for lossy compression of the PNG map image (could also use the Pngcrush command line utility [written in C] for lossless compression of the PNG map image)


Notes on the GeoNames database


* GeoNames database characteristics
   * database dump file location is http://download.geonames.org/export/dump/
   * world place names file is called allCountries.zip, which contains a file called allCountries.txt
   * size is 1,053,587,582 bytes, i.e., 1 GB
   * contains 8,526,023 records! 
   * each record is on one line in the allCountries.zip file
   * each record has up to 19 fields that are tab delimited
   * world populated places contains 3,235,027 records
   * there are 249 country codes (ISO 3166-1 alpha-2)
   * there are 3,878 admin1 codes (FIPS and ISO 3166-2; first administrative division [note: UK and Greece use this field for a supra-division between the country code and the FIPS code)
   * there are 35,118 admin2 codes (second administrative division, akin to a a county; for UK and Greece, this is the FIPS code first administrative division [they have a supra-division between the FIPS and country code in the admin1 field)
   * there are 418 timezone IDs
   * there are 9 feature classes and 668 feature codes (GeoNames defined codes)
   * the two feature classes and their accompanying feature codes that are probably of most interest in the context of the CWRC project are the following:
      * P (Populated Place Features) feature class, with feature codes of PPL*
      * A (Administrative Boundary Features) feature class, with feature codes of ADM*
   * Could also restrict the GeoNames world data set to just the feature codes of PPL* and ADM* for the countries of CA (Canada), US (United States), and GB (Great Britain).  For example,
      * Canadian place names (geographic features) file is called CA.txt
      * Canadian place names file size is 33,297,638 bytes, i.e., 33 MB
      * Canadian place names contains 308,288 records
      * Canadian populated places (PPL*) contains 19,457 records
      * Canadian administrative places (ADM*) contains 1,636 records
      * So, using this approach, the Canadian data set would be reduced from 308,288 records to 21,093 records


Notes on Unix spell-checker and writing-aid programs


* words file
   * words file located at /usr/share/dict/words or /usr/dict/words
   * command to find a word with missing letters to help solve a crossword puzzle; command is case insensitive, and the output is displayed with the words appearing as a string along the line, with each word separated by a space (i.e., instead of each word appearing on its own line, the words appear after each other on the same line, separated by a space)
      *  grep -i '^s...c..m$' /usr/share/dict/words | tr '\n' ' '; printf '\n'
   * command to print out all palindromes in words file; the output is displayed with the words appearing as a string along the line, with each word separated by a space (i.e., instead of each word appearing on its own line, the words appear after each other on the same line, separated by a space)
      * perl -lne 'print if $_ eq reverse' /usr/share/dict/words | tr '\n' ' '; printf '\n'
      * perl -lne 'print if $_ eq reverse' /usr/share/dict/words | grep -vw '.' | tr '\n' ' '; printf '\n' (only words of two or more characters, i.e., remove words of a single character)
* Programs
   * spell (1975) (written in C)
   * style (1979) (analyze surface characteristics of a document; rewritten and ported to Linux in 1997) (written in C)
   * diction (1979) (print wordy and commonly misused phrases in sentences; rewritten and ported to Linux in 1997) (written in C)
   * Ispell (1971; ported to Unix in 1983) (written in C)
   * Aspell (1998) (successor to Ispell) (written in C)
   * Pspell (2000) (Pspell codebase was later merged into Aspell) (written in C)
   * Hunspell (2003) (based on MySpell) (written in C++)
   * sdcv (2003) (console version of the StarDict dictionary program) (written in C)
   * Enchant (2010) (meta spell-checker; is a frontend wrapper to 8 other backend spell-checkers, including Aspell and Hunspell) (written in C)
   * Can use the man pages to read more about how to use the programs.  GPU (OpenBSD) has spell, Ispell, and Aspell.
   * Using Aspell, can use these commands:
      * meaning of returned symbols:
         * a returned asterisk ("*") means the word was spelled correctly
         * a returned pound sign ("#") means the word was not found in the dictionary
         * a returned ampersand sign ("&") means the word was misspelled
      * look up a word from the command line -- if misspelled, will return suggestions -- a correctly spelled word will return an asterisk ("*"):
         * echo 'word' | aspell -a
         * aspell -a (then type word and hit enter -- then type Ctrl-d to exit)
      * look up several words from the command line, and return the misspelled words with suggestions:
         * echo 'word1 word2 word3' | aspell -a
      * look up several words from the command line, and return the misspelled words in a list with no suggestions:
         * echo 'word1 word2 word3' | aspell --list
      * spell check a document (will save the previous version with a .bak extension):
         * aspell check filename.txt
   * Using Aspell, can use these commands to do the following on multiple files:
      * interactive spell check on multiple files, to make corrections to spelling mistakes ("check" opens each file sequentially in interactive spell checker mode):
         * for f in *.txt; do aspell check $f; done
      * after correcting words in interactive mode, these corrected words are then automatically checked in future dictionary searches and are stored in .aspell.en.prepl in your home directory
      * you can also create a list of stop words which are ignored -- these are located in the dot file .aspell.en.pws in your home directory; this file can be referenced in a search using the --personal flag after the list command, so like so:
         * aspell list --personal=$HOME/.aspell.en.pws
      * print list of misspelled words to the screen ("list" prints all misspelled words; could also redirect standard out to a file if you want):
         * cat *.txt | aspell list | sort -u
      * one can also select a filter mode, which will filter out the input text according to the mode selected -- the default filter mode is "url"; to filter out XML elements and attributes from the spell-checking routine, use the sgml mode, like so:
         * cat input.xml | aspell list --mode=sgml > output.txt
      * to print a list of each file name that contains misspelled words, along with a sorted list of the misspelled words, including the number of times the misspellings occur:
         * for f in *.txt ; do echo $f ; aspell list < $f | sort | uniq -c ; done


        Server documentation
List of Unix servers


* List of Unix servers that I have access to
   * arrl-web001.artsrn.ualberta.ca (142.244.64.73) (ARC Web server)
   * cwrc-apps-01.srv.ualberta.ca (142.244.64.84, beta.cwrc.ca) (production Islandora installation)
   * cwrc-apps-02.srv.ualberta.ca (142.244.64.85, cwrc.ca) (production CWRC Web server)
   * cwrc-apps-07.srv.ualberta.ca (142.244.64.90, apps.testing.cwrc.ca) (testing server for applications)
   * cwrc-apps-09.srv.ualberta.ca (142.244.64.116, dev.alpha.cwrc.ca) (MRB's server)
   * cwrc-dev-01.srv.ualberta.ca (129.128.56.120) (development Islandora installation)
   * cwrc-dev-05.srv.ualberta.ca (129.128.56.215) (development Islandora installation)
   * cwrc-dev-06.srv.ualberta.ca (129.128.56.187) (Digital Echidna Islandora sandbox installation)
   * cwrc-dev-07.srv.ualberta.ca (129.128.56.182) (Digital Echidna Islandora sandbox installation)
   * gpu.srv.ualberta.ca (129.128.5.151, login.srv.ualberta.ca, ualoginprod-01.srv.ualberta.ca, login-new.srv.ualberta.ca) (Uof A GPU server)


Web root directories on CWRC Web servers


* cwrc-apps-01.srv.ualberta.ca: /var/www/html
* cwrc-apps-02.srv.ualberta.ca: /data/opt/cwrc/WEB-ROOT
* cwrc-apps-07.srv.ualberta.ca: /data/opt/cwrc
* cwrc-apps-09.srv.ualberta.ca: /var/www/html
* cwrc-dev-01.srv.ualberta.ca: /var/www/html
* cwrc-dev-05.srv.ualberta.ca: /var/www/html


        Problem resolution
Oxygen XML Editor search and replace using both XPath expressions and regular expressions (Regeneration document <hi> element problem)


* Problem task: delete beginning and ending <hi> and </hi> element tags for hi elements that have no attributes, and preserve the mixed content (both elements and text) in these hi elements
* Initial XPath queries: in the XPath 2.0 text field, ran these XPath queries
   * //hi ==> 4,800 hi elements in total
   * //hi[@*] ==> 3,549 hi elements with attributes
   * //hi[not(@*)] ==> 1,251 hi elements without attributes (these are the elements where the opening and closing tags are to be removed, but the content in these elements is to be preserved)
   * //hi[not(@*)]/hi ==> 22 hi elements with no attributes and with a child hi element
* Therefore used two regular expression search and replace operations, so a first pass search and replace operation on hi elements with no attributes and no hi children, and then a second pass search and replace operation on the 22 hi elements with no attributes but with a hi child element
* The first regular expression search and replace operation:
   * Open Oxygen's Find/Replace dialog box
   * In the "Text to find:" textbox, type the following: <hi>(.*)</hi>
   * In the "Replace with:" textbox, type the following: \1
   * In the "XPath:" textfield, type the following: //hi[not(@*)][not(hi)]
   * Check the "Regular expression" checkbox to enable regular expression searching
   * Click the "Replace All" form button
   * The operation will remove all hi elements with no attributes but retain these elements' contents, except for 22 hi elements with no attributes that have embedded child hi elements with attributes
* The second regular expression search and replace operation:
   * In Oxygen's Find/Replace dialog box
   * In the "Text to find:" textbox, type the following: <hi>(<hi.*</hi>)</hi>
   * In the "Replace with:" textbox, type the following: \1
   * In the "XPath:" textfield, type the following: //hi[not(@*)]
   * Check the "Regular expression" checkbox to enable regular expression searching
   * Click the "Replace All" form button
   * The operation will remove these 22 hi elements with no attributes but retain their child hi elements with attributes and other content.


Resolution of Dell laptop display problem


* Problem: The Dell laptop periodically will have the display turn black, and then often would recover, and display this message: "Display driver stopped responding and has recovered".  This problem is because "the GPU is taking more time than permitted to display graphics to your monitor".  (Another problem that this fix did not resolve is that when I take the Dell out of the dock and switch from Ethernet to wireless, the screen goes black -- still searching for a solution to this problem.)
* Fix: "Increase the GPU (Graphics Processing Unit) processing time by adjusting the Timeout Detection and Recovery registry value" -- increased it from the default of 2 seconds, to 8 seconds (see Microsoft Support Web page here: http://support.microsoft.com/en-us/kb/2665946)
* Steps:
   * Launched the Windows Registry Editor, and then browsed to and clicked on the following registry subkey: HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\GraphicsDrivers
   * On the Edit menu, clicked New, and then selected the registry value "QWORD (64-bit) value" from the drop-down menu
   * Typed TdrDelay as the Name and clicked Enter
   * Double-clicked TdrDelay and added 8 for the Value data and clicked OK
   * Closed the Windows Registry Editor and then restarted the Dell for the changes to take effect


Resolution of user profile problem


* Problem: After my Active Directory account was deleted, and then a new Active Directory account was created for me, I no longer had access to my original local profiles on the Dell Precision laptop CWRCLAB04 (Windows 7 operating system) and the MacBook Pro laptop cwrcmac02 (Mac OS X operating system).  I detail below the problem on each of the laptops, and how we resolved the problem on each laptop.  As part of the process to fix the problems, I used another "superuser" (Mac) and "administrator" (Dell) account called "orlando".  The Mac was relatively straightforward to fix, but the Dell was more challenging to resolve.
* MacBook Pro laptop cwrcmac02 (Mac OS X operating system):
   * Problem: After logging into the Mac, I could not write to certain directories in my home directory (/Users/brundin), such as Documents and Downloads, because these directories were no longer owned by "brundin", but rather were owned by my new Active Directory "numeric ID".
   * Resolution:
      * I logged into the Mac using the orlando account
      * Using Terminal, I navigated to the "/Users" directory
      * From the "/Users" directory, I then ran the following command: sudo chown -R brundin brundin
      * I then navigated to System Preferences > Security and Privacy > FileVault, and then there was a message to the effect that not all users had access to the login screen, I clicked on the button, and then provided the password for "Michael Brundin"
      * I then logged out of the orlando account, and logged back in to the brundin account
      * Fixed!
* Dell Precision laptop CWRCLAB04 (Windows 7 operating system):
   * Problem: After logging into the Dell, I was in an entirely new profile, "C:\Users\brundin.ARTSRN", and thus no longer had access to all of my application configuration settings and data in my original profile of "C:\Users\brundin".
   * Resolution:
      * I logged into the Del using the orlando account
      * Using the Windows Registry Editor, I navigated to the Registry key HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\ProfileList.  The ProfileList key has several subkeys beginning with the name "S-1-5-*".  Starting at the bottom, I found the "S-1-5-*" subkey with a name/data value of name of "ProfileImagePath" and data of "C:\Users\brundin.ARTSRN", and deleted this subkey.  I then found the subkey with a name/data value of name of "ProfileImagePath" and data of "C:\Users\brundin", and deleted this subkey as well.
      * I then renamed the "C:\Users\brundin" directory to "C:\Users\brundin.original", and deleted the directory "C:\Users\brundin.ARTSRN"
      * I then logged out of the orlando account, and logged back in with my brundin account; the correct user profile path was successfully created (both in the Registry, and also a new C:\Users\brundin directory), and the Command Prompt command "echo %USERPROFILE%" returns "C:\Users\brundin"
      * I then logged out of my brundin account, and logged back in with the orlando account
      * After displaying hidden and system files, I then deleted all of the "ntuser*" files in the root of "C:\Users\brundin.original", and then copied over all of the "ntuser*" files in the root of "C:\Users\brundin" into the root of "C:\Users\brundin.original"
      * I then deleted all of the "UsrClass.dat*" files in the root of "C:\Users\brundin.original\AppData\Local\Microsoft\Windows"
      * I then deleted the "C:\Users\brundin" directory
      * I then renamed the "C:\Users\brundin.original" directory to "C:\Users\brundin"
      * I then logged out of the orlando account, and logged back in to my brundin account
      * I then used Windows Explorer to right-click on the directory "C:\Users\brundin", and then I selected Properties > Security > Advanced > Owner > Edit > Other users or groups, and then I changed the owner from "S-1-5-21-4193484638-2761506387-369918953-1525" to "Michael Brundin (brundin@artsrn.ualberta.ca)" and selected OK, and then I selected the checkbox "Replace owner on subcontainers and objects", and then clicked on OK
      * Fixed!


Remotely accessing the laptop in the office


Note: use DuckDNS Updater (written in Java, and compiled as a Windows EXE executable) now, rather than DynDNS


* Ethernet: Faculty of Arts LAN (artsrn.ualberta.ca)
   * Applications: DynDNS; freeSSHd (written in C++); PuTTY (written in C); TightVNC (written in C, C++, and Java)
   * Can't initiate a connection from outside the LAN in because of firewall restrictions, but can bypass the firewall by establishing a connection from inside the LAN out; can use a VNC (on port 5900) or RDP (on port 3389) over a reverse SSH tunnel (on port 22), or a TightVNC Server reverse connection to the TightVNC Viewer; can also use Microsoft SkyDrive to browse and access files on the office laptop
   * (1) VNC over reverse SSH tunnel
      * Netbook at home (perform these operations before leaving for work)
         * Launch WMP and pause on song, so that netbook doesn't go to sleep or hibernate
         * Launch DynDNS, and resolve the dynamic IP address to mrb.dyndns-home.com
         * Launch the freeSSHd server; on the "Tunneling" tab, check that "Allow local port forwarding" and "Allow remote port forwarding" are checked
      * Laptop in the office
         * Remove laptop from dock, open lid, plug in AC power cord, and plug in Ethernet cable
         * Launch WMP and pause on song, so that laptop doesn't go to sleep or hibernate
         * Launch DynDNS, and resolve the static IP address dhcp-40-171.arts.ualberta.ca (142.244.40.171) to mrb.dyndns-work.com
         * Launch PuTTY with port forwarding to make the reverse connection to the freeSSHd server on the home netbook; the tunnel is 4R5900 to 127.0.0.1:5900 (on the "Tunnels" Categroy screen), and the host name is mrb.dyndns-home.com (on the "Session" Category screen); in the "Port forwarding" section of the "Tunnels" Category screen, check that "Local ports accept incoming connections from other hosts" and "Remote ports do the same (SSH-2 only)" are checked; reverse connect to the freeSSHd server on the home netbook via PuTTY
         * Launch TightVNC Server in either Application Mode or Service Mode; on the "Server" tab, check that the "Primary password" is set; on the "Access Control" tab, check that "Allow loopback connections" is checked
      * Netbook at home
         * Launch TightVNC Viewer; in the "Remote Host" connection field, type 127.0.0.1 and click on the "Connect" button  to connect via a reverse SSH tunnel to the TightVNC Server on the office laptop
         * Use TightVNC Viewer to remotely control the office laptop
   * (2) TightVNC Server reverse connection to TightVNC Viewer
      * Netbook at home (perform these operations before leaving for work)
         * Launch DynDNS, and resolve the dynamic IP address to mrb.dyndns-home.com
         * Launch TightVNC Viewer in "Listening mode"
      * Laptop in the office
         * Remove laptop from dock, open lid, plug in AC power cord, and plug in Ethernet cable
         * Launch WMP and pause on song, so that laptop doesn't go to sleep or hibernate
         * Launch TightVNC Server, right click on the TightVNC Server system tray icon, and select the "Attach Listening Viewer" dialog box; type mrb.dyndns-home.com and then click on "Attach" to reverse connect the office laptop to the home netbook
      * Netbook at home
         * Use TightVNC Viewer to remotely control the office laptop
   * (3) Microsoft SkyDrive
      * Launch the Microsoft SkyDrive application on the office laptop
      * From the remote location, using a browser, go to this URL https://skydrive.live.com/
      * Log in and authenticate with Microsoft Hotmail account
      * Click on the CWRCLAB04 PC entry
      * Can now browse and access all the directories and files on the office laptop


* Wireless: UWS
   * Applications: DynDNS; freeSSHd; PuTTY; TightVNC
   * Can establish a straight VNC or RDP connection, or for a secure connection, can forward either of those connections over an SSH tunnel
   * VNC over SSH tunnel
      * Laptop in the office
         * Unplug Ethernet cable, remove laptop from dock, open lid, and plug in AC power cord
         * Launch WMP and pause on song, so that laptop doesn't go to sleep or hibernate
         * Launch DynDNS, and resolve the dynamic IP address to mrb.dyndns-work.com
         * Launch the freeSSHd server as administrator; on the "Tunneling" tab, check that "Allow local port forwarding" is checked
         * Launch TightVNC Server in either Application Mode or Service Mode; on the "Server" tab, check that the "Primary password" is set; on the "Access Control" tab, check that "Allow loopback connections" is checked
      * Netbook at home
         * Launch PuTTY with port forwarding to make the connection to the freeSSHd server on the office laptop; the tunnel is L5900 to 127.0.0.1:5900 (on the "Tunnels" Category screen), and the host name is mrb.dyndns-work.com (on the "Session" Category screen); connect to the freeSSHd server on the office laptop via PuTTY
         * Launch TightVNC Viewer; in the "Remote Host" connection field, type 127.0.0.1 and click on the "Connect" button to connect via an SSH tunnel to the TightVNC Server on the office laptop
         * Use TightVNC Viewer to remotely control the office laptop